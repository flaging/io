

#### [OpenCV dnn模块中的矩阵乘法操作在龙芯上最快可提速10倍！最近OpenCV中国团队冯远滔工程 @龙芯中科](https://weibo.com/1917002727/NktDp0wE1)

Note: OpenCV dnn模块中的矩阵乘法操作在龙芯上最快可提速10倍！最近OpenCV中国团队冯远滔工程师，用gemm矩阵乘法改写了之前的fc操作，并针对龙芯进行了优化，特别是在大矩阵情况下提速明显。详情请见 h踢踢皮 s://github.com/opencv/opencv/pull/23897  感谢 捐赠的大别山系列龙芯服务器，让OpenCV可以在龙芯指令上优化和测试。除了龙芯之外，OpenCV还针对RISC-V架构使用RVV指令进行了优化，欢迎关注我们的开发进度。加油！踢踢皮转发微博

Picture: [724323e7gy1hi5fjbftenj21em0k8n74.jpg](https://weibo.cn//mblog/pic/NktDp0wE1?rl=1)

Github: [github.com/opencv/opencv/pull/23897](https://github.com/opencv/opencv/pull/23897)

#### [黄国平在西南大学计算机与信息科学学院软件学院开学典礼上的发言（有删减） 老师如何看待rust？跟py @翻译驴](https://weibo.com/1917002727/NkkCIBUAd)

Note: 黄国平在西南大学计算机与信息科学学院软件学院开学典礼上的发言（有删减） 老师如何看待rust？跟python有仇似的。后悔过，但以后不会了//:转发微博

Picture: [66a07ce3gy1hi4bixa36lj21u69367wl.jpg](https://weibo.cn//mblog/pic/NkkA94dHn?rl=1)

#### [【Flax: 为JAX设计的神经网络库和生态系统，旨在提供灵活性】'Flax: A neural n @#开源#](https://weibo.com/1402400261/Nkj2Eftoo)

Note: 【Flax: 为JAX设计的神经网络库和生态系统，旨在提供灵活性】'Flax: A neural network library and ecosystem for JAX designed for flexibility - Flax is a neural network library for JAX that is designed for flexibility.' Google GitHub: github.com/google/flax   你好，你感兴趣的“开源”已开通了超话社区～ 超话社区是微博旗下兴趣互动社区，快来与志同道合的小伙伴们一起交流互动吧！ 戳我进入>> 

Picture: [5396ee05ly8hi44pd8clej20ub0u0jwh.jpg](https://weibo.cn//mblog/pic/Nkj2Eftoo?rl=1)

Github: [github.com/google/flax](https://github.com/google/flax)

#### [【用DeepSpeed、LoRA和Flash Attention微调Falcon 180B】- Fa @网页链接](https://weibo.com/1402400261/Nkh8jbqM3)

Note: 【用DeepSpeed、LoRA和Flash Attention微调Falcon 180B】- Falcon 180B是目前最大的开源语言模型，参数量达180B。文章介绍了如何在多GPU环境下微调该模型。   - 使用了DeepSpeed ZeRO进行内存优化，以在有限GPU内存下训练超大规模模型。   - 使用Hugging Face Transformers加载数据集，提供简单的Trainer API。   - 使用LoRA仅更新部分参数，大大降低了计算和内存成本。   - 采用Flash Attention加速了注意力机制计算。   - 详细介绍了环境配置、数据准备、模型微调的代码。   - 整合各种技术后，允许在有限资源下高效微调超过100B参数的语言模型。   - 该示例提供了高效调优最大公开语言模型的模板。   - 相比Falcon 180B的预训练计算量，微调仅需要约3500万分之一的计算资源。《Fine-tune Falcon 180B with DeepSpeed ZeRO, LoRA & Flash Attention》    

Picture: [5396ee05ly8hi3w920th1j21720pgjvz.jpg](https://weibo.cn//mblog/pic/Nkh8jbqM3?rl=1)

#### [【用积量化(PQ)将(向量搜索)内存需求降低90%以上】- PQ通过压缩向量来降低内存需求，但代价是 @网页链接](https://weibo.com/1402400261/Nk7EUBoGu)

Note: 【用积量化(PQ)将(向量搜索)内存需求降低90%以上】- PQ通过压缩向量来降低内存需求，但代价是牺牲部分召回率。   - 在Weaviate 1.21中，通过在粗搜索后进行精确距离重新评分来改进PQ，可以在降低内存的同时保持召回率。   - 新的PQ实现索引时间与不压缩相当，召回率也接近。   - 在实验中，压缩向量可将内存需求降低约85%，对于10亿向量数据集，从2.7TB降至0.7TB。   - 用户可以用自己的数据基准测试PQ，在召回率、延迟和内存之间找到平衡。   《How to Reduce Memory Requirements by up to 90%+ using Product Quantization | Weaviate - vector database》  先粗pq后精sq提升召回，google 在21年已经开源了 scannPQ,SQ，这些Milvus也早就实现了，听说还能后台自动针对更新向量生成索引，这个步骤在faiss里面是非常耗时的

Picture: [5396ee05ly8hi2qddz2l0j20xd0snjto.jpg](https://weibo.cn//mblog/pic/Nk7EUBoGu?rl=1)

#### [【Petals：像使用BitTorrent一样分布式运行大型语言模型，支持模型的微调和推断，通过连接 @爱可可-爱生活](https://weibo.com/1402400261/Nk2V7iOJC)

Note: 【Petals：像使用BitTorrent一样分布式运行大型语言模型，支持模型的微调和推断，通过连接到分布式网络上的模型层，实现了模型的分布式运行，依赖于社区共享GPU资源，用户可以通过运行特定命令来帮助托管模型的一部分】'petals - Decentralized platform for running 100B+ language models' by BigScience Workshop GitHub: github.com/bigscience-workshop/petals  

Picture: [5396ee05ly1hi25ew36ybj20gv0xcq64.jpg](https://weibo.cn//mblog/pic/M4klNak8S?rl=1)

Github: [github.com/bigscience-workshop/petals](https://github.com/bigscience-workshop/petals)

#### [【micro{gl}：C++11矢量图形库，可以在没有FPU或GPU的任意32/64位计算机上运行】 @#开源#](https://weibo.com/1402400261/Nk0fS37pZ)

Note: 【micro{gl}：C++11矢量图形库，可以在没有FPU或GPU的任意32/64位计算机上运行】’micro{gl} - Headers Only C++11 CPU Vector Graphics. no std-lib, no FPU and no GPU required !' GitHub: github.com/micro-gl/micro-gl   

Picture: [5396ee05ly8hi1trjwvibj20sg0aajtk.jpg](https://weibo.cn//mblog/pic/Nk0fS37pZ?rl=1)

Github: [github.com/micro-gl/micro-gl](https://github.com/micro-gl/micro-gl)

#### [【spo4onnx：用于部分优化ONNX模型的简单工具】'spo4onnx - Simple too @#开源#](https://weibo.com/1402400261/Nk09zbkkd)

Note: 【spo4onnx：用于部分优化ONNX模型的简单工具】'spo4onnx - Simple tool for partial optimization of ONNX' Katsuya Hyodo GitHub: github.com/PINTO0309/spo4onnx   

Picture: [5396ee05ly8hi1tbhvurvj20hs08wwee.jpg](https://weibo.cn//mblog/pic/Nk09zbkkd?rl=1)

Github: [github.com/PINTO0309/spo4onnx](https://github.com/PINTO0309/spo4onnx)

#### [【LLama2派生模型的高质量、INT4量化、ONNX版本】Llama-2-7b-hf-onnx-i @网页链接](https://weibo.com/1402400261/NjYm6oedd)

Note: 【LLama2派生模型的高质量、INT4量化、ONNX版本】Llama-2-7b-hf-onnx-int4 Llama-2-7b-chat-hf-onnx-int4 Llama-2-13b-hf-onnx-int4 Llama-2-13b-chat-hf-onnx-int4 Llama-2-70b-hf-onnx-int4 Llama-2-70b-chat-hf-onnx-int4  

Picture: [5396ee05ly8hi1lcwwgljj20z20jwacw.jpg](https://weibo.cn//mblog/pic/NjYm6oedd?rl=1)

#### [【examor：面向学生、学者、面试者和终身学习者，用LLM(语言模型)来辅助学习】'examor  @#开源#](https://weibo.com/1402400261/NjR4Sp0ZZ)

Note: 【examor：面向学生、学者、面试者和终身学习者，用LLM(语言模型)来辅助学习】'examor - For students, scholars, interviewees and lifelong learners. Let LLMs assist you in learning' leyoonafr GitHub: github.com/codeacme17/examor    

Picture: [5396ee05ly8hi0p7we7spj21st0u046b.jpg](https://weibo.cn//mblog/pic/NjR4Sp0ZZ?rl=1)

Github: [github.com/codeacme17/examor](https://github.com/codeacme17/examor)

#### [【苹果的基于Transformer的文本预测模型概览】- 该模型内嵌在macOS系统中，可以在用户输 @网页链接](https://weibo.com/1402400261/NjOaQ0gBI)

Note: 【苹果的基于Transformer的文本预测模型概览】- 该模型内嵌在macOS系统中，可以在用户输入时提供单词预测。   - 模型词表包含大约1.5万个token，包括特殊token、缩写和表情等。   - 模型参数约3400万，隐层大小512，类似GPT-2架构但更小。   - 与GPT-2相比，该模型生成的文本连贯性较差，但预测单词效果不错。   - 相比竞争对手，苹果较晚采用Transformer模型，该模型可能是苹果首个流片式部署的Transformer模型。   《A look at Apple’s new Transformer-powered predictive text model》  

Picture: [5396ee05ly8hi0ccs6m8hj21510u0aga.jpg](https://weibo.cn//mblog/pic/NjOaQ0gBI?rl=1)

#### [【arXiv Paper Recommender：使用LLM(大型语言模型)的能力，可以根据用户需求 @#开源#](https://weibo.com/1402400261/NjHVgzcxr)

Note: 【arXiv Paper Recommender：使用LLM(大型语言模型)的能力，可以根据用户需求自定义筛选论文的方式，包括选择关注的论文类别、作者列表、以及与研究主题相关的提示】'arXiv Paper Recommender' by Hacky Huang GitHub: github.com/Kaffaljidhmah2/Arxiv-Recommender   

Github: [github.com/Kaffaljidhmah2/Arxiv-Recommender](https://github.com/Kaffaljidhmah2/Arxiv-Recommender)

#### [谷歌现已推出「Emoji Kitchen」网页版，可以随意组合两个emoji创造自己的新表情。如果选 @玩Switch的呆呆兽](https://weibo.com/1727858283/NjXSoB7TN)

Note: 谷歌现已推出「Emoji Kitchen」网页版，可以随意组合两个emoji创造自己的新表情。如果选择两个同样的emoji则会诞生强化版[丰收了] 

Picture: [c0448018gy1hi0tvjpr7vj20h908jgn8.jpg](https://weibo.cn//mblog/pic/NjS7N3uXf?rl=1)

#### [这篇论文《从稀疏到密集：在 GPT-4 中使用密度链提示生成摘要 | From Sparse to  @网页链接](https://weibo.com/1727858283/NjXaM1huM)

Note: 这篇论文《从稀疏到密集：在 GPT-4 中使用密度链提示生成摘要 | From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting》，提出了一个改进LLM生成摘要的Prompt。论文地址：原理很有意思：它通过对文章几轮摘要，每一轮都提炼出新的关键词，并根据新的关键词融合、压缩旧版本摘要，一步步提升摘要中信息密度，直到你觉得摘要中信息密度足够为止。详细解释一下Prompt的工作原理：首先要说明一个名词：“missing entity  缺失的实体”“informative entity 信息实体” 指的是能代表文章核心观点的关键词。“missing entity  缺失的实体” 指的是：- 具体并且简洁的关键词- 与文章相关- 新的（之前的摘要中没出现过）- 忠于原文（不能自己编造）- 任意位置（可以在文章的任意位置）然后就是具体在每一轮摘要中的步骤：步骤1：从文章中找出在上一轮摘要中没有出现的1-3条“missing entity”步骤2：基于前面生成的摘要和新找出来的1-3条“missing entity”，重写摘要，加上新的信息实体的细节。最后就是每一轮摘要的指导原则：1. 每一轮摘要的长度保持一致2. 第一份摘要密度最低，由于只有最初的1-3个信息实体，需要加上很多填充词（如 "本文讨论了"）3. 每一轮基于前一轮重写，加上新的信息实体内容，通过融合、压缩和删除 "本文讨论了"等信息量不大的填充词来腾出空间。4. 旧的信息实体的优先级更高，如果空间不足，则减少新实体的数量。总结这个Prompt被命名为CoD（Chain of Density 密度链），还比较形象生动，跟打铁似得，捶打一番再加点新料，继续捶打，直到成型。完整的提示词如下，有兴趣的话可以自己试试：Article: {{ ARTICLE }}You will generate increasingly concise, entity-dense summaries of the above article. Repeat the following 2 steps 5 times. Step 1. Identify 1-3 informative entities (";" delimited) from the article which are missing from the previously generated summary. Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. A missing entity is:- relevant to the main story, - specific yet concise (5 words or fewer), - novel (not in the previous summary), - faithful (present in the article), - anywhere (can be located anywhere in the article).Guidelines:- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., "this article discusses") to reach ~80 words.- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.- Make space with fusion, compression, and removal of uninformative phrases like "the article discusses".- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. - Missing entities can appear anywhere in the new summary.- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. Remember, use the exact same number of words for each summary.Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are "Missing_Entities" and "Denser_Summary".回复:我觉得思路还是靠谱的，感觉微调对生成摘要不会有提升思维链这个方法对gpt4比较有效。同样的描述给3.5，每次输出极其不稳定。如果用国产的大模型，甚至连规则都没法写全就超token了我感觉还是不太行还是要从微调的方向解决这个问题这不就是类似迭代的原理吗

Picture: [66fd066bly8hi1g6hs8dgj21i80u0k2x.jpg](https://weibo.cn//mblog/pic/NjXaM1huM?rl=1)

#### [在家手搓一个CPU！“欢迎来到Pineapple ONE! Pineapple ONE是一个功能（宏 @转发[107]](https://weibo.com/2194035935/NjOQwzIro)

Note: 在家手搓一个CPU！“欢迎来到Pineapple ONE! Pineapple ONE是一个功能（宏）处理器，基于开源架构RISC-V。这种架构现在变得非常流行，并且它是开源的，因此我们选择只使用离散的现成元件来构建CPU。是的，你没听错，没有FPGA或任何微控制器，只有逻辑门和存储器。我们的目标是证明设计现代CPU并不那么困难，因此我们发布了我们的原理图，并将其开源。”详细介绍：pineapple-one.github.io/牛人家是微电路 你这是轧马路 还不如三体里的人体阵列计算机硬核没那么难外壳有圆有方懂了是苹果垃圾桶 

Picture: [82c654dfly1hi0feek142j211w14q4qp.jpg](https://weibo.cn//mblog/pic/NjOQwzIro?rl=1)

#### [这篇论文“Large Language Models for Compiler Optimizati @转发[16]](https://weibo.com/2194035935/NjOPdzChB)

Note: 这篇论文“Large Language Models for Compiler Optimization”探索了大型语言模型在代码优化中的新应用。论文地址：arxiv.org/abs/2309.07062作者提出了一个从头开始训练的7B参数的转换模型，用于优化LLVM汇编的代码大小。该模型将未经优化的汇编作为输入，并输出最佳优化程序的编译器选项列表。“我们在大型测试程序集上进行了评估。我们的方法在减少指令计数方面比编译器提高了3.0％，超过了需要数千次编译的两个最先进的基线测试。此外，该模型表现出惊人的代码推理能力，能够产生91％的可编译代码，并在70％的情况下完美地模拟编译器的输出结果。”

#### [同学们写了一个MathGLM paper，，在数学的数据上训练了一下，只需要20亿参数，数学任务可以 @唐杰THU](https://weibo.com/2194035935/NjJS70O4C)

Note: 同学们写了一个MathGLM paper，，在数学的数据上训练了一下，只需要20亿参数，数学任务可以做得很好，也不需要去overfitting榜单，甚至超过GPT-4。不过。。。。。。怎么说呢，模型一下子变成了一个专用模型，通用能力减弱了。。。。算是一个探索性工作，模型和代码也开源出来了在github上，所有的结果可以重现，希望对大家的研究还有功能能有帮助吧。通用模型和专用模型配合应该能更好的解决问题，毕竟人和人也是要分工的

Picture: [7ebeb44bgy1hhzt4we3w6j20y01cmaqi.jpg](https://weibo.cn//mblog/pic/NjJPVkAtb?rl=1)

#### [在过去的一周里，为了更好的构建 AI Agent 框架 Chocolate Factory（以下简称 @Phodal](https://weibo.com/2194035935/NjIB7wyFk)

Note: 在过去的一周里，为了更好的构建 AI Agent 框架 Chocolate Factory（以下简称 CF），我们加入了一个新的应用：代码库 AI 助手。在设计时，为了更好的在框架底层提供这种能力，我们参阅了 Bloop 应用、LangChain、Spring AI、LlamaIndex 框架等的代码与思想，参考/复制（基于 Apache 2.0 协议） LangChain4j 的一部分 VectorStore 实现。详细见代码库：github.com/unit-mesh/chocolate-factory 。详细文档见： 。 

Github: [github.com/unit-mesh/chocolate-factory](https://github.com/unit-mesh/chocolate-factory)

#### [：IncarnaMind支持多文档对话的开源项目，和其他文档对话项目一样，可以上传PDF、txt文档 @宝玉xp](https://weibo.com/1727858283/NjIcupkRD)

Note: ：IncarnaMind支持多文档对话的开源项目，和其他文档对话项目一样，可以上传PDF、txt文档进行对话，并且支持多文档对话，后台兼容多种大语言模型，例如OpenAI和Anthropic Claude。这个项目是有一些值得借鉴之处的：1. 用LLM将用户的提问转换成独立查询当用户提问后，它先针对问题和历史消息，让LLM对问题重新提炼，拆分成多个独立查询。（参考图2）举例来说：用户提问：“Transformer论文作者是谁？”LLM回复：“Ashish Vaswani，....”用户提问：“那这篇论文和GPT论文有什么区别？”上面的两个用户问题和一个LLM回复提交给LLM后，LLM将其提炼成两个独立的查询：- “论文《Attention is all you need》的内容是什么？”- “论文《GPT》的内容是什么？”这样做的好处就是在做相似度检索时，能过滤掉用户输入问题的无用信息，检索效果更好。2. 用小分块保证尽可能找到更多的相关内容，用大分块保证内容完整性如果是用Embedding做相似度检索，第一步就是要对文档进行分块，分块太大的话检索效果要差一些，分块太小检索出来的内容不完整。所以它在分块时，分的比较小，但在分块的元数据中会保留大块的信息，当做相似度检索时，借助滑块遍历结果时，可以根据小块反向找出大块，这样最终在提交给LLM作为上下文时，可以提交完整的段落，不影响上下文的效果。（参考图3）github.com/junruxiong/IncarnaMind

Picture: [66fd066bgy1hhzk77ac0kj22me1kib29.jpg](https://weibo.cn//mblog/pic/NjHMAm5it?rl=1)

Github: [github.com/junruxiong/IncarnaMind](https://github.com/junruxiong/IncarnaMind)

#### [电子书《C++ Core Guidelines》 C++ 核心指南地址：isocpp.github. @转发[95]](https://weibo.com/2194035935/NjGxBC1OB)

Note: 电子书《C++ Core Guidelines》 C++ 核心指南地址：isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines“本文档的目的是帮助开发人员采用现代C++(当前是C++17),并在代码库中实现更统一的风格。我们并不妄想这些规则中的每一条都能有效地应用于每一个代码库。升级旧系统是困难的。然而,我们确实相信,使用某条规则的程序比不使用该规则的程序更不易出错和更可维护。通常,这些规则也可以使初始开发更快捷/更容易。据我们所知,这些规则会产生性能与使用较老的、更传统技术一样好或更好的代码;它们旨在遵循零开销原则(“你不用的,你就不付出代价”或“当你适当地使用一个抽象机制时,你会获得至少与使用较低级语言构造手工编写一样好的性能”)。请将这些规则视为新代码的理想目标,在处理旧代码时可以利用的机会,并尽可能贴近这些理想目标。”

Picture: [82c654dfly1hhzeq6mm8vj20nb1np10e.jpg](https://weibo.cn//mblog/pic/NjGxBC1OB?rl=1)

#### [【Computer Science from the Bottom Up】https:///www. @转发[27]](https://weibo.com/1715118170/Njt2Md9ok)

Note: 【Computer Science from the Bottom Up】https:///www.bottomupcs.com 自下而上的计算机科学。PDF格式下载地址：https:///www.bottomupcs.com/csbu.pdf 

Picture: [663aa05aly1hhxr4zhys0j20hm0oyq5i.jpg](https://weibo.cn//mblog/pic/Njt2Md9ok?rl=1)

#### [【头1%工程师的七个简单习惯】1、使用一致的编码标准和风格，使代码易读易维护。   2、编写整洁、简 @网页链接](https://weibo.com/1402400261/NjAXixfQG)

Note: 【头1%工程师的七个简单习惯】1、使用一致的编码标准和风格，使代码易读易维护。   2、编写整洁、简单、逻辑清晰的代码，命名遵循明确的约定。   3、编写可预测的代码，遵循编码原则，编写合适的测试。   4、经常沟通反馈，接受新的观点改进代码。   5、不迷恋自己的代码，必要时果断重构。   6、速度要快，但代码要慢，花时间使用标准、规范测试、使用原则并经常沟通，长远来看，可以节省更多时间。   7、编写可读的代码，不能仅仅满足计算机，还要考虑其他人。   明智灵活运用以上规则，不要墨守成规。     《7 simple habits of the top 1% of engineers》  这1%应该是最容易被裁的，还能不能找到饭吃都不一定，不搞点屎山出来证明自己不可取代的价值，那不是等着年龄一到就被换。

Picture: [5396ee05ly8hhypxryrd7j20qp0qa0v2.jpg](https://weibo.cn//mblog/pic/NjAXixfQG?rl=1)

#### [【大语言模型课程notebooks集】’Large Language Model Course -  @爱可可-爱生活](https://weibo.com/1402400261/NjzPK8MoE)

Note: 【大语言模型课程notebooks集】’Large Language Model Course - Course with a roadmap and notebooks to get into Large Language Models (LLMs).' Maxime Labonne GitHub: github.com/mlabonne/llm-course   

Picture: [5396ee05ly8hhykgs7azbj213d0u0wk0.jpg](https://weibo.cn//mblog/pic/NjzGu1sT4?rl=1)

Github: [github.com/mlabonne/llm-course](https://github.com/mlabonne/llm-course)

#### [【edX和Databricks Academy的大型语言模型课程资料和幻灯片】'Large Lang @#开源#](https://weibo.com/1402400261/NjzFZgn96)

Note: 【edX和Databricks Academy的大型语言模型课程资料和幻灯片】'Large Language Models: Foundation Models from the Ground Up' by Databricks Academy GitHub: github.com/databricks-academy/llm-foundation-models   

Picture: [5396ee05ly8hhykffe6kqj21c40aitba.jpg](https://weibo.cn//mblog/pic/NjzFZgn96?rl=1)

Github: [github.com/databricks-academy/llm-foundation-models](https://github.com/databricks-academy/llm-foundation-models)

#### [【Auto-News: 自动新闻聚合器，结合了多种信息来源和LLM(ChatGPT)以帮助高效阅读并 @#开源#](https://weibo.com/1402400261/NjzE9iiWE)

Note: 【Auto-News: 自动新闻聚合器，结合了多种信息来源和LLM(ChatGPT)以帮助高效阅读并减少噪音，信息来源包括Tweets、RSS、YouTube、Web文章、Reddit和日记笔记等】’Auto-News: An Automatic News Aggregator with LLM - A personal news aggregator to pull information from multi-sources + LLM (ChatGPT via LangChain) to help us reading efficiently with less noises, the sources including: Tweets, RSS, YouTube, Web Articles, Reddit, and personal Journal notes.' Yuzhang Hu GitHub: github.com/finaldie/auto-news 

Picture: [5396ee05ly1hhyka5t3m7j21qc0mf4bw.jpg](https://weibo.cn//mblog/pic/NjzE9iiWE?rl=1)

Github: [github.com/finaldie/auto-news](https://github.com/finaldie/auto-news)

#### [【NanoSAM：可实时运行的SAM蒸馏版模型，基于NVIDIA TensorRT】’NanoSAM @#开源#](https://weibo.com/1402400261/NjoJRxdLW)

Note: 【NanoSAM：可实时运行的SAM蒸馏版模型，基于NVIDIA TensorRT】’NanoSAM - A distilled Segment Anything (SAM) model capable of running real-time with NVIDIA TensorRT' NVIDIA-AI-IOT GitHub: github.com/NVIDIA-AI-IOT/nanosam   

Picture: [5396ee05ly8hhx84s1ysnj20pv0gk76a.jpg](https://weibo.cn//mblog/pic/NjoJRxdLW?rl=1)

Github: [github.com/NVIDIA-AI-IOT/nanosam](https://github.com/NVIDIA-AI-IOT/nanosam)

#### [：chathn一个Hacknews的智能聊天机器人，可以在聊天窗口中和Hacknews交互，例如：- @#AI开源项目推荐#](https://weibo.com/1727858283/NjCv55Tc4)

Note: ：chathn一个Hacknews的智能聊天机器人，可以在聊天窗口中和Hacknews交互，例如：- 用 Markdown 表格格式为我提供 Hacker News 上排名前 5 的新闻。- 总结 Hacker News 新闻中的评论。- 现在 Hacker News 上最热门的新闻是什么？它背后实现是OpenAI的Function Calling，后台基于Hacknews的API实现了若干功能（例如：get_top_stories、get_story、get_story_with_comments、summarize_top_story），然后根据用户输入的消息内容决定是不是需要调用这些功能。很好的Function Calling应用案例！github.com/steven-tey/chathn 反过来，各个公司其实也可以集成 ChatGPT API 到自己的业务内部，让一切数据都可以通过自然对话而方便使用。这意味着，任何人都可以编程了，只要用自然语言描述自己的需求（最好是一步一步的）就好了。和open interpreter有点像

Github: [github.com/steven-tey/chathn](https://github.com/steven-tey/chathn)

#### [5分钟经典英文技术演讲系列博文5分钟经典英文技术演讲1：如何快速掌握新技术 - Kathy Sier @蚁工厂](https://weibo.com/2194035935/NjhqKydWc)

Note: 5分钟经典英文技术演讲系列博文5分钟经典英文技术演讲1：如何快速掌握新技术 - Kathy Sierra地址：decodezp.github.io/2018/12/12/eng-talk1-fast-learn/ 无论是谁，以有限的精力来面对层出不穷的新技术挑战都是不够的。你需要学会一套方法论来帮助你快速习得新的技能。5分钟经典英文技术演讲2：软件设计真正的精髓-Scott Meyer，《Effective C++》的作者地址：decodezp.github.io/2018/12/21/eng-talk2-things-matter/成功的软件产品都有其共性。在Scott Meyer看来，这些共性由几个要素组成。在你的作品中考虑这些要素，将帮助你掌握软件设计真正的精髓。5分钟经典英文技术演讲3：如何应对信息过载并提高生产效率-Scott Hanselman 地址：decodezp.github.io/2019/06/19/eng-talk3-scaling-yourself/信息过载的时代，能否找到一种在信息洪流中独善其身的方法，让我们专注于真正重要事情？当生活/工作的范畴一再扩大，我们也需要延展自身的边界，在充分利用信息的同时，保持持续的生产效率。

Picture: [82c654dfly1h65y3yxue2j20gc0fnjsj.jpg](https://weibo.cn//mblog/pic/M5H2qCzBT?rl=1)

#### [翻看一位领域专家的公众号，能看出的心路历程：1，先研究benchmark机制（怎么才算好？）2，研究 @转发[2]](https://weibo.com/2144454703/NjdxSBob0)

Note: 翻看一位领域专家的公众号，能看出的心路历程：1，先研究benchmark机制（怎么才算好？）2，研究竞对架构（为什么人家能做到更好）3，研究编译器（有什么神器能解决软件生态问题，让产品广泛落地）这可以是DSA的必经三部曲 dsa是什么意思回复:+1想知道公众号的名字这是哪个公众号

#### [Linux kernel map地址：makelinux.github.io/kernel/map/ @转发[149]](https://weibo.com/2194035935/Njdb4Evsa)

Note: Linux kernel map地址：makelinux.github.io/kernel/map/交互式Linux内核地图可帮助您在探索其源代码的同时遍历内核子系统之间的复杂互联。该地图描绘了400多个重要函数和结构,这些函数和结构被划分为几个主要子系统。您可以放大任何函数并以图形方式移动这些函数。所有函数之间的关系通过连接线显示,点击任何函数都会将您带到Linux交叉引用和Linux内核文档集合中的源代码。厉害了昨天刚在hack news看到

Picture: [82c654dfly1hhvt220ckoj21tv18mnpd.jpg](https://weibo.cn//mblog/pic/Njdb4Evsa?rl=1)

#### [中午休息期间写了篇技术方案：订单在提交的时候会面临不同的校验规则，不同的校验规则会有不同的处理。假设 @写代码的铲屎官](https://weibo.com/2194035935/Nj7LB2Y54)

Note: 中午休息期间写了篇技术方案：订单在提交的时候会面临不同的校验规则，不同的校验规则会有不同的处理。假设这个处理就是弹窗。有的时候会命中规则1，则弹窗1，有的时候同时命中规则1、2、3，但由于存在规则的优先级，则会处理优先级最高的弹窗1。老的业务背景下，弹窗优先级或者说校验规则是统一的。直接用函数翻译实现，写多个 if 问题不大。但在新业务背景下，不同的条件，弹窗优先级不一致，之前的写法需要写大量的嵌套判断，代码难以维护。所以问题抽象为：如何设计一个校验器？责任链设计模式与工厂设计模式的权衡github.com/FantasticLBP/knowledge-kit/blob/master/Chapter1%20-%20iOS/1.110.md 策略模式：当需要根据客户端的条件选择算法、策略时，可用该模式，客户端会根据条件选择合适的算法或策略，并将其传递给使用它的对象（前端Vue-Validator form 各种 rules） 职责链模式：当需要根据请求的内容选择处理器时，可用该模式，请求会沿着链传递，直到被处理(Node洋葱模型 //:转发微博

Picture: [be616557gy1hhv54ypooyj21g60u0at5.jpg](https://weibo.cn//mblog/pic/Nj7KYE4Nb?rl=1)

Github: [github.com/FantasticLBP/knowledge-kit/blob/master/Chapter1](https://github.com/FantasticLBP/knowledge-kit/blob/master/Chapter1)

#### [电子书 《Practical Guide to Bare Metal C++》 裸机 C++ 实用指 @转发[103]](https://weibo.com/2194035935/Nj3IMacye)

Note: 电子书 《Practical Guide to Bare Metal C++》 裸机 C++ 实用指南地址：arobenko.github.io/bare_metal_cpp/作者 “每当我遇到C++是否适合嵌入式开发,尤其是裸机开发的问题时,我就会对此进行思考。有许多文章阐述C++比C更优越,凡是能用C实现的,用C++都可以实现,还有很多额外的功能,即使是裸机开发也应该使用C++。然而,我没有找到多少关于如何利用C++的优势来提高开发流程,相比传统的使用“C”编程语言的方法而言的实用指南或教程。通过这本书,我希望解释并展示在嵌入式裸机开发中,如何实现软实时系统,而无需优先中断和复杂的实时任务调度。希望它可以帮助有人开始在嵌入式裸机开发中使用C++。本书的主要目标受众是想要更好地理解裸机开发的专业C++开发人员,了解如何在嵌入式环境中使用自己喜欢的编程语言,并可能将C++技能提升到“专家”级别。为什么是专业级的?因为裸机平台有许多限制。在大多数情况下,不会有异常和运行时类型信息(RTTI)支持。在许多情况下,动态内存分配也会被排除在外。为了能够有效地使用C++,您需要对现有的C++惯用法、构造和STL内容有深入的了解。您必须知道自己喜欢的数据结构是如何实现的,以及是否可以在自己的环境中重用它们。如果无法“原封不动”地使用STL(或任何其他库),您将不得不实现它的精简版本,最好知道库开发人员是如何实现该功能的,以及如何使其在您环境的限制下工作。”机翻回复:成功保存到你的Notion若无需要，没必要强行用。linux内核都要引入rust了。

Picture: [82c654dfly1hhund8v8brj20ln1nfh0s.jpg](https://weibo.cn//mblog/pic/Nj3IMacye?rl=1)

#### [nndeploy是一款最新上线的支持多平台、简单易用、高性能的机器学习部署框架，一套实现可在多端(云 @转发[60]](https://weibo.com/2194035935/Nj3Gu2lRc)

Note: nndeploy是一款最新上线的支持多平台、简单易用、高性能的机器学习部署框架，一套实现可在多端(云、边、端)完成模型的高性能部署。地址：github.com/Alwaysssssss/nndeploy“作为一个多平台模型部署工具，我们的框架最大的宗旨就是简单贴心(^‹^)，目前nndeploy已完成TensorRT、OpenVINO、ONNXRuntime、MNN、TNN、NCNN六个业界知名的推理框架的继承，后续会继续接入tf-lite、paddle-lite、coreML、TVM、AITemplate，在我们的框架下可使用一套代码轻松切换不同的推理后端进行推理，且不用担心部署框架对推理框架的抽象而带来的性能损失。”下面的 runtime 也是一个类似于 nndeploy 的框架么？

Picture: [82c654dfly1hhun7j1d54j20vk0sbnd9.jpg](https://weibo.cn//mblog/pic/Nj3Gu2lRc?rl=1)

Github: [github.com/Alwaysssssss/nndeploy](https://github.com/Alwaysssssss/nndeploy)

#### [【C++ UML diagram generator based on Clang：自动 C++ 到 @#开源#](https://weibo.com/1402400261/Nj5Q7APi7)

Note: 【C++ UML diagram generator based on Clang：自动 C++ 到 UML 类、序列、包和包含图生成器，由 YAML 配置文件驱动】'C++ UML diagram generator based on Clang - Customizable automatic UML diagram generator for C++ based on Clang.' Bartek Kryza GitHub: github.com/bkryza/clang-uml  

Picture: [5396ee05ly8hhuwpub5dij20u30u0dl0.jpg](https://weibo.cn//mblog/pic/Nj5Q7APi7?rl=1)

Github: [github.com/bkryza/clang-uml](https://github.com/bkryza/clang-uml)

#### [【ExLlamaV2：用于在现代消费级 GPU 上运行本地 LLM 的推理库】'ExLlamaV2  @#开源#](https://weibo.com/1402400261/Nj5GUoDcU)

Note: 【ExLlamaV2：用于在现代消费级 GPU 上运行本地 LLM 的推理库】'ExLlamaV2 - A fast inference library for running LLMs locally on modern consumer-class GPUs' turboderp GitHub: github.com/turboderp/exllamav2     

Picture: [5396ee05ly8hhuw1dk65sj21co0kagp0.jpg](https://weibo.cn//mblog/pic/Nj5GUoDcU?rl=1)

Github: [github.com/turboderp/exllamav2](https://github.com/turboderp/exllamav2)

#### [【Huggingface Transformers量化方案概览】 1. 量化模型主要用于两个目的：在 @网页链接](https://weibo.com/1402400261/Nj3voou4h)

Note: 【Huggingface Transformers量化方案概览】 1. 量化模型主要用于两个目的：在较小的设备上运行大模型的推理；在量化模型的基础上微调adapter。    2. Huggingface Transformers中目前主要支持两种量化方案：bitsandbytes和auto-gptq。    3. bitsandbytes的优势：     - 使用简单，可以开箱即用对任何包含torch.nn.Linear模块的模型进行量化。       - 跨模态兼容性好，任意包含torch.nn.Linear的模型都可以直接量化，如Whisper、ViT、Blip2等。       - 与adapter合并后性能不降低。    4. auto-gptq的优势：       - 对文本生成速度快。       - 支持2-bit量化，但可能精度损失严重。       - 可以序列化任意bit数的模型。       - 支持AMD GPU。     5. bitsandbytes的改进空间：       - 对文本生成速度较慢。       - 4-bit模型目前不支持序列化。     6. auto-gptq的改进空间：       - 需要校准数据集，可能会让部分用户望而却步。       - 目前只支持语言模型量化。     7. 基准测试显示bitsandbytes更适合微调，auto-gptq更适合生成。     8. 一种获取更好合并模型的方法：       - 用bitsandbytes量化基模型       - 微调adapter       - 将adapter合并到基模型或反量化模型       - 用auto-gptq量化合并后的模型用于部署     9. 量化导致的性能下降很小，大模型下降更小。     10. 整体而言，两种量化方案各有优势，可以根据实际使用场景选择。    《Overview of natively supported quantization schemes in 🤗 Transformers》  

Picture: [5396ee05ly8hhum3r5o9cj20u00xuwj7.jpg](https://weibo.cn//mblog/pic/Nj3voou4h?rl=1)

#### [【Megatron-LLaMA: 在 Megatron-LM 上训练 LLaMA 模型的最佳实践，M @#开源#](https://weibo.com/1402400261/NiZ7LrY2f)

Note: 【Megatron-LLaMA: 在 Megatron-LM 上训练 LLaMA 模型的最佳实践，Megatron-LLaMA是阿里巴巴内部优化的LLaMA训练框架，可用来轻松、快速和经济的训练自己的LLaMA】'Megatron-LLaMA: Easy, Fast and Affordable Training of Your Own LLaMA - Best practice for training LLaMA models in Megatron-LM' Alibaba GitHub: github.com/alibaba/Megatron-LLaMA  

Picture: [5396ee05ly8hhu30jg5lyj22or0u0agi.jpg](https://weibo.cn//mblog/pic/NiZ7LrY2f?rl=1)

Github: [github.com/alibaba/Megatron-LLaMA](https://github.com/alibaba/Megatron-LLaMA)

#### [电子书《The Pinouts Book》引脚书地址：pinouts.org/引脚图书是一本面向设计 @转发[56]](https://weibo.com/2194035935/NiUifio7b)

Note: 电子书《The Pinouts Book》引脚书地址：pinouts.org/引脚图书是一本面向设计师和工程师的免费数字图书,它可以作为您电子项目中不同引脚功能的快速参考。这本书涵盖了130个常用组件(查看列表),比如连接器、单板计算机、开发板、存储卡、微控制器芯片等等。如果您需要更多技术信息,每页的顶部都有一个链接(例如 pinouts.org/XXX),这些链接会重定向到官方数据手册/规范。 

Picture: [82c654dfly1hhthos984xj21an1bj7n9.jpg](https://weibo.cn//mblog/pic/NiUifio7b?rl=1)

#### [电子书《Computer Science from the Bottom Up》自下而上的计算机科学 @蚁工厂](https://weibo.com/2194035935/NiPmWwKjK)

Note: 电子书《Computer Science from the Bottom Up》自下而上的计算机科学一般的计算机科学是从“自上而下”教授的;应用程序,高级编程,软件设计和开发理论。学生可能会浅显地接触二进制,希望是二进制逻辑,可能甚至是一些低级概念,比如寄存器,操作码等。而这本书的目的是以完全相反的方向工作,从操作系统基础知识开始,直到那些应用程序是如何编译和执行的。

Picture: [82c654dfly1hhsqxj7kp0j21bt0z61jv.jpg](https://weibo.cn//mblog/pic/NiP3XCe2m?rl=1)

#### [  全国六强“差分进化优化AC自动机”团队决赛答辩分享团队名称：差分进化优化AC自动机团队成员：陈子 @数据派THU](https://weibo.com/6004911042/NiL3P7rkV)

Note:   全国六强“差分进化优化AC自动机”团队决赛答辩分享团队名称：差分进化优化AC自动机团队成员：陈子皓（）白来敏（）李浩荣（）团队名次：全国第一名  想你所想，有视频，有PPT

#### [【blip-caption：用Salesforce BLIP实现的命令行图像描述生成工具】’blip @#开源#](https://weibo.com/1402400261/NiEZfnprC)

Note: 【blip-caption：用Salesforce BLIP实现的命令行图像描述生成工具】’blip-caption - Generate captions for images with Salesforce BLIP' Simon Willison GitHub: github.com/simonw/blip-caption   

Picture: [5396ee05ly8hhrlysexnqj21c20oagp3.jpg](https://weibo.cn//mblog/pic/NiEZfnprC?rl=1)

Github: [github.com/simonw/blip-caption](https://github.com/simonw/blip-caption)

#### [【FastEmbed：易用的轻量、快速的 Python 库，专为检索嵌入生成而构建】'FastEmb @#开源#](https://weibo.com/1402400261/NiEPmmqqT)

Note: 【FastEmbed：易用的轻量、快速的 Python 库，专为检索嵌入生成而构建】'FastEmbed - Fast, Accurate, Lightweight Python library to make State of the Art Embedding' Qdrant GitHub: github.com/qdrant/fastembed   

Picture: [5396ee05ly8hhrlgasymzj218l0u0wje.jpg](https://weibo.cn//mblog/pic/NiEPmmqqT?rl=1)

Github: [github.com/qdrant/fastembed](https://github.com/qdrant/fastembed)

#### [【Llama2.c L2E LLM – Multi OS Binary and Unikernel  @转发[1]](https://weibo.com/1715118170/Nilz0DhaM)

Note: 【Llama2.c L2E LLM – Multi OS Binary and Unikernel Release】https:///github.com/trholding/llama2.c/releases/tag/L2E_v0.1 Llama2.c L2E LLM – 多操作系统二进制和 Unikernel 版本。 

Github: [github.com/trholding/llama2.c/releases/tag/L2E_v0.1](https://github.com/trholding/llama2.c/releases/tag/L2E_v0.1)

#### [【Awesome-LLM-Eval：一份精选的工具、演示、论文和文档清单，用于评估类似ChatGPT @#开源#](https://weibo.com/1402400261/NiufRqWN7)

Note: 【Awesome-LLM-Eval：一份精选的工具、演示、论文和文档清单，用于评估类似ChatGPT、LLaMA和GLM这样的大型语言模型】'Awesome-LLM-Eval - Awesome-LLM-Eval: a curated list of tools, demos, papers, docs for Evaluation on Large Language Models like ChatGPT, LLaMA, GLM' JUN GitHub: github.com/onejune2018/Awesome-LLM-Eval   回复:谢谢！回复:powerpoint就可以画，网上搜教程就可以Awesome请问这个图用什么工具画的？

Picture: [5396ee05ly8hhqarq65mhj20wj0q2dkw.jpg](https://weibo.cn//mblog/pic/NiufRqWN7?rl=1)

Github: [github.com/onejune2018/Awesome-LLM-Eval](https://github.com/onejune2018/Awesome-LLM-Eval)

#### [【Awesome Domain LLM：收集和梳理垂直领域的开源大语言模型、数据集及评测基准】'Aw @#开源#](https://weibo.com/1402400261/Niu6m1aVD)

Note: 【Awesome Domain LLM：收集和梳理垂直领域的开源大语言模型、数据集及评测基准】'Awesome Domain LLM - 收集和梳理垂直领域的开源模型、数据集及评测基准。' luban-agi GitHub: github.com/luban-agi/Awesome-Domain-LLM     

Picture: [5396ee05ly8hhq9xus0hjj21440u0tco.jpg](https://weibo.cn//mblog/pic/Niu6m1aVD?rl=1)

Github: [github.com/luban-agi/Awesome-Domain-LLM](https://github.com/luban-agi/Awesome-Domain-LLM)

#### [别说算法导论了，当你工作几年就会明白，，以下几个任何一个都可以超过90%程序员：1.把事情想明白，说 @不能度己何能渡人](https://weibo.com/2194035935/Nimy51e1U)

Note: 别说算法导论了，当你工作几年就会明白，，以下几个任何一个都可以超过90%程序员：1.把事情想明白，说清楚，跟别人商量好2.写代码，注意边界条件和编码规范，写单测，基本做到无bug提测3.工作中做好计划和进度跟踪，沟通和汇报，不把问题遗留到变成事故4.思考和分析，如何优化目前的工作流程，引入工具和方法，提升生产效率5.把自己工作中用到的技术用熟，搞清楚原理，优点短处，适用场景6.不断接触新技术思想和工具，完善自身知识体系结构7.深入学习至少一个常用开源项目，源码层面系统掌握这项技术8.持续坚持学习和技术内容输出，每个星期产出2篇原创技术文章这个人可能把“原创”的质量想得太低了，技术文章的原创性、数量、质量不可能三角。下午五点提需求，明天上线。收到了，像这个方向努力。谢谢//:确实

#### [【认知负荷(Cognitive Load)开发人员手册】'Cognitive Load Develo @#开源#](https://weibo.com/1402400261/NikuNx1IU)

Note: 【认知负荷(Cognitive Load)开发人员手册】'Cognitive Load Developer's Handbook' Artem Zakirullin GitHub: github.com/zakirullin/cognitive-load   

Picture: [5396ee05ly8hhp3ntscf2j214n0u00x5.jpg](https://weibo.cn//mblog/pic/NikuNx1IU?rl=1)

Github: [github.com/zakirullin/cognitive-load](https://github.com/zakirullin/cognitive-load)

#### [【Falcon 180B：最新发布的180B大语言模型，目前在预训练模型Open LLM排行榜排名第 @网页链接](https://weibo.com/1402400261/NiiUubU9v)

Note: 【Falcon 180B：最新发布的180B大语言模型，目前在预训练模型Open LLM排行榜排名第一】《tiiuae/falcon-180B · Hugging Face》  Demo:   

Picture: [5396ee05ly8hhovgm2jz6j20u00u043v.jpg](https://weibo.cn//mblog/pic/NiiUubU9v?rl=1)

#### [【BMF：跨平台、可定制多媒体/视频处理框架，支持强大的GPU加速、异构设计、多语言支持、易于使用、 @#开源#](https://weibo.com/1402400261/NidG8s01P)

Note: 【BMF：跨平台、可定制多媒体/视频处理框架，支持强大的GPU加速、异构设计、多语言支持、易于使用、兼容多框架和高性能等特点，非常适合转码、AI推理、算法集成、视频直播等】'BMF - Cross-platform, customizable video processing framework with strong GPU acceleration' BabitMF GitHub: github.com/BabitMF/bmf 

Picture: [5396ee05ly1hho9m827jpg20hs0dce82.gif](https://weibo.cn//mblog/pic/NidG8s01P?rl=1)

Github: [github.com/BabitMF/bmf](https://github.com/BabitMF/bmf)

#### [【TomatoBar：超简洁的 macOS 菜单栏番茄时钟】’TomatoBar - World's @#开源#](https://weibo.com/1402400261/NidmW1uv4)

Note: 【TomatoBar：超简洁的 macOS 菜单栏番茄时钟】’TomatoBar - World's neatest Pomodoro timer for macOS menu bar' Ilya Voronin GitHub: github.com/ivoronin/TomatoBar  

Picture: [5396ee05ly8hho87qdsgaj20j00hqjsa.jpg](https://weibo.cn//mblog/pic/NidmW1uv4?rl=1)

Github: [github.com/ivoronin/TomatoBar](https://github.com/ivoronin/TomatoBar)

#### [【comgra：一个与 pytorch 一起使用的库，可以更轻松地检查神经网络的内部结构，允许可视化 @#开源#](https://weibo.com/1402400261/NidbVoZUN)

Note: 【comgra：一个与 pytorch 一起使用的库，可以更轻松地检查神经网络的内部结构，允许可视化计算图并检查不同时间点的各张量的值】’comgra - a library for use with pytorch that makes it easier to inspect the internals of your neural networks' by Florian Dietz GitHub: github.com/FlorianDietz/comgra  

Picture: [5396ee05ly8hho7egvuxcj21gk0pzad3.jpg](https://weibo.cn//mblog/pic/NidbVoZUN?rl=1)

Github: [github.com/FlorianDietz/comgra](https://github.com/FlorianDietz/comgra)

#### [使用纯C/C++进行Meta的 Segment Anything（分割一切）模型推理地址：githu @蚁工厂的微博视频](https://weibo.com/2194035935/NiiAbFEzu)

Note: 使用纯C/C++进行Meta的 Segment Anything（分割一切）模型推理地址：github.com/YavorGIvanov/sam.cpp  回复:已保存到notion回复:已保存到你的notion

Github: [github.com/YavorGIvanov/sam.cpp](https://github.com/YavorGIvanov/sam.cpp)

#### [【eza：现代风ls替代品】’eza - A modern, maintained replacem @爱可可-爱生活](https://weibo.com/2194035935/NieeTaHth)

Note: 【eza：现代风ls替代品】’eza - A modern, maintained replacement for ls' eza GitHub: github.com/eza-community/eza  

Picture: [5396ee05ly8hho9acifwzj21jr0u0gtg.jpg](https://weibo.cn//mblog/pic/NidBU9QVQ?rl=1)

Github: [github.com/eza-community/eza](https://github.com/eza-community/eza)

#### [Enhance Your English Writing 👊地址：github.com/yzy199 @蚁工厂](https://weibo.com/2194035935/NidMgbFBr)

Note: Enhance Your English Writing 👊地址：github.com/yzy1996/English-Writing这个Github项目是作者在阅读论文的过程中收集整理的一些好的英语用法，希望能帮助到你的写作。 

Picture: [82c654dfly1h5xtgkl7rlj20ol0hj0un.jpg](https://weibo.cn//mblog/pic/M4CiUxpns?rl=1)

Github: [github.com/yzy1996/English-Writing](https://github.com/yzy1996/English-Writing)