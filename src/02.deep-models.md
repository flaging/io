# 深度学习模型与算法

模型, 应用和解决方案.

## CV

dinov2
* 图像子监督对比训练模型
* [github](https://github.com/facebookresearch/dinov2), [haai](https://hub.baai.ac.cn/view/25489), [aijishu](https://aijishu.com/a/1060000000398895)

dino
* [wechat](https://mp.weixin.qq.com/s?__biz=MzU1MzY0MDI2NA==&mid=2247499393&idx=1&sn=83e6eecb11e4dc03dde296d06d689e3c&chksm=fbed76a6cc9affb0095fa04452a111fd483720980208fc6edd09c0bd8f207e7f94d6c151a1cb&scene=21#wechat_redirect), [github](https://github.com/facebookresearch/dino), [arxiv](https://arxiv.org/abs/2104.14294)

## NLP

[From deep to long learning ?](https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning)
* [hacker news](https://news.ycombinator.com/item?id=35502187)对此进行了详细的讨论

[深度学习进阶篇-预训练模型\[1\]：预训练分词Subword、ELMo、Transformer模型原理](https://aijishu.com/a/1060000000408210)

[深度学习进阶篇-预训练模型\[2\]：Transformer-XL、GPT原理、模型结构等详细讲解](https://aijishu.com/a/1060000000408421)

[深度学习进阶篇-预训练模型[3]：XLNet、BERT、GPT,ELMO的区别优缺点等原理详解](https://aijishu.com/a/1060000000408630)

[深度学习进阶篇-预训练模型\[4\]：RoBERTa、ALBERT、ELECTRA算法原理应用等详解](https://aijishu.com/a/1060000000408669)

[深度学习进阶篇-国内预训练模型\[5\]：ERINE、ERNIE 3.0、ERNIE-的设计思路等详解](https://aijishu.com/a/1060000000408803)

## AUDIO

[whisper](https://github.com/openai/whisper)
* [whisper.cpp](https://github.com/ggerganov/whisper.cpp): whisper模型的C++实现
* [Whisper JAX](https://github.com/sanchit-gandhi/whisper-jax): jax版本实现和加速
  * 加速内容: 音频分段批处理(7倍), 使用jax jit(2倍), 使用TPU替换GPU(5倍), 共70倍
* [Whisper](https://github.com/Const-me/Whisper): whisper模型的Windows客户端, 支持GPU, [小众软件](https://www.appinn.com/const-me-whisper/)推荐
* [openai-whisper-cpu](https://github.com/MiscellaneousStuff/openai-whisper-cpu): CPU量化提升吞吐
* [whisperX](https://github.com/m-bain/whisperX): 提升时间戳精准度
* [whisper_real_time](https://github.com/davabase/whisper_real_time): 实时调用音频转换成字符的demo
* [WAAS](https://github.com/schibsted/WAAS): whisper as a service
* [whisper openvino](https://github.com/zhuzilin/whisper-openvino): whisper openvino优化版本
* [faster-whisper](https://github.com/guillaumekln/faster-whisper): 基于CTranslate2的whisper实现
* [huggingface finetune whisper](https://github.com/huggingface/blog/blob/main/fine-tune-whisper.md): huggingface官方的whisper finetune博客
* [AWS finetune whisper](https://aws.amazon.com/cn/blogs/china/fine-tuning-and-deploying-whisper-models-with-sagemaker/): AWS finetune博客, 示例在[github](https://github.com/aws-samples/amazon-sagemaker-fine-tune-and-deploy-wav2vec2-huggingface)
* [LanguageLeapAI](https://github.com/SociallyIneptWeeb/LanguageLeapAI): 基于whisper的翻译助理

[AudioGPT](https://www.aminer.cn/pub/6448967c71ac66d2cbd88151/audiogpt-understanding-and-generating-speech-music-sound-and-talking-head?f=wb): 论文[AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head](https://www.aminer.cn/pub/6448967c71ac66d2cbd88151/audiogpt-understanding-and-generating-speech-music-sound-and-talking-head?f=wb), 代码在[github](https://github.com/AIGC-Audio/AudioGPT)

## RECO

[Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited](https://arxiv.org/abs/2303.13835)
* 推荐系统能不能将ID细化

[KDD 2023 | 美团技术团队精选论文解读](https://tech.meituan.com/2023/08/11/meituan-kdd-2023.html): 美团23年推荐相关论文简介

[如何利用「深度上下文兴趣网络」提升点击率？](https://tech.meituan.com/2023/11/09/how-to-model-context-information-in-deep-interest-network.html): 美团推荐网络

## LLMs

[makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch): 博客, MoE LLM的一种实现, [github](https://github.com/AviSoori1x/makeMoE)

[AWQ vs SpQR: 量化又见量化](https://zhuanlan.zhihu.com/p/638514540): 知乎解析, AWQ和SoQR, 保护关键的1%参数不变

[AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://github.com/mit-han-lab/llm-awq): llama, int3/4量化推理工具

[TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385): 小型化llama

[llama-mistral](https://github.com/dzhulgakov/llama-mistral): mistral实现代码

[How to Fine-Tune CLIP Model with Custom Data](https://www.ai-contentlab.com/2023/11/how-to-fine-tune-clip-model-with-custom.html): finetune示例代码

[Implementation of Q-Learning with Python](https://www.ai-contentlab.com/2023/11/implementation-of-q-learning-with-python.html): Q-learning示例代码

[Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms): finetune建议

[StripedHyena-Nous-7B](https://github.com/togethercomputer/stripedhyena): 7B模型的高效实现

[论文: WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia](https://arxiv.org/abs/2305.14292): 减少模型的幻觉

[phi-2x2_8](https://huggingface.co/mlabonne/phixtral-2x2_8): phi-2的2x8的MoE模型

[phi-4x2_8](https://huggingface.co/mlabonne/phixtral-4x2_8): phi-2的4x8 MoE模型

[phi-2](https://huggingface.co/microsoft/phi-2): 微软用教科书训练的2.7B模型

[air-llm](https://github.com/lyogavin/Anima/tree/main/air_llm): 层级推理/flash attention, 实现4GB跑70B模型

[SwiftInfer](https://github.com/hpcaitech/SwiftInfer): streaming-llm的框架, streaming的意思是将首token和滑动窗口结合,实现超长上下文.

[RAG方法介绍](https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b): 其中RAG介绍的图在[medium](https://d3ddy8balm3goa.cloudfront.net/llamaindex/rag-cheat-sheet-final.svg)上

[论文:Optimizing Distributed Training on Frontier for Large Language Models](https://arxiv.org/abs/2312.12705): 基于AMD MI250X的大模型训练优化论文

[涓海录AIGC学习文档](https://qwu28uf3j5.feishu.cn/docx/Ofp1dBDp2oH1nGxTmk9cRZDvnQe): AIGC飞书文档

[calm](https://github.com/zeux/calm/tree/main):基于C的推理框架

[Mixtral of Experts](https://arxiv.org/abs/2401.04088): SMoE, [news](https://mistral.ai/news/mixtral-of-experts/)

[mixtral-offloading](https://github.com/dvmazur/mixtral-offloading): mixtral加速框架

[LiteLlama-460M-1T](https://huggingface.co/ahxt/LiteLlama-460M-1T): 使用460M参数和1Ttoken训练的小型LLama

[LLaVA](https://llava-vl.github.io/)
* 多模态大模型,开源了数据集,模型和性能
* [github](https://github.com/haotian-liu/LLaVA), [arxiv](https://arxiv.org/abs/2304.08485), [haai](https://hub.baai.ac.cn/view/25839)

[minGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)
* [github.io](https://minigpt-4.github.io/), [githubdaily](https://githubdaily.gitee.io/posts/2023-04-17-minigpt-4/), [haai](https://hub.baai.ac.cn/view/25493), [jack cui](https://cuijiahua.com/blog/2023/04/ai-32.html), [paper with code](https://paperswithcode.com/paper/minigpt-4-enhancing-vision-language), [arxiv](https://arxiv.org/abs/2304.10592)
* [MiniGPT-4实现原理及其核心BLIP2模型实践：从代表性图文对数据集、BLIP2模型结构到调用实践](https://mp.weixin.qq.com/s/riPR6dAvhlJD0Gp4Rtbw1w)
* [weibo分析](https://weibo.com/1497035431/MCXq5BmDe)
  * CV 部分采用了 EVA、BEIT、timm 和 DeiT
  * NLP 部分采用了 LLaMA
  * 框架主体使用了 PyTorch
  * 分布式部分，用了Salesforce.com的一个基于 PyTorch 分布式的简单封装和加强库

[A Survey of Large Language Models](https://arxiv.org/abs/2303.18223)
* 大语言模型综述, [github](https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese_0418.pdf)上有中文版

[Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://github.com/IBM/Dromedary)
* IBM对模型产生内容进行无害化过滤

[Open-Llama](https://github.com/s-JoL/Open-Llama)
* 开源训练llama流程, [这里](http://home.ustc.edu.cn/~sl9292/)还部署了一个demo.

[GrilfriendGPT](https://github.com/EniasCailliau/GirlfriendGPT)
* 一个小应用,生成一个会发照片,语音,文字的虚拟女友

[multilingual-model-speech-recognition](https://ai.facebook.com/blog/multilingual-model-speech-recognition/)
* META开源的ASR模型,特点是支持1000多种语言,对小语种友好
* [github](https://github.com/facebookresearch/fairseq/tree/main/examples/mms), [paper](https://scontent-cgk1-1.xx.fbcdn.net/v/t39.8562-6/348836647_265923086001014_6878005808275791319_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ae5e01&_nc_ohc=hjR-6nxcdOYAX-rEdBF&_nc_ht=scontent-cgk1-1.xx&oh=00_AfDmuOFfAUiIzeJ9TG0faxUXs_x6M81aj4GW3fhxjoOx3Q&oe=6475A14F)

## train

[Building Systems with the ChatGPT API](https://learn.deeplearning.ai/chatgpt-building-system/lesson/1/introduction)
* 吴恩达DLAI的prompt课程, [youtube](https://www.youtube.com/watch?v=1SZOGp1D17E&list=PLiuLMb-dLdWKjX8ib9PhlCIx1jKMNxMpy)有宝玉的翻译版本

[Transformer Math 101](https://blog.eleuther.ai/transformer-math/)
* transformer训练的一些数字知识, 可用于进行性能预估

[Transformer Inference Arithmetic](https://kipp.ly/blog/transformer-inference-arithmetic/)
* 推理部分详细计算

[Drag Your GAN](https://github.com/XingangPan/DragGAN)
* 使用GAN实现图像修改,只需要拖动图片

[SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities](https://github.com/0nutation/SpeechGPT)
* 使用GPT实现多模态对话, 能实现语音对话

[【LLM系列之底座模型对比】LLaMA、Palm、GLM、BLOOM、GPT模型结构对比](https://mp.weixin.qq.com/s/kqBROBbXIeu5Zu1luNpitw?v_p=90&WBAPIAnalysisOriUICodes=10000001&wm=3333_2001&aid=01A2GUVvCiJ0bN45VH0AOVftc20OVPaYUZmVa1h1s_8-8xrdg.&from=10D5193010)
* GPT: decoder只保留一个Masked-Multi-Head Attention
* LLaMA: RMSNorm,SwiGLU,Rotary Embedding, AdamW+Cosine learning rate schedule,因果多头注意力
* Palm:swiGLU,Parallel layers, Multi-Query Attention, RoPE, shared Input-Output Embedding
* GLM: layer norm的顺序和残差重新排列, 用于输出标记的单个线性层, Gelu, 二维位置编码
* BLOOM:ALiBi位置嵌入, Embedding LayerNorm再第一嵌入层后直接使用, 250K词汇表,字节BPE,两个全连接层

[chatglm_tuning: 基于 LoRA 和 P-Tuning v2 的 ChatGLM-6B 高效参数微调](https://mp.weixin.qq.com/s/fF1jci5l65opO1YPMOUuaw?v_p=90&WBAPIAnalysisOriUICodes=10000001&wm=3333_2001&aid=01A2GUVvCiJ0bN45VH0AOVftc20OVPaYUZmVa1h1s_8-8xrdg.&from=10D5193010)
* [Github 代码](https://github.com/zejunwang1/chatglm_tuning)

[Full Parameter Fine-tuning for Large Language Models with Limited Resources](https://arxiv.org/abs/2306.09782)
* 全参数finetune大模型,最少使用8卡3090(24GB*8)即可
* 使用SGD能减少显存用量
* 提出LOMO的优化器及配套归一缩放规则

[LLMTuner](https://github.com/zejunwang1/LLMTuner)
* 大模型调优,支持全参数/LoRA/QLoRA

[Open LLMs](https://github.com/eugeneyan/open-llms)
* 大模型列表

[Inflection-1:Pi’s Best-in-Class LLM](https://inflection.ai/inflection-1)
* 新的大模型,用于对话效果很好, 使用见[hey pi](https://heypi.com/talk)

[MPT-30B](https://huggingface.co/spaces/mosaicml/mpt-30b-chat)
* 来自[@mosaicML](https://twitter.com/MosaicML), [benchmark](https://huggingface.co/spaces/lmsys/mt-bench)

[LLM 全景图（The Landscape of LLM）](https://zhuanlan.zhihu.com/p/637154782)
* 大模型介绍, 文件包再[google drive](https://drive.google.com/file/d/1-n7ZN6wXzgWWlv9kF5DT1QrvPOoguDCT/view?usp=sharing)

[GPT/AIGC/LLM/NLP/ChatGPT 学习](https://gofurther.feishu.cn/docx/Enofdl25BotoVrxth8ec4rNBn5c)

[【VALSE 2023】走向计算机视觉的通用人工智能：GPT和大语言模型带来的启发](https://zhuanlan.zhihu.com/p/620631150)
* 通用人工智能发展方向

[The New Language Model Stack](https://www.sequoiacap.com/article/llm-stack-perspective/)
* 红衫资本对大模型未来的调研和判断

[Hacker and Geeker's Way](https://zhaozhiming.github.io/):ChatGLM3的部署应用等

## dataset

[MathPile](github.com/GAIR-NLP/MathPile):一个多样化且高质量的以数学为中心的语料库, 包含约 95 亿个tokens。其数据包括教科书（包括讲义）、arXiv、维基百科、ProofWiki、StackExchange 和网页。它包含适合 K-12、大学、研究生水平和数学竞赛的数学内容

[Download Terabyte Click Logs](https://labs.criteo.com/2013/12/download-terabyte-click-logs-2/): CTR数据集,推荐数据集

[The Mechanics of Machine Learning](https://mlbook.explained.ai/): 深度学习的原理,图书

[大模型赛道的技术分析](https://whjlnspmd6.feishu.cn/wiki/DBnWwik1piTB6Iki02CcXoVQn3S?continueFlag=15c65df5e495a04816b3bfb2a052bc03):飞书文档,随时失效

## GPU

[NVIDIA Unveils Updated GH200 'Grace Hopper' Superchip with HBM3e Memory, Shipping in Q2'2024](https://www.anandtech.com/show/20001/nvidia-unveils-gh200-grace-hopper-gpu-with-hbm3e-memory): GH200在24年使用HBM3e

[Nvidia H100 GPUs: Supply and Demand](https://gpus.llm-utils.org/nvidia-h100-gpus-supply-and-demand/): H100的供需问题, 介绍了一些细节

[使用什么工具可以监控GPU使用情况?@知乎](https://www.zhihu.com/question/376875425/answer/3170169345?utm_medium=social&utm_oi=49336847171584&utm_psn=1675675386296467456&utm_source=ZHShareTargetIDMore): nvidia-smi/gpustat/[nvtop](https://www.cyberciti.biz/hardware/nvtop-command-in-linux-to-monitor-nvidia-amd-intel-gpus/)/[nvitop](https://github.com/XuehaiPan/nvitop/blob/main/README.md)

[Register pressure in AMD CDNA2™ GPUs](https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-register-pressure-readme/): AMD GPU中降低寄存器的压力

[芯片会议:SIGGRAPH](https://s2023.siggraph.org/): 需要注册

[论文:Understanding Latency Hiding on GPUs](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.pdf): GPU性能提升的方法论文/指南

[[PATCH 0/5] AMD64 EDAC GPU Updates](https://lore.kernel.org/lkml/20230515113537.1052146-1-muralimk@amd.com/T/#t): AMD MI200系列的GPU patch

[GPU编程（CUDA）](https://face2ai.com/program-blog/#GPU%E7%BC%96%E7%A8%8B%EF%BC%88CUDA%EF%BC%89): CUDA编程博客

[NVIDIA Launches A2 Tensor Core GPU, An Entry-Level Design Powered By Ampere GA107 GPU & 16 GB GDDR6 Memory](https://wccftech.com/nvidia-launches-a2-tensor-core-gpu-an-entry-level-design-powered-by-ampere-ga107-gpu-16-gb-gddr6-memory/):性能较差,未实际上市

[Nvidia Reveals Hopper H100 GPU With 80 Billion Transistors](https://www.tomshardware.com/news/nvidia-hopper-h100-gpu-revealed-gtc-2022): 22年旧闻,有H100的PPT和老黄的视频

[Introduction to profiling tools for AMD hardware](https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-profilers-readme/): AMD profiling工具介绍

[matrix-compendium](https://gpuopen.com/learn/matrix-compendium/matrix-compendium-intro/): AMD GPU矩阵运算

[AMD Instinct™ MI200 GPU memory space overview](https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-mi200-memory-space-overview/): AMD GPU显存

[GPU Programming Guide GeForce 8 and 9 Series](https://developer.download.nvidia.com/GPU_Programming_Guide/GPU_Programming_Guide_G80.pdf): Nvidia GPU编程指南

[How to accelerate AI applications on RDNA 3 using WMMA](https://gpuopen.com/learn/wmma_on_rdna3/): AMD GPU矩阵编程

[详解AMD RDNA2 GPU架构](https://mp.weixin.qq.com/s/ZZfUdjplMVV0lk8tAf7QeQ): RDNA2

[AMD MI300 – Taming The Hype – AI Performance, Volume Ramp, Customers, Cost, IO, Networking, Software](https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance): MI300的简单介绍

[GPU巨头，拼什么？](https://mp.weixin.qq.com/s/AGZU66wZll36o9_Ri01OSw): Nvidia Ada, AMD Navi 31和Intel ACM-G10的对比

[NVIDIA GPU Microarchitecture](https://www.ece.lsu.edu/koppel/gp/notes/set-nv-org.pdf): Nvidia架构课件

[CUDA STREAMS](https://on-demand.gputechconf.com/gtc/2014/presentations/S4158-cuda-streams-best-practices-common-pitfalls.pdf): Stream课件

[学习设计GPU：3.执行单元之LSU（load store unit）](https://zhuanlan.zhihu.com/p/644124799?utm_campaign=&utm_medium=social&utm_oi=49336847171584&utm_psn=1668775304565837824&utm_source=com.microsoft.todos): Load store单元, 代码在[opengpgpu@github](https://github.com/OpenGPGPU/opengpgpu), [学习设计GPU：2.后端流水线设计和VectorALU](https://zhuanlan.zhihu.com/p/643184661?utm_medium=social&utm_oi=49336847171584&utm_psn=1664435410360201216&utm_source=ZHShareTargetIDMore)

[Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/): 通过对GPU的量化分析选择合适的型号

[Finite difference method – Laplacian part 4](https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-finite-difference-docs-laplacian_part4/): AMD GPU差分算法实现

[GPGPU-Sim 3.1.1](http://gpgpu-sim.org/manual/index.php/Main_Page): GPU cycle level的性能模拟器, [拜月神使-曌鵷鶵@知乎](https://www.zhihu.com/question/610069168/answer/3101615269?utm_medium=social&utm_oi=49336847171584&utm_psn=1665507212742049792&utm_source=ZHShareTargetIDMore)有一些解析

[NVIDIA CUDA-X GPU-Accelerated Libraries](https://developer.nvidia.com/gpu-accelerated-libraries): Nvidia GPU库合集

[GPU-aware MPI with ROCm](https://gpuopen.com/learn/amd-lab-notes/amd-lab-notes-gpu-aware-mpi-readme/): ROCm MPI

[vSphere ML Accelerator Spectrum Deep Dive – GPU Device Differentiators](https://frankdenneman.nl/2023/05/16/vsphere-ml-accelerator-spectrum-deep-dive-gpu-device-differentiators/): Nvidia GPU 显存/Nvlink/Encoder/Decoder等都有些图表

[vSphere ML Accelerator Spectrum Deep Dive for Distributed Training – Multi-GPU](https://frankdenneman.nl/2023/05/12/vsphere-ml-accelerator-spectrum-deep-dive-for-distributed-training-multi-gpu/): GPU多卡训练分析

[vSphere ML Accelerator Deep Dive – Fractional and Full GPUs](https://frankdenneman.nl/2023/05/10/vsphere-ml-accelerator-deep-dive-fractional-and-full-gpus/): 单GPU分析, 是有整个系列

[oidn@github](https://github.com/OpenImageDenoise/oidn): Intel的GPU denoise库,还支持Nvidia和AMD GPU

[AMD support for Microsoft® DirectML optimization of Stable Diffusion ](https://gpuopen.com/amd-microsoft-directml-stable-diffusion/): 使用AMD GPU做SD

[I want to talk about WebGPU](https://cohost.org/mcc/post/1406157-i-want-to-talk-about-webgpu): WebGPU的简介

[Finetuning Large Language Models On A Single GPU Using Gradient Accumulation](https://sebastianraschka.com/blog/2023/llm-grad-accumulation.html): Finetune

[AMDVLK@github](https://github.com/GPUOpen-Drivers/AMDVLK): AMD的开源vulkan驱动

[NVIDIA Magnum IO GPUDirect Storage Overview Guide](https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html): Nvidia GDR官方文档, [GDR官网](https://developer.nvidia.com/gpudirect-storage)

[CPU vs (GP)GPU: the difference](https://yurichev.org/GPU/): CPU和GPU区别,以及SIMT的解释

[DeepLearning-500-questions@github](https://github.com/scutan90/DeepLearning-500-questions): GPU问题

[快速认识英伟达的GPU-CPU超级芯片Grace Hopper](https://mp.weixin.qq.com/s/0mvtj7s3FdYXPx-oJjrRgA): GH介绍

[GPU 利用率低常见原因分析及优化](https://mp.weixin.qq.com/s/q1SLsLlet2pQ7q0CJZtykA) 

[深入浅出扩散模型(Diffusion Model)系列](https://mp.weixin.qq.com/s/sD8--wbpJPb46D3AXkmkFQ):[第一篇](https://mp.weixin.qq.com/s?__biz=Mzg2NjcwNjcxNQ==&mid=2247485029&idx=1&sn=1cc60284430cef628b2704936fa0c530&chksm=ce47f211f9307b0745a757cf9bb02430fed5cb04491766afb63733c62b79d9ee403e137fe724&scene=21#wechat_redirect), [第二篇](https://mp.weixin.qq.com/s?__biz=Mzg2NjcwNjcxNQ==&mid=2247485206&idx=1&sn=716b8cd4a67d130a55dd18d7e9a9bbf8&chksm=ce47f362f9307a74ff19b154dac3990b387452c81f1912988acddd53853b8c45feae3e5c387f&scene=21#wechat_redirect), [第三篇](https://mp.weixin.qq.com/s/sD8--wbpJPb46D3AXkmkFQ), 从原理,数学推导和代码实现分别讲解SD

[Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463): phi-1.3,从教科书训练的模型,但泛化性存在争议

[paper: OLM](https://allenai.org/olmo/olmo-paper.pdf): 开源Open Language Model, 预计会开源训练/推理框架和代码, 训练log, 数据集等,还在AMD的超算中运行了.

[Create GPT from scratch using Python — Part 1](https://medium.com/@fareedkhandev/create-gpt-from-scratch-using-python-part-1-bd89ccf6206a): 学习GPT