

#### [Twitter上有人说GPT-4的详细信息已经泄露，不知道可信度如何。一些关键信息：- GPT-4的 @转发[133]](https://weibo.com/1727858283/N9kZE2niR)

Note: Twitter上有人说GPT-4的详细信息已经泄露，不知道可信度如何。一些关键信息：- GPT-4的大小是GPT-3的10倍以上。我们认为它在120层中总共有大约1.8万亿个参数。- GPT-4是多个专家模型混合在一起，但不是之前说的8个专家，而是16个。研究人员已经证明，使用64到128个专家比16个专家能够获得更好的损失，但这只是纯粹的研究。OpenAI选择16个专家的一个原因是，更多的专家在许多任务上难以泛化。更多的专家也可能更难以达到收敛。- 预训练阶段的上下文长度（seqlen）为8k。GPT-4的32k seqlen版本是在预训练后对8k进行微调的结果。- 为了在所有的A100s GPUs上并行化，他们使用了8路张量并行，因为这是NVLink的限制。- 如果他们在云中的成本约为每小时$1美元/A100，那么这次运行的训练成本将约为6300万美元。- GPT-4推理成本是175B参数的Davinchi的3倍。这主要是由于GPT-4需要更大的集群和实现的利用率更低。它的成本估计是$0.0049/ 1K tokens。（目前GPT-4的API价格大约是$0.03 / 1K tokens）- 新的GPT-4质量下降的阴谋论可能只是因为他们让Oracle模型接受来自推测解码模型的较低概率序列。- 推理在128个GPU的集群上运行。它在8路张量并行和16路管道并行中完成。每个8个GPU的节点只有约1300亿个参数，或者在FP16下每个GPU少于30GB，在FP8/int8下少于15GB。- 视觉多模态是一个与文本编码器分开的视觉编码器，具有交叉注意力。该架构类似于Flamingo。这在GPT-4的1.8T之上增加了更多的参数。它在文本预训练后，用另外约2万亿个Token进行微调。在视觉模型上，OpenAI希望从头开始训练，但它还不够成熟，所以他们希望通过从文本开始来降低风险。他们训练的一部分数据是联合数据（渲染的LaTeX/文本），网页截图，YouTube视频：采样帧，并在其周围运行Whisper以获取转录。将他的推文线程翻译一下供参考：GPT-4的详细信息已经泄露。一切都在这里：参数数量：GPT-4的大小是GPT-3的10倍以上。我们认为它在120层中总共有大约1.8万亿个参数。混合专家模型 - 已确认。OpenAI通过使用混合专家（MoE, mixture of experts）模型，能够保持合理的成本。他们在模型中使用了16个专家，每个专家的MLP参数约为1110亿。每次前向传递都会路由到这些专家中的2个。混合专家（MoE）路由：虽然文献中大量讨论了选择将每个Token路由到哪个专家的高级路由算法，但OpenAI的GPT-4模型的路由方式据说相当简单。大约有550亿个共享参数用于注意力。推理：每次前向传递推理（生成1个Token）只使用约2800亿个参数和约560 TFLOPs。这与纯密集模型每次前向传递所需的约1.8万亿个参数和约3700 TFLOP形成对比。数据集：GPT-4在约13万亿个Token上进行训练。这些并非唯一的Token，他们也将更多的Token计算为纪元（Epoch）。纪元数量（Epoch number）：文本数据为2个纪元，代码数据为4个纪元。有数百万行来自ScaleAI和内部的指令微调数据。GPT-4 32K：预训练阶段的上下文长度（seqlen）为8k。GPT-4的32k seqlen版本是在预训练后对8k进行微调的结果。批量大小（Batch Size）：批量大小在集群运行的几天内逐渐增加，但到最后，OpenAI使用的批量大小为6000万！当然，这“只是”每个专家看到的Token数量为750万的批量大小，因为并非每个专家都看到所有的Token。对于真实的批量大小：将这个数字除以seq len就可以得到真实的批量大小。已经停止使用这些误导性的数字了。并行策略：为了在所有的A100s GPUs上并行化，他们使用了8路张量并行，因为这是NVLink的限制。除此之外，他们还使用了15路管道并行。（可能使用了ZeRo Stage 1。他们可能使用了块级FSDP）训练成本：OpenAI的GPT-4训练FLOPS约为2.15e25，在约25,000个A100s上运行90到100天，MFU约为32%到36%。这种极低的利用率部分是由于需要从中重新启动的检查点的数量巨大。如果他们在云中的成本约为每小时1美元/A100，那么这次运行的训练成本将约为6300万美元。（今天，预训练可以在约55天内用约8192个H100完成，成本为2150万美元，每小时H100的成本为2美元。）混合专家权衡：采取了多种MoE权衡：例如，MoE在推理上非常难处理，因为并非每个模型的部分都在每个Token生成时使用。这意味着当其他部分被使用时，部分可能会处于休眠状态。当服务用户时，这真的会损害利用率。研究人员已经证明，使用64到128个专家比16个专家能够获得更好的损失，但这只是纯粹的研究。有多种原因选择更少的专家。OpenAI选择16个专家的一个原因是，更多的专家在许多任务上难以泛化。更多的专家也可能更难以达到收敛。对于如此大的训练运行，OpenAI选择在专家数量上更保守。GPT-4推理成本：GPT-4的成本是175B参数的Davinchi的3倍。这主要是由于GPT-4需要更大的集群和实现的利用率更低。它的成本估计是$0.0049 / 1K Tokens，用128个A100推理GPT-4 8k seqlen，$0.0021/ 1K Tokens，用128个H100推理GPT-4 8k seqlen。应该注意的是，我们假设利用率较高，并保持批量大小较大。多查询注意力：OpenAI像其他人一样使用MQA（Multi-Query Attention）。因此，只需要1个头，可以显著减少KV缓存的内存容量。即使如此，32k seqlen的GPT-4肯定无法在40GB的A100s上运行，8k的最大bsz也受到限制。连续批处理：OpenAI实现了可变批量大小和连续批处理。这样做是为了允许一定程度的最大延迟，同时优化推理成本。视觉多模态：这是一个与文本编码器分开的视觉编码器，具有交叉注意力。该架构类似于Flamingo。这在GPT-4的1.8T之上增加了更多的参数。它在文本预训练后，用另外约2万亿个Token进行微调。在视觉模型上，OpenAI希望从头开始训练，但它还不够成熟，所以他们希望通过从文本开始来降低风险。这种视觉能力的主要目的之一是为了能够阅读网页并转录图像和视频中的内容。他们训练的一部分数据是联合数据（渲染的LaTeX/文本），网页截图，YouTube视频：采样帧，并在其周围运行Whisper以获取转录。推测解码（Speculative Decoding）：OpenAI可能在GPT-4的推理上使用推测解码（不确定100%）。这个想法是使用一个更小更快的模型提前解码几个Token，然后将它们作为一个单独的批次输入到一个大的oracle模型中。如果小模型对其预测正确 - 大模型同意，我们可以在一个批次中解码几个Token。但是如果大模型拒绝了由草案模型预测的Token，那么剩下的批次就会被丢弃，继续使用大模型。新的GPT-4质量下降的阴谋论可能只是因为他们让oracle模型接受来自推测解码模型的较低概率序列。推理架构：推理在128个GPU的集群上运行。在不同位置的多个数据中心中有多个这样的集群。它在8路张量并行和16路管道并行中完成。每个8个GPU的节点只有约1300亿个参数，或者在FP16下每个GPU少于30GB，在FP8/int8下少于15GB。模型有120层，所以它适合在15个不同的节点中。[可能在第一个节点上有更少的层，因为它需要计算嵌入]根据这些数字：如果OpenAI试图按照chinchilla的最优去训练，他们应该在2倍的Token上进行训练。[更不用说像我们一样超越它了]这说明他们正在努力获取高质量的数据。为什么没有FSDP？可能的原因是他们获得的一些硬件基础设施是旧一代的。这在本地计算集群中很常见，因为组织通常会在几个“波”中升级基础设施，以避免完全暂停操作。由于管道并行度非常高，就像我们其他人一样，他们很可能遭受“批量泡沫”：批次之间的轻微空闲时间。再次：没有魔法。他们知道他们在做什么，但这不是魔法。Mark再细读。另外一点，我看了原twitter，作者在利用这个热点给自己的订阅服务引流量。信息的真实性和目的性，需要大家自己判断这里有一篇公众号文章信息更全：《GPT-4详细架构技术细节泄漏，训练一次要 6300 万美元》嗯，不一定都靠谱，仅作为一个参考//:这是个AI网红，可信度很低。有了详细的技术路线，华为百度360科大讯飞 又可以再突破，又可以独立研发了原来如此——“新的GPT-4质量下降的阴谋论可能只是因为他们让oracle模型接受来自推测解码模型的较低概率序列。”回复: 作者的来源其实是一篇付费文章Mark再细读。另外一点，我看了原twitter，作者在利用这个热点给自己的订阅服务引流量。信息的真实性和目的性，需要大家自己判断这里有一篇公众号文章信息更全：《GPT-4详细架构技术细节泄漏，训练一次要 6300 万美元》马克一下嗯，不一定都靠谱，仅作为一个参考//:这是个AI网红，可信度很低。It’s over这句话听起来不太有信息

Picture: [66fd066bly8hfsqgsmjl4j20u085eb29.jpg](https://weibo.cn//mblog/pic/N9kZE2niR?rl=1)

#### [公开课《AI系统 & 深度学习系统》文字版地址：chenzomi12.github.io/视频、pp @转发[110]](https://weibo.com/2194035935/N9kl11wSY)

Note: 公开课《AI系统 & 深度学习系统》文字版地址：chenzomi12.github.io/视频、ppt等可以在这里找到链接：github.com/chenzomi12/DeepLearningSystem这个开源项目英文名字叫做 Deep Learning System 或者 AI System(AISys)、AI Infra、ML System(MLSys)，中文名字叫做 深度学习系统 或者 AI系统。本开源项目主要是跟大家一起探讨和学习人工智能、深度学习的计算机系统设计，而整个系统是围绕着 ZOMI 在华为昇腾工作当中所积累、梳理、构建 AI 系统全栈的内容。希望跟所有关注 AI 开源项目的好朋友一起探讨研究，共同促进学习讨论。华为昇思的博士

Picture: [82c654dfly1hfsnj09fjcj20ol14y7ey.jpg](https://weibo.cn//mblog/pic/N9kl11wSY?rl=1)

Github: [github.com/chenzomi12/DeepLearningSystem](https://github.com/chenzomi12/DeepLearningSystem)

#### [《CPU性能分析与优化》读书笔记（3）1、在打算从CPU特性角度分析性能问题之前，需要确保已经解决了 @转发[10]](https://weibo.com/1202332555/N956DuJI0)

Note: 《CPU性能分析与优化》读书笔记（3）1、在打算从CPU特性角度分析性能问题之前，需要确保已经解决了应用程序的主要性能缺陷。否则，从CPU特性的角度进行性能分析就没有意义，反而可能引导你走向错误的方向。2、自顶向下微架构分析（Top-Down Microarchitecture Analysis，TMA）能够识别程序中导致热点停滞执行的原因。停滞的瓶颈可能与前端绑定、后端绑定、退休和错误投机相关。3、如果在指定的执行周期中，指令对应的微操作没有被分配，可能有两种原因：一是无法对其进行取指和译码（前端绑定），二是后端负载过重导致无法为新的微操作分配资源（后端绑定）。4、操作非规范的浮点值会导致程序变得极慢，此时即使退休率高，整体性能也会很差。5、自Linux 4.8内核开始，perf工具支持--topdown参数，可以打印TMA的第一层指标。使用命令perf stat --topdown -a -- taskset -c 0 ./a.out b，可以打印出四个指标：退休指令（retiring）、错误投机（bad speculat）、前端绑定（FE bound）和后端绑定（BE bound）。6、个人理解：退休指令表示有效执行完毕的指令，错误投机表示分支预测错误导致执行了无效的指令，前端绑定表示取指令停滞，例如iTLB miss和iCache miss，后端绑定表示由于读写内存等待内存传输而导致CPU停滞。7、上述四个指标是TMA的第一层，还可以进一步细化，例如内存绑定可以分为L1、L2、L3和内存绑定。8、通过perf工具，可以定位到具体函数、代码行或指令导致的内存绑定等问题。一旦定位到具体代码，可以通过预取内存（__builtin_prefetch）和优化紧凑的数据结构热点成员等方式进行优化。9、最后分支记录（Last Branch Record，LBR）特性可以持续记录大量已执行的分支跳转指令，而该特性的性能开销大多低于1%。可以使用命令perf record -b采集此信息，该信息可以识别热点代码分支和热点分支的错误预测率。10、通过以直通方式运行热点路径，可以极大地提升程序性能。例如，对于一个分支，了解其条件分支在99%的情况下是真还是假对编译器的优化决策至关重要。11、LBR还可用于基于剖析文件的编译优化、采集函数的参数和基本块的执行次数。12、基于处理器时间的采样（Precise Event-Based Sampling，PEBS）提供了多种方法来增强分析。它具有几个优点，包括精准事件采样、降低采样开销、分析内存访问和检测False sharing。13、Intel处理器跟踪技术（Processor Trace，PT）主要用于性能问题的事后分析和根因定位。例如，可以分析程序在无响应的一小段时间内所执行的操作，进行事后调试或对程序执行进行回溯。该工具被认为是性能分析的终极手段，运行开销较低。

#### [【Making C++ Memory-Safe Without Borrow Checking, R @转发[27]](https://weibo.com/1715118170/N8YcPxfT5)

Note: 【Making C++ Memory-Safe Without Borrow Checking, Reference Counting, or Tracing Garbage Collection】https:///verdagon.dev/blog/vale-memory-safe-cpp 使 C++ 内存安全，无需借用检查、引用计数或跟踪垃圾收集。 

Picture: [663aa05aly8hfpxv75znpj20i7cmke82.jpg](https://weibo.cn//mblog/pic/N8YcPxfT5?rl=1)

#### [【方便的封装器，用于在内存受限环境中进行大型语言模型(LLM)的微调和推理，支持参数高效微调(例如L @#开源#](https://weibo.com/1402400261/N95qezBCN)

Note: 【方便的封装器，用于在内存受限环境中进行大型语言模型(LLM)的微调和推理，支持参数高效微调(例如LoRA、Adapter)和量化技术(8位、4位)】’Memory Efficient Fine-tuning of Large Language Models (LoRA + quantization) - Convenient wrapper for fine-tuning and inference of Large Language Models (LLMs) with several quantization techniques (GTPQ, bitsandbytes)' Tuan Anh Nguyen Dang GitHub: github.com/taprosoft/llm_finetuning  

Picture: [5396ee05ly8hfqtkev5u7j20ua0u0jws.jpg](https://weibo.cn//mblog/pic/N95qezBCN?rl=1)

Github: [github.com/taprosoft/llm_finetuning](https://github.com/taprosoft/llm_finetuning)

#### [【Why use fast_matrix_market：用于C++和Python的快速且功能齐全的M @#开源#](https://weibo.com/1402400261/N9yfc5ted)

Note: 【Why use fast_matrix_market：用于C++和Python的快速且功能齐全的Matrix Market I/O库】'Why use fast_matrix_market - Fast and full-featured Matrix Market I/O library for C++ and Python' Adam Lugowski GitHub: github.com/alugowski/fast_matrix_market   

Picture: [5396ee05ly8hfucyhji87j21800jcjud.jpg](https://weibo.cn//mblog/pic/N9yfc5ted?rl=1)

Github: [github.com/alugowski/fast_matrix_market](https://github.com/alugowski/fast_matrix_market)

#### [Emerging Architectures for LLM Applications 在这篇文章中 @网页链接](https://weibo.com/2194035935/N9yW3cBZ4)

Note: Emerging Architectures for LLM Applications 在这篇文章中，作者将分享一种新兴的LLM应用堆栈的参考架构。这展示了作者在AI初创公司和先进技术公司中看到的最常见的系统、工具和设计模式。这个堆栈仍然非常新，随着底层技术的进步可能会发生实质性的变化，但希望它能成为现在使用LLM的开发者的有用参考。回复:成功收藏到你的Notion

Picture: [82c654dfgy1hfug0mtjcej21jk12wqe0.jpg](https://weibo.cn//mblog/pic/N9yW3cBZ4?rl=1)

#### [：mazzzystar/Queryable这是一个月入 3k 的产品：「寻隐/Queryable」是 @宝玉xp](https://weibo.com/1727858283/N9ApgfKV3)

Note: ：mazzzystar/Queryable这是一个月入 3k 的产品：「寻隐/Queryable」是一个离线的自然语言相册搜索工具，你可以用「一只狗在玩滑梯」来搜索你的 iPhone 相册，而不是搜单纯的“狗”，并且不联网。它的实现原理是集成了iOS上的CLIP模型，CLIP（Contrastive Language-Image Pre-Training）是OpenAI于2021年提出的一个模型。CLIP将图像和文本编码成向量，可以在同一空间进行比较的表示。具体原理可以参考作者博文：mazzzystar.github.io/2022/12/29/Run-CLIP-on-iPhone-to-Search-Photos/之前确实不知道iOS上也可以离线实现这种图片的Embedding，这真的可以解锁很多应用场景！作者的故事：项目地址：github.com/mazzzystar/Queryable据说每次升完ios莫名发热就是在给相册打标签开发者把图像识别和语言翻译两个功能的模型都集成到了手机里，方便手机完全离线也可以用，导致安装包非常大，这也是个以后大语言模型要遇到的问题。//://:卧槽这他吗太有用了各手机自带相册app很快会有这个功能。谢谢作者的贡献

Picture: [66fd066bly8hfssvj5jbbj215m0saq9e.jpg](https://weibo.cn//mblog/pic/N9lxR9hnp?rl=1)

Github: [github.com/mazzzystar/Queryable](https://github.com/mazzzystar/Queryable)

#### [【InfiniTensor：深度学习领域的编译器集合，旨在缩小深度学习应用与后端硬件之间的鸿沟，通过 @#开源#](https://weibo.com/1402400261/N9HujlyJw)

Note: 【InfiniTensor：深度学习领域的编译器集合，旨在缩小深度学习应用与后端硬件之间的鸿沟，通过使用编译器超优化技术，对神经网络模型进行优化，从而获得更好的性能】’InfiniTensor' GitHub: github.com/InfiniTensor/InfiniTensor    

Picture: [5396ee05ly8hfvhral4fnj225z0u0agw.jpg](https://weibo.cn//mblog/pic/N9HujlyJw?rl=1)

Github: [github.com/InfiniTensor/InfiniTensor](https://github.com/InfiniTensor/InfiniTensor)

#### [Linux 内核代码风格指南这是一个简短的文档，描述了 linux 内核的首选代码风格。代码风格是因 @蚁工厂](https://weibo.com/2194035935/N9vnzpVMA)

Note: Linux 内核代码风格指南这是一个简短的文档，描述了 linux 内核的首选代码风格。代码风格是因人而异的， 而且我不愿意把自己的观点强加给任何人，但这就像我去做任何事情都必须遵循的原则 那样，我也希望在绝大多数事上保持这种的态度。请 (在写代码时) 至少考虑一下这里 的代码风格。

Picture: [82c654dfly1h43ckqjjx3j20i018zdhh.jpg](https://weibo.cn//mblog/pic/LBTOzEGc4?rl=1)

#### [【engblogs.dev：从各大科技公司的RSS源获取数据，用gpt-3.5生成简要摘要，并将数据 @#开源#](https://weibo.com/1402400261/N9fHGkAlw)

Note: 【engblogs.dev：从各大科技公司的RSS源获取数据，用gpt-3.5生成简要摘要，并将数据存储在supabas中】'engblogs.dev - learn from your favorite tech companies' Ishan GitHub: github.com/ishan0102/engblogs   回复:是的，应该按主题切，不然效果不好的其实，切分这块是需要研究下的看了下文章分段时，直接暴力切

Picture: [5396ee05ly8hfs31pdiu8j20xs0nkady.jpg](https://weibo.cn//mblog/pic/N9fHGkAlw?rl=1)

Github: [github.com/ishan0102/engblogs](https://github.com/ishan0102/engblogs)

#### [【Keras Core：完全重写的Keras代码库，基于模块化后端架构进行重构，可以在任意框架上运行 @网页链接](https://weibo.com/1402400261/N9tusxOQv)

Note: 【Keras Core：完全重写的Keras代码库，基于模块化后端架构进行重构，可以在任意框架上运行Keras工作流，包括TensorFlow、JAX和PyTorch。新功能包括：完整的Keras API，适用于TensorFlow、JAX和PyTorch；跨框架的深度学习低级语言；与JAX、PyTorch和TensorFlow原生工作流的无缝集成；支持所有后端的跨框架数据流水线；预训练模型等】《Keras: Deep Learning for humans》  

Picture: [5396ee05ly8hftrxw91qyj21e00qa43o.jpg](https://weibo.cn//mblog/pic/N9tusxOQv?rl=1)

#### ['Awesome-VQVAE - A collection of resources and pap @#开源#](https://weibo.com/1402400261/N9xgwwS9W)

Note: 'Awesome-VQVAE - A collection of resources and papers on Vector Quantized Variational Autoencoder (VQ-VAE) and its application' Wenhao (Reself) Chai GitHub: github.com/rese1f/Awesome-VQVAE   Awesome

Picture: [5396ee05ly8hfu8kx9shaj20zd0u0di7.jpg](https://weibo.cn//mblog/pic/N9xgwwS9W?rl=1)

Github: [github.com/rese1f/Awesome-VQVAE](https://github.com/rese1f/Awesome-VQVAE)

#### [看这篇论文《Efficient Large-Scale Language Model Trainin @Barret李靖](https://weibo.com/2194035935/N9DFWqVcn)

Note: 看这篇论文《Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM》 体会到做大模型工程的难度有多么大，文中提到使用 1024 张 80G 显存的 A100 卡训练 1750 亿参数的 GPT-3 模型，需要花费 34 天，这是理论上的数据，但是实际情况大概率不是如此。过程中，最容易遇到的是基础设施瓶颈和稳定性问题，针对大模型的训练有三种并行策略，分别是数据并行、流水线并行和张量并行，理解起来并不复杂：- 数据并行：搞多个集群并行计算- 流水线并行：单个集群上有几条并行的流水线- 张量并行：将矩阵乘法分解后多 GPU 并行计算不管是哪一种并行策略，都涉及到数据的拆分、合并等动作，这些动作是在不同的机器上实现的，因此过程中很容易遇到磁盘 IO、网络 IO 等问题，除此之外还有机房环境控制、单机故障后的自动组网、数据异常时的显存溢出、设备算力不均等等各种千奇百怪的问题。要确保整个集群的算力稳定，就需要建设一个可观测、可自愈、可扩展、可调试的大型系统，例如可以观测整个集群的算力动图，从图中识别出是否有 GPU 掉卡、是否有显存 OOM、是否有网络异常、是否有储存异常等等，需要有巡检模块和自愈模块，还有具备非黑屏环境的互动调试模块。从整个工程的整体设计来看，不算复杂，但是要深入进去解决每一个问题，还是很让人头疼的，一次运行的失败，可能会丢失好多天的算力数据，如果是一个不能稳定复现的问题，排查起来就更要命了。一旦将整个集群的稳定性控制下来以后，那么就有希望将大模型的训练时间优化到接近理论值，这类工程的建设还是十分有价值的，商业化的前景也比较大。

#### [【VisualRWKV：RWKV语言模型的视觉增强版本，使RWKV能够处理各种视觉任务】'Visua @#开源#](https://weibo.com/1402400261/N9y7QrtHI)

Note: 【VisualRWKV：RWKV语言模型的视觉增强版本，使RWKV能够处理各种视觉任务】'VisualRWKV - the visual-enhanced version of the RWKV language model, enabling RWKV to handle various visual tasks.' Haowen Hou GitHub: github.com/howard-hou/VisualRWKV   

Picture: [5396ee05ly8hfucffm16ij219e0u0gp8.jpg](https://weibo.cn//mblog/pic/N9y7QrtHI?rl=1)

Github: [github.com/howard-hou/VisualRWKV](https://github.com/howard-hou/VisualRWKV)

#### [Tesla Dojo Supercomputers Will Provide 100 Exaflop @网页链接](https://weibo.com/2144454703/N9EGErzan)

Note: Tesla Dojo Supercomputers Will Provide 100 Exaflops in 2024Jan 2023 3 Exaflops of AI compute, 10,000 Nvidia A100June 2023 5.5 Exaflops, 17,000 Nvidia A100Oct 2023 13 Exaflops, 40,000 Nvidia A100Feb 2024 33 Exaflops, 100,000 Nvidia A100October 2024 100 ExaflopsMid 2025 300 Exaflops

Picture: [7fd1c82fly1hfv5dnvmp7j20sg0f3jv9.jpg](https://weibo.cn//mblog/pic/N9EGErzan?rl=1)

#### [【Xorbits Inference: ：功能强大、多用途的库，旨在为LLM、语音识别模型和多模态模 @#开源#](https://weibo.com/1402400261/N9FhkhpkY)

Note: 【Xorbits Inference: ：功能强大、多用途的库，旨在为LLM、语音识别模型和多模态模型提供服务，甚至在笔记本电脑上也能使用，支持各种模型组合。Xinference简化了大规模语言、语音识别和多模态模型的服务过程，支持的模型包括baichuan、chatglm、chatglm2、wizardlm、vicuna、orca等】’Xorbits Inference: Model Serving Made Easy - Xorbits Inference (Xinference) is a powerful and versatile library designed to serve LLMs, speech recognition models, and multimodal models, even on your laptop. It supports a variety of models compatible with GGML, such as chatglm, baichuan, whisper, vicuna, orac, and many others.' Xorbits GitHub: github.com/xorbitsai/inference 

Github: [github.com/xorbitsai/inference](https://github.com/xorbitsai/inference)

#### [arthurchiao大佬更新了一个《Linux 网络栈原理、监控与调优》系列博文本文尝试从技术研发 @蚁工厂](https://weibo.com/1240212845/N9X48cTv8)

Note: arthurchiao大佬更新了一个《Linux 网络栈原理、监控与调优》系列博文本文尝试从技术研发与工程实践（而非纯理论学习）角度，在原理与实现、监控告警、 配置调优三方面介绍内核5.10 网络栈。由于内容非常多，因此分为了几篇系列文章。原理与实现    Linux 网络栈原理、监控与调优：前言    Linux 中断（IRQ/softirq）基础：原理及内核实现    Linux 网络栈接收数据（RX）：原理及内核实现    Linux 网络栈发送数据（TX）：原理及内核实现（未完成）监控    Monitoring Linux Network Stack调优    Linux 网络栈接收数据（RX）：配置调优    Linux 网络栈发送数据（TX）：配置调优（未完成）

Picture: [82c654dfgy1h45a6141x8j20te0je0uc.jpg](https://weibo.cn//mblog/pic/LC5vo7TlJ?rl=1)

#### [【Curated Transformers：PyTorch的Transformer库，提供了一系列经 @#开源#](https://weibo.com/1402400261/N9OQ5fRLn)

Note: 【Curated Transformers：PyTorch的Transformer库，提供了一系列经过精选的最先进的模型和可组合的组件，支持包括Falcon、LLaMA和Dolly v2在内的最先进的Transformer模型，每个模型都由可重用的构建块组成，具有一致的类型注释】’Curated Transformers - A PyTorch library of curated Transformer models and their composable components' Explosion GitHub: github.com/explosion/curated-transformers  哈喽你也是股友吗？

Picture: [5396ee05ly8hfwe76d2cuj21460u0dlx.jpg](https://weibo.cn//mblog/pic/N9OQ5fRLn?rl=1)

Github: [github.com/explosion/curated-transformers](https://github.com/explosion/curated-transformers)

#### [【Bark语音合成推理部分的C/C++移植版】’Port of Suno AI's Bark in  @#开源#](https://weibo.com/1402400261/Na07zoLML)

Note: 【Bark语音合成推理部分的C/C++移植版】’Port of Suno AI's Bark in C/C++ for fast inference' PAB GitHub: github.com/PABannier/bark.cpp   

Picture: [5396ee05ly8hfxs17p70fj20mg0bw74w.jpg](https://weibo.cn//mblog/pic/Na07zoLML?rl=1)

Github: [github.com/PABannier/bark.cpp](https://github.com/PABannier/bark.cpp)

#### [整理了一批学摄影和提高审美必看的网站，供大家参考，欢迎补充。▪️ CNU视觉联盟：网站活跃度高，作品 @班叔](https://weibo.com/2194035935/Na7PbeI84)

Note: 整理了一批学摄影和提高审美必看的网站，供大家参考，欢迎补充。▪️ CNU视觉联盟：网站活跃度高，作品质量也很ok，非常适合摄影师拍摄学习。（www.cnu.cc）▪️ 马格南：网站里面每张照片都是大片，非常具有故事性，都可以当作摄影教材。（www.magnumphotos.com）▪️ MODEL：时尚界的百科全书，如果你是时尚摄影爱好者，速度的立马收藏。（models.com）▪️ IPPA：手机摄影作品展示网站，平时喜欢用手机拍摄的朋友可以学学。（www.ippawards.com）▪️ 1X：摄影作品质量高，很适合摄影师在上面找灵感。（1x.com）▪️ Jens ingvarsson：绝对是一个宝藏网站，摄影作品涵盖了人物的造型、服装的搭配、场景的设计、光影的运用，非常值得我们学习。（www.jensingvarsson.com）▪️ 500PX：网站里的摄影作品质量很高，有众多摄影师经常浏览及上传作品。（500px.com）▪️ 大都市Numero：从巴黎起步的优质杂志，独特、简约、清晰、现代的杂志美学，令人惊艳的图片影像。（www.numero.com.cn）▪️ Nhu Xuan Hua Photography：一个很高级的网站，作品非常有设计感且干净，是找灵感的好地方。（www.nhuxuanhua.com）▪️ Styledumonde：时尚街拍杂志，图片都收录于vogue杂志，适合时尚穿搭和街头摄影。（styledumonde.com）▪️ Aint-bad：艺术与摄影相关，很多艺术大牛的采访，还可以购买杂志，深度学习审美。（aint-bad.com）▪️ 摄影笔记：系统做摄影教程的网站，论坛类型的，内容涵盖丰富，学摄影必备。（sybiji.com）▪️ FOTOMEN摄影之友：内容丰富，涵盖了摄影教程、器材测评、摄影作品展示、摄影类资讯。（fotomen.cn）▪️ Future Rep：收录了许多国际艺术家的摄影作品，风格多种多样，有复古的也有偏向于大胆前卫的。（futurerep.com）▪️ LFI：徕卡旗下网站。既有全球顶级摄影作品展示，又可在线浏览徕卡旗下的摄影杂志。（lfi-online.de）▪️ Pexels：有很多优秀的摄影作者，在这里免费分享了许多精彩的照片和视频。（www.pexels.com）▪️ 国家地理中文网：风光摄影爱好者必备，有世界各地的风光影像。（www.ngchina.com.cn）抄送： 

#### [开源LLM微调训练指南：如何打造属于自己的LLM模型迁移学习和 QLoRA 框架为我们提供了强大的工 @开源LLM微调训练指南：如何打造属于自己的LLM模型](https://weibo.com/2194035935/Naf3TvrSL)

Note: 开源LLM微调训练指南：如何打造属于自己的LLM模型迁移学习和 QLoRA 框架为我们提供了强大的工具，可以高效地利用预训练的语言模型（LLM）来解决特定任务。通过根据基准评估选择合适的基础 LLM 模型，我们可以确保我们微调工作的最佳性能。使用 Hugging Face Transformer 和 PEFT 库，我们对基础 LLM 进行了微调，使其专门适应期望的任务。转发微博

#### [在树莓派集群运行650亿参数大模型？llama.cpp最新更新加入了基于MPI的分布式推理支持，每个 @thinkpc](https://weibo.com/2194035935/Na9Y5CQnP)

Note: 在树莓派集群运行650亿参数大模型？llama.cpp最新更新加入了基于MPI的分布式推理支持，每个节点可以只进行一部分推理操作，这样用一堆非常老的机器理论上也可以跑超大模型。感觉LLM分布式共享推理服务有的搞了 github.com/ggerganov/llama.cpp/pull/2099 又不是不能用这IO得多慢啊……%

Github: [github.com/ggerganov/llama.cpp/pull/2099](https://github.com/ggerganov/llama.cpp/pull/2099)

#### [AI知识库：waytoagi.com一份非常棒的AI资料库，包括输入提示词、AI绘画、AI音乐、AI @班叔](https://weibo.com/2194035935/NafSYm8k5)

Note: AI知识库：waytoagi.com一份非常棒的AI资料库，包括输入提示词、AI绘画、AI音乐、AI网站、AI插件、AI课程……你可以把这份文档看做是一个AI学习路径，可以按照文档的章节来学习和了解AI行业，也可以用它来查找一些资料。 

Picture: [005FMk8Tly1hfyhhssxlxj324k1dchdt.jpg](https://weibo.cn//mblog/pic/Na5TeCWnk?rl=1)

#### [一个将写 Prompt 门槛降低到地板以下的工具网站，PromptPerfect，根据它提供的流水线 @Barret李靖](https://weibo.com/2194035935/NayPD2uRR)

Note: 一个将写 Prompt 门槛降低到地板以下的工具网站，PromptPerfect，根据它提供的流水线能力，只要给出最初始的需求，过程中跟 AI 一起结对编（调）程（试），最终就可以获得一个符合预期的 Prompt，在确保 AI 回答质量不下降的情况下，还可以利用这个工具对 Prompt 做精简，增加道德约束、法律约束等。更优秀的是，你可以直接在网站上完成多个 AI 模型的效果测试，直观地观测上面拿到的 Prompt 在所有流行的大模型中，效果是不是都符合预期。它甚至还提供了一个 Prompt as a Service 的服务，提示词即服务，这可以帮助开发者（和非开发者）构建一个立马可用的 Agent 服务，例如设计一个翻译接口、数据转换接口、业务建模服务、学生助教服务、互动游戏设计服务等等。

Picture: [6c0378f8gy1hg219dvjrsj20zd10l162.jpg](https://weibo.cn//mblog/pic/NayPspb3n?rl=1)

#### [可解释的人工智能：可视化transformer中的注意力机制https://generativeai @转发[79]](https://weibo.com/2194035935/Nazkm80b8)

Note: 可解释的人工智能：可视化transformer中的注意力机制https://generativeai.pub/explainable-ai-visualizing-attention-in-transformers-4eb931a2c0f8在这篇文章中，我们将探讨一个用于可视化 Transformer 架构的核心特征——注意力机制的最流行工具（BertViz）。 回复:已收藏至notion

Picture: [82c654dfly1hg1wzwm8edj216h0vin5s.jpg](https://weibo.cn//mblog/pic/Nazkm80b8?rl=1)

#### [《系统重构与迁移指南》手把手教你分析、评估现有系统、制定重构策略、探索可行重构方案、搭建测试防护网、 @蚁工厂](https://weibo.com/2194035935/NaGItp54q)

Note: 《系统重构与迁移指南》手把手教你分析、评估现有系统、制定重构策略、探索可行重构方案、搭建测试防护网、进行系统架构重构、服务架构重构、模块重构、代码重构、数据库重构、重构后的架构守护。Github地址：github.com/phodal/migration另外该作者Phodal Huang还发布了一个重构工具Coca：Github地址：github.com/inherd/cocaCoca 是一个用于系统重构、系统迁移和系统分析的工具箱。它可以分析代码中的测试坏味道、模块化分析、行数统计、分析调用与依赖、Git 分析以及自动化重构等。

Picture: [002otWYnly1gsn3ik5ovoj61m20u0n1n02.jpg](https://weibo.cn//mblog/pic/KpzJG7onB?rl=1)

Github: [github.com/phodal/migration](https://github.com/phodal/migration)

Github: [github.com/inherd/cocaCoca](https://github.com/inherd/cocaCoca)

#### [Xorbits Inference：模型推理， 轻而易举 🤖地址：github.com/xorbit @转发[29]](https://weibo.com/2194035935/NaH4D6rM5)

Note: Xorbits Inference：模型推理， 轻而易举 🤖地址：github.com/xorbitsai/inferenceXorbits Inference（Xinference）是一个性能强大且功能全面的分布式推理框架。可用于大语言模型（LLM），语音识别模 型，多模态模型等各种模型的推理。通过 Xorbits Inference，你可以轻松地一键部署你自己的模型或内置的前沿开源模型。 无论你是研究者，开发者，或是数据科学家，都可以通过 Xorbits Inference 与最前沿的 AI 模型，发掘更多可能。

Github: [github.com/xorbitsai/inferenceXorbits](https://github.com/xorbitsai/inferenceXorbits)

#### [Advanced Python Mastery （精通高级Python）地址：github.com/ @转发[194]](https://weibo.com/2194035935/NaHcrq7s6)

Note: Advanced Python Mastery （精通高级Python）地址：github.com/dabeaz-course/python-mastery这是一门以练习为主导的高级Python编程课程，已在企业培训环节上经过数百次的实战检验，历经十多年。课程由David Beazley编写，他是《Python Cookbook, 3rd Edition》（O'Reilly出版）和《Python Distilled》（Addison-Wesley出版）的作者。课程以Creative Commons许可证发布，无广告、无跟踪、无弹窗、无新闻通讯，也不涉及人工智能。

Picture: [82c654dfly1hg326wq3lxj21j411uahn.jpg](https://weibo.cn//mblog/pic/NaHcrq7s6?rl=1)

Github: [github.com/dabeaz-course/python-mastery](https://github.com/dabeaz-course/python-mastery)

#### [Simply Parse in C这篇文章教你用150行左右的C语言代码写一个简单的ini文件解析器 @网页链接](https://weibo.com/2194035935/NaHgvdYM1)

Note: Simply Parse in C这篇文章教你用150行左右的C语言代码写一个简单的ini文件解析器 

#### [lazygit，一个简单的 git 命令终端 UI地址：github.com/jesseduffie @转发[33]](https://weibo.com/2194035935/NaIGo38LY)

Note: lazygit，一个简单的 git 命令终端 UI地址：github.com/jesseduffield/lazygit你以前听过，git是强大的，但是当所有的事情都如此难以进行时，这种力量有什么好处呢？交互式的rebase需要你在编辑器中编辑一份该死的TODO文件？你在开玩笑吗？要想暂存文件的一部分，你需要使用一个命令行程序逐个检查每个区块，如果一个区块不能再被分解，但包含你不想暂存的代码，你必须手动编辑一个神秘的补丁文件？你在开玩笑吗？！有时你在切换分支时会被要求保存你的更改，只有在切换和恢复更改后才意识到其实并没有任何冲突，直接检出分支就好了？你一定是在跟我开玩笑！如果你像我一样只是一个凡人，并且厌倦了听到 git 的强大功能，而在你的日常生活中它却给你带来了巨大的痛苦，lazygit 可能适合你。可以文件名是中文就会乱码回复:已保存到你的Notion[哇]还没有更多这种复古界面好玩的应用，我喜欢

Github: [github.com/jesseduffield/lazygit](https://github.com/jesseduffield/lazygit)

#### [《通过例子学 Rust》电子书的中文翻译版。本文档按照 Rust 文档翻译指引规范进行翻译。在线阅读 @蚁工厂](https://weibo.com/2194035935/NaM0woM3q)

Note: 《通过例子学 Rust》电子书的中文翻译版。本文档按照 Rust 文档翻译指引规范进行翻译。在线阅读地址：  本站支持文档中英文切换《通过例子学 Rust》（Rust By Example, RBE）内容由一系列可运行的实例组成，通过这些例子阐明了各种 Rust 的概念和基本库。想获取这些例子外的更多内容，不要忘了安装 Rust 到本地并查阅官方标准库文档。另外为了满足您的好奇心，你可以查阅本网站的源代码。

Picture: [82c654dfly1gsn3ejg5nej20gt1oxn04.jpg](https://weibo.cn//mblog/pic/KpzyjmmRE?rl=1)

#### [四本电子书《500 Lines or Less》、《The Performance of Open  @蚁工厂](https://weibo.com/1240212845/NaVUhkEhm)

Note: 四本电子书《500 Lines or Less》、《The Performance of Open Source Applications》、《The Architecture of Open Source Applications》（2卷）在这基本书中，多个开源应用程序的作者解释了他们的软件是如何结构化的，以及为什么这么做。 每个程序的主要组成部分是什么？ 它们是如何互动的？ 他们的建造者在开发过程中学到了什么？ 在回答这些问题时，这些书的贡献者提供了他们如何思考的独特见解。如果你是一个初级开发人员，并且想了解你更有经验的同事是如何思考的，这些书是开始的地方。 如果你是一个中级或高级开发人员，想看看你的同行是如何解决困难的设计问题的，这些书也可以帮助你。

Picture: [82c654dfly1h4e98597adj20al0drq4c.jpg](https://weibo.cn//mblog/pic/LDgMlF9Ap?rl=1)

#### [《CPU性能分析与优化》读书笔记（4）1、CPU后端低效：当前端已完成取指和译码，但后端因过载而无法 @转发[5]](https://weibo.com/1202332555/NasCuEuCc)

Note: 《CPU性能分析与优化》读书笔记（4）1、CPU后端低效：当前端已完成取指和译码，但后端因过载而无法处理新指令。例如，数据缓存未命中和除法单元过载都会导致停滞。2、内存绑定：当应用程序大量访问内存且等待时间较长时，被称为内存绑定。要提高性能，需改善内存访问情况，减少次数或升级内存子系统。3、设计缓存友好的数据结构：遵循时间和空间局部性原则，从缓存行角度考虑是有意义的，而不仅关注单个变量和内存位置。4、按顺序访问数据：最佳方法是按顺序访问内存，利用硬件预取器识别访问模式，提前引入下一个数据块。5、二分搜索中的Eytzinger布局：可利用该布局存储大型数组元素，维护隐式二叉搜索树，以广度优先搜索布局打包到数组中。6、提高内存利用率：通过使数据更紧凑，例如位存储，或重排结构体字段来减少内存使用。7、数据对齐：提高内存利用率的技术，避免16B对象跨越两个内存行，造成读取两次内存行。8、避免线程冲突：当一个对象中的字段a和b在同一缓存行，不同线程访问不同字段可能导致缓存一致性问题，降低运行速度。9、自定义内存分配器：将热数据放在一起，共享高速缓存行，提高内存带宽利用率和空间局部性。10、显式内存预取：利用__builtin_prefetch进行内存预取，时机把握好，不宜过早或过晚。注意显式内存预取不可移植，可能在不同平台效果不同。11、减少ITLB未命中次数：使用更大的页是一种方法。12、显式大页EHP：可用于应用程序所有段，包括文本段，对DTLB和ITLB都有好处。13、核心绑定：表示CPU乱序执行引擎中非内存问题导致的停滞，包括硬件计算资源短缺和软件指令依赖。优化手段有：函数内联、向量化和循环优化。14、循环优化：几乎是所有高性能程序的核心。常见的优化包括循环不变代码外提、循环展开、循环强度折减和循环判断外提。15、向量化：通常编译器会进行内循环向量化、外循环向量化和超字并行向量化。

#### [【OneTrainer：满足所有stable diffusion训练需求的一站式解决方案】’OneT @#开源#](https://weibo.com/1402400261/Nat0GgSXt)

Note: 【OneTrainer：满足所有stable diffusion训练需求的一站式解决方案】’OneTrainer - OneTrainer is a one-stop solution for all your stable diffusion training needs.' Nerogar GitHub: github.com/Nerogar/OneTrainer   

Picture: [5396ee05ly8hg1bjquwvbj214o0u0ag8.jpg](https://weibo.cn//mblog/pic/Nat0GgSXt?rl=1)

Github: [github.com/Nerogar/OneTrainer](https://github.com/Nerogar/OneTrainer)

#### [【关于自学如何使用最新AI工具、框架和理念的资源大列表】'The Ultimate List of  @#开源#](https://weibo.com/1402400261/NasP0yWPm)

Note: 【关于自学如何使用最新AI工具、框架和理念的资源大列表】'The Ultimate List of AI Resources - The ultimate list of resources to teach yourself how to use the latest AI tools, frameworks, and ideas.' Emmet Halm GitHub: github.com/emmethalm/AI   

Picture: [5396ee05ly8hg1apipcdcj20wm0u0n0q.jpg](https://weibo.cn//mblog/pic/NasP0yWPm?rl=1)

Github: [github.com/emmethalm/AI](https://github.com/emmethalm/AI)

#### [【Transformer发展文献综述，涵盖了22种模型、11种架构变化、7种预训练后技术和3种训练技 @爱可可-爱生活](https://weibo.com/1917491813/Nb0mw1JBS)

Note: 【Transformer发展文献综述，涵盖了22种模型、11种架构变化、7种预训练后技术和3种训练技术。模型包括GPT-3、GPT-4、Gopher、AlphaCode、RETRO、GPT-3.5、Chinchilla、Flamingo等。一些重要的架构变化包括多查询注意力、稀疏注意力、专家混合等。同时还介绍了RLHF、CAI、Minerva等后预训练技术以及超参数设置和采样技术等。这份文档对于了解AI发展的最新进展很有帮助】《Transformer Taxonomy (the last lit review) | kipply's blog》  //:转发微博

Picture: [5396ee05ly8hg5cqdqwy6j20ux0u00yn.jpg](https://weibo.cn//mblog/pic/NaZTggHXR?rl=1)

#### [发布了头条文章：《使用 Transformers 量化 Meta AI LLaMA2 中文版大模型》 @soulteary](https://weibo.com/2194035935/Nb1z2ie0W)

Note: 发布了头条文章：《使用 Transformers 量化 Meta AI LLaMA2 中文版大模型》 本篇文章聊聊如何使用 HuggingFace 的 Transformers 来量化 Meta AI 出品的 LLaMA2 大模型，让 LLaMA2 模型能够只使用 5GB 左右显存就能够运行。 [我来了]  

#### [电子书 Putting the “You” in CPU 想知道在计算机上运行程序时到底会发生什么吗 @网页链接](https://weibo.com/2194035935/Nb32rhepj)

Note: 电子书 Putting the “You” in CPU 想知道在计算机上运行程序时到底会发生什么吗？阅读本文以了解多处理的工作原理、什么是系统调用、计算机如何通过硬件中断管理内存以及Linux如何加载可执行文件。 顶！ 

Picture: [82c654dfly1hg5qm9po12j21hf15jwq7.jpg](https://weibo.cn//mblog/pic/Nb32rhepj?rl=1)

#### [【K9s: A lazier way to manage Kubernetes Clusters】h @转发[4]](https://weibo.com/1715118170/NaCEFlgRD)

Note: 【K9s: A lazier way to manage Kubernetes Clusters】https:///github.com/derailed/k9s K9s：一种管理 Kubernetes 集群的惰性方式。K9s提供了一个终端UI来与Kubernetes集群交互。这个项目的目的是使您的应用程序更容易导航、观察和管理。k9会持续监视Kubernetes的变化，并提供后续命令与观察到的资源进行交互。

Picture: [663aa05aly1hg2i58cqxzj20rs0dw75g.jpg](https://weibo.cn//mblog/pic/NaCEFlgRD?rl=1)

Github: [github.com/derailed/k9s](https://github.com/derailed/k9s)

#### ['JAX ONNX Runtime - a robust and user-friendly too @#开源#](https://weibo.com/1402400261/NaCLQDyFa)

Note: 'JAX ONNX Runtime - a robust and user-friendly tool chain that enables the seamless execution of ONNX models using JAX as the backend’ by Google GitHub: github.com/google/jaxonnxruntime   

Picture: [5396ee05ly8hg2inuiwrvj21c40kewif.jpg](https://weibo.cn//mblog/pic/NaCLQDyFa?rl=1)

Github: [github.com/google/jaxonnxruntime](https://github.com/google/jaxonnxruntime)

#### [【BEVDet的TensorRT推理实现，使用C++编程】'BEVDet implemented b @#开源#](https://weibo.com/1402400261/NaCN0aiOr)

Note: 【BEVDet的TensorRT推理实现，使用C++编程】'BEVDet implemented by TensorRT, C++ - BEVDet implemented by TensorRT, C++； Achieving real-time performance on Orin' Chuanhao1999 GitHub: github.com/LCH1238/bevdet-tensorrt-cpp   

Picture: [5396ee05ly8hg2iq0q5itj22og0u0agc.jpg](https://weibo.cn//mblog/pic/NaCN0aiOr?rl=1)

Github: [github.com/LCH1238/bevdet-tensorrt-cpp](https://github.com/LCH1238/bevdet-tensorrt-cpp)

#### [【Minigpt4 Inference on CPU】https:///github.com/Mak @转发[4]](https://weibo.com/1715118170/NaJnQCnFT)

Note: 【Minigpt4 Inference on CPU】https:///github.com/Maknee/minigpt4.cpp CPU 上的 Minigpt4 推理。MiniGPT 4 的 C++ 中的 移植（使用 GGML 进行 4 位、5 位、6 位、8 位、16 位 CPU 推理）。 

Picture: [663aa05aly1hg3buxuppoj21so0u0k16.jpg](https://weibo.cn//mblog/pic/NaJnQCnFT?rl=1)

Github: [github.com/Maknee/minigpt4.cpp](https://github.com/Maknee/minigpt4.cpp)

#### [【Chatbot Arena对话数据集：33K带有成对人工偏好的对话；20个SOTA模型，如 GPT @网页链接](https://weibo.com/1402400261/NaQzagtfE)

Note: 【Chatbot Arena对话数据集：33K带有成对人工偏好的对话；20个SOTA模型，如 GPT-4、Claude 和基于 LLaMA 的 Vicuna；来自13K独立IP；针对MT基准问题的额外3K专家级标注】《Chatbot Arena Conversation Dataset Release | LMSYS Org》  Download:   

Picture: [5396ee05ly8hg47jxmgjpj20vh0u0ahn.jpg](https://weibo.cn//mblog/pic/NaQzagtfE?rl=1)

#### [【peS2o：约4000万篇学术论文集合，经过清理、过滤和格式化，用于语言模型预训练】“allena @网页链接](https://weibo.com/1402400261/NaQHHaaGh)

Note: 【peS2o：约4000万篇学术论文集合，经过清理、过滤和格式化，用于语言模型预训练】“allenai/peS2o - a collection of ~40M creative open-access academic papers, cleaned, filtered, and formatted for pre-training of language models · Datasets at Hugging Face”  

Picture: [5396ee05ly8hg484z6ha1j211m0u0n0l.jpg](https://weibo.cn//mblog/pic/NaQHHaaGh?rl=1)

#### [简要谈一下Memo（AI 驱动的视频、播客转文字、字幕工具  ）这个产品的技术实现，有蛮多可借鉴之处 @网页链接](https://weibo.com/1727858283/NaWBb37xU)

Note: 简要谈一下Memo（AI 驱动的视频、播客转文字、字幕工具  ）这个产品的技术实现，有蛮多可借鉴之处：1. 使用的Electron构建，跨平台支持不错2. 语音识别基于Whisper.CPP，Electron底层是Nodejs，Node对C++支持很好，但目前还没有直接的npm包支持whisper的，团队应该自己做了一些处理3. 视频处理是基于ffmpeg和ffprobe，这两都有现成的npm包可用4. 翻译支持Google和OpenAI，其中OpenAI是用的gpt-3.5-turbo-16k5. Prompt比较简单：将字幕按行拼成字符串，每行前面加上序号，用一个one-shot示例，让它能按原始序号返回翻译后的结果。这种翻译模式优缺点明显，优点就是经济实惠，缺点就是如果原始的字幕拆分不好，翻译后的结果不太容易对应回去，导致合并和漏行。宝老师，请教一下，剪映也有这样的免费功能，这个memo的翻译质量更高？宝玉老师您好，请问中文语音转文字，有啥好用的工具吗(除了剪映)。和我搞的方案一模一样（我是用C#写的感觉实测下来好像没有剪映识别字幕方便快速飞书妙记也可以转写文字，速度也比较快，就是不能转字幕回复:没用过剪映的翻译功能最好加上语音翻译功能宝老师，请教一下，剪映也有这样的免费功能，这个memo的翻译质量更高？回复:讯飞听见的准确率最高，但要收费Repost Weibo等苹果出了不带刘海的MacPro就装一个回复:Memo这个选Large模型应该还可以

Picture: [66fd066bly8hg4y707a56j215q0u0dk8.jpg](https://weibo.cn//mblog/pic/NaWBb37xU?rl=1)

#### [【在仅有1GB VRAM的GPU上运行Stable Diffusion】'Tiny optimize @#开源#](https://weibo.com/1402400261/NaUUb2TSj)

Note: 【在仅有1GB VRAM的GPU上运行Stable Diffusion】'Tiny optimized Stable-diffusion that can run on GPUs with just 1GB of VRAM.' ThisisBillhe GitHub: github.com/ThisisBillhe/tiny-stable-diffusion   过于极限了

Picture: [5396ee05ly8hg4qp7ta45j21bi0e8tc9.jpg](https://weibo.cn//mblog/pic/NaUUb2TSj?rl=1)

Github: [github.com/ThisisBillhe/tiny-stable-diffusion](https://github.com/ThisisBillhe/tiny-stable-diffusion)

#### [【Dejavu：一款开源跨平台工具，旨在帮助记录和搜索看到的任何内容，可以通过有效地捕捉和组织视觉记 @#开源#](https://weibo.com/1402400261/Nb3PTBsq6)

Note: 【Dejavu：一款开源跨平台工具，旨在帮助记录和搜索看到的任何内容，可以通过有效地捕捉和组织视觉记录来拥有完美的记忆】'Dejavu - With Dejavu, you can have a perfect memory by capturing and organizing your visual recordings efficiently.' Zhou Zhiqiang GitHub: github.com/STRRL/dejavu  

Picture: [5396ee05ly8hg5u45tntij21by0nu77u.jpg](https://weibo.cn//mblog/pic/Nb3PTBsq6?rl=1)

Github: [github.com/STRRL/dejavu](https://github.com/STRRL/dejavu)

#### [推荐阅读《向量数据库》，这篇文章很长，但是很详细，无论是入门还是进阶，都有知识点可以学习到。主要介绍 @网页链接](https://weibo.com/1727858283/Nb8eKxMjI)

Note: 推荐阅读《向量数据库》，这篇文章很长，但是很详细，无论是入门还是进阶，都有知识点可以学习到。主要介绍了向量数据库的原理和实现，包括向量数据库的基本概念、相似性搜索算法、相似性测量算法、过滤算法和向量数据库的选型等等。 感谢推荐。感觉除了一开始的vector embeddings讨论（一带而过），其他的内容都和传统的低维流形（manifold）很相似，不知道是不是看漏了什么。k-means貌似图例里字误写错了。图四求求你们别发了，我真的看不完了[苦涩]转发微博Repost Weibo求求你们别发了，我真的看不完了[苦涩]回复:个人觉得向量数据库是一种工程层面的辅助 跟以前感觉没什么特别新的改变回复:本来就都是基于低维的传统技术，和新的大模型技术不是一个层面的，这里提到很多算法在高纬会有性能瓶颈，这两者配合如同老牛拖新车，所以向量数据库这波强蹭其实也是很醉的转k-means貌似图例里字误写错了。图四

Picture: [66fd066bly8hg6dizreplj20n90rj76z.jpg](https://weibo.cn//mblog/pic/Nb8eKxMjI?rl=1)

#### [Meta 6月放出的自监督学习Cookbook：《A Cookbook of Self-Superv @ChatbotsChina](https://weibo.com/2194035935/NbjxSFASX)

Note: Meta 6月放出的自监督学习Cookbook：《A Cookbook of Self-Supervised Learning》，作者中包括了大佬Yann LeCun老师。  

Picture: [006tQI9Wly1hg7rcz5udyj318e0yk46x.jpg](https://weibo.cn//mblog/pic/Nbjwlsn79?rl=1)

#### [想本地玩一下meta上周发布的llama2的可以试下ollama这个项目（github.com/jm @小咖喱橙不辣](https://weibo.com/2194035935/NblcHizpG)

Note: 想本地玩一下meta上周发布的llama2的可以试下ollama这个项目（github.com/jmorganca/ollama/）M1 Pro的mbp实测可以在可接受的时间内本地运行7B的模型。试了下简单的代码是ok的（比如reverse 字符），稍微复杂点的就开始乱写了（毕竟只是个7B的模型）。    大家感兴趣的话可以瞅瞅

Picture: [006FwlnBgy1hg7yk41ykbj31j013mgyh.jpg](https://weibo.cn//mblog/pic/NblcmkQwm?rl=1)

Github: [github.com/jmorganca/ollama/](https://github.com/jmorganca/ollama/)

#### [一个Github上的计算机论文收集项目地址：github.com/0voice/computer_e @蚁工厂](https://weibo.com/2194035935/Nbo39CtjX)

Note: 一个Github上的计算机论文收集项目地址：github.com/0voice/computer_expert_paper1000+份计算机paper，卡耐基梅隆大学，哈佛，斯坦福，芝加哥大学，MIT，facebook，google，微软，Amazon，twitter等大牛一作主要分类：复杂而有序的数据结构、网络编程那些事儿、牛B的基础组件、中间件很重要、高大上的分布式、接近原始的LinuxOS、阅读工具

Picture: [82c654dfly1gsqlnuvkulj21710u0woe.jpg](https://weibo.cn//mblog/pic/KqbIj3Z9x?rl=1)

Github: [github.com/0voice/computer_expert_paper1000](https://github.com/0voice/computer_expert_paper1000)

#### [【JAX Implementation of Llama 2：Llama 2的JAX实现，目标包括用 @#开源#](https://weibo.com/1402400261/NbcBqpyd7)

Note: 【JAX Implementation of Llama 2：Llama 2的JAX实现，目标包括用JAX实现LLaMA 2模型，以实现在Google Cloud TPU上高效的训练和推理；开发高质量的代码库作为Transformer模型的示范实现，帮助NLP社区发现不同Transformer模型之间的常见错误和不一致性】’JAX Implementation of Llama 2 - JAX implementation of the Llama 2 model' Ayaka GitHub: github.com/ayaka14732/llama-2-jax  

Picture: [5396ee05ly8hg6wtviqufj21ay0oggqm.jpg](https://weibo.cn//mblog/pic/NbcBqpyd7?rl=1)

Github: [github.com/ayaka14732/llama-2-jax](https://github.com/ayaka14732/llama-2-jax)

#### [【扩散模型论文按其子领域分类的集合】'collection of diffusion model p @#开源#](https://weibo.com/1402400261/NbnwoE93A)

Note: 【扩散模型论文按其子领域分类的集合】'collection of diffusion model papers categorized by their subareas' kai wang GitHub: github.com/wangkai930418/awesome-diffusion-categorized   

Picture: [5396ee05ly8hg891ftyg6j20u70u0q6n.jpg](https://weibo.cn//mblog/pic/NbnwoE93A?rl=1)

Github: [github.com/wangkai930418/awesome-diffusion-categorized](https://github.com/wangkai930418/awesome-diffusion-categorized)

#### [【llama2-webui：在本地使用Gradio用户界面在GPU或CPU上运行Llama 2，支持 @#开源#](https://weibo.com/1402400261/NbnpwBvCA)

Note: 【llama2-webui：在本地使用Gradio用户界面在GPU或CPU上运行Llama 2，支持Linux/Windows/Mac系统。支持Llama-2-7B/13B/70B模型，支持8位和4位模式】'llama2-webui - Run Llama 2 locally with gradio UI on GPU or CPU from anywhere (Linux/Windows/Mac). Supporting Llama-2-7B/13B/70B with 8-bit, 4-bit. Supporting GPU inference (6 GB VRAM) and CPU inference.' Tom GitHub: github.com/liltom-eth/llama2-webui  回复:需要多大显存呢？可以教一下？回复:13B 8bit回复:多大的模型？Text generation web ui 本身也可以加载llama2，我部署了一个私人的很方便

Picture: [5396ee05ly8hg87vnb0ytj20zb0qadn0.jpg](https://weibo.cn//mblog/pic/NbnpwBvCA?rl=1)

Github: [github.com/liltom-eth/llama2-webui](https://github.com/liltom-eth/llama2-webui)

#### [Attention Is Off By One作者（Evan Miller）发表博文说他发现了Att @网页链接](https://weibo.com/2194035935/Nbsb89dDI)

Note: Attention Is Off By One作者（Evan Miller）发表博文说他发现了Attention 公式中的一个错误，这个错误已经被忽视了 8 年多了。所有 Transformer 模型（GPT、LLaMA 等）都会受到影响。 回复:👍下次发后续的时候请回复:嗯，改善softmax边界情况，但目前作者还没有实验，或者说正在找同伴一起论证，设计实验和发paper回复:👍🏻扫了眼原文，他改的不是attention，是softmax 然后计算中估计会更稳定，更少出现极大极小值，然后训练更稳定。估计量化时性能会更好。。至于原大模型性能估计提升很小乃至于没有提升（因为大力出奇迹）小一点模型会有用。。不知道猜的对不对阅后个人理解，作者结果：softmax公式的分母加上一个数值“1”（1+求和exp(…))，这样当输入为0(或趋于0），或者趋向于负无穷大时，概率等于（或趋于）0。而原有softmax在此类边界情况下无法“跳出来”，举例来说就是有的输入可能是无效的，其概率(或梯度?)应趋于或等于0，但原softmax做不到？…

Picture: [82c654dfly1hg8tlt353sj21o50yg79s.jpg](https://weibo.cn//mblog/pic/Nbsb89dDI?rl=1)

#### [Tuning Guide for AI on the 4th Generation Intel® X @网页链接](https://weibo.com/2144454703/NbtGnuvL7)

Note: Tuning Guide for AI on the 4th Generation Intel® Xeon® Scalable Processors2023年2月还在出用AMX做AI加速的教程  ... ... 应该没有大问题吧 回复:必须能超搞好了一个CPU说不定能超过t4

#### [Intel® Advanced Performance Extensions (Intel® APX @网页链接](https://weibo.com/2144454703/NbtBLq9zj)

Note: Intel® Advanced Performance Extensions (Intel® APX)doubles the number of general-purpose registers (GPRs) from 16 to 32Intel's New AVX10 （怎么回事儿， AMX 不香了么？），又回到AVX-512的路线上来了？   Intel的这种摇摆... ... 算是没有战略定性的表现么？ 

#### [现在多核CPU上的高性能编程，要秉持一些理念：（1）把多核之间通信看成网络通信，尽量减到最少。多核之 @转发[56]](https://weibo.com/1659957501/NbsDaefan)

Note: 现在多核CPU上的高性能编程，要秉持一些理念：（1）把多核之间通信看成网络通信，尽量减到最少。多核之间通信的开销是很大的，多线程加上spinlock、mutex lock这样几乎无视核间通信开销的编程模式，是不可能达到高性能的。具体怎么做呢？可以想想分布式数据库是怎么设计的？把一个表按照key来做range-sharding，由不同服务器节点负责处理不同分段。要想充分发挥多核CPU能力，也只能朝着分布式数据库这个方向去努力。例如，可以按照客户端来区分，不同客户端由不同核处理，每个核负责几个。或者按照TCP链接来区分，每个CPU核负责几个TCP链接，内部的需要处理的对象也要按核分开。尽量避免多核通过锁去竞争同一个内存对象。（3）把线程上下文切换看作是开关机。线程上下文切换的代价是很大的，两个线程之间通过pthread_cond_wait/pthread_cond_signal协调，如果两个线程在相同CPU核上，大概需要2-5us，若它们在不同核上就需要10-20us。具体怎么做呢？避免阻塞等待，尽量采用异步轮询模式。没有任务的时候，让CPU空转一会再调用阻塞syscall让CPU核闲下来，目的是省电。（4）把访问DRAM看做是访问硬盘，尽量减少DRAM访问。把有用的东西都放到cache里面，至少是放到LLC里面。不必要的数据，用non-temporal指令访问（MOVNTI, MOVNTQ，prefechnta， ...），避免cache污染。（5）把操作系统看做控制面，高性能的数据处理，尽量绕开OS内核。现在有人搞用户态TCP/IP协议栈，用户态NVME盘驱动，大概就是这个思路。异步轮询，轮一会没有再阻塞让出cpu。这个实现似乎有点难。低负载的时候，每20us来个IO，高负载的时候，每5us来个IO。"轮一会"很难定，不同业务模型下，时间轴上io到达分布都不一样。转发微博回复:省电和高性能之间，要权衡选择异步轮询不会很费电吗

#### [【AI00 RWKV Server：基于RWKV模型的推理API服务器】'AI00 RWKV Ser @#开源#](https://weibo.com/1402400261/NbusvBAWH)

Note: 【AI00 RWKV Server：基于RWKV模型的推理API服务器】'AI00 RWKV Server - A localized open-source AI server that is better than ChatGPT.' cgisky1980 GitHub: github.com/cgisky1980/ai00_rwkv_server   

Picture: [5396ee05ly8hg93no9af2j20vk0twtcl.jpg](https://weibo.cn//mblog/pic/NbusvBAWH?rl=1)

Github: [github.com/cgisky1980/ai00_rwkv_server](https://github.com/cgisky1980/ai00_rwkv_server)

#### [【Llama 2本地运行指南：介绍了如何在本地运行和调试Llama 2，不需要互联网连接，提供了三种 @爱可可-爱生活](https://weibo.com/2194035935/NbBqnCj13)

Note: 【Llama 2本地运行指南：介绍了如何在本地运行和调试Llama 2，不需要互联网连接，提供了三种开源工具在不同设备上运行Llama 2：Llama.cpp适用于Mac/Windows/Linux，Ollama适用于Mac，MLC LLM适用于iOS和Android】《A comprehensive guide to running Llama 2 locally - Replicate – Replicate》   这玩意要啥配置才能流畅运行呀？

#### [电子书《Effective Rust》一本名字和写作风格模仿《Effective C++》的书。Sc @网页链接](https://weibo.com/2194035935/NbFAhkBcC)

Note: 电子书《Effective Rust》一本名字和写作风格模仿《Effective C++》的书。Scott Meyers的原版《Effective C++》一书非常成功,因为它引入了一种新的编程书籍风格,聚焦于根据实际的C++软件开发经验总结出的一系列准则。值得注意的是,这些准则都是在解释它们为何必要的背景下阐述的 - 让读者自己决定在各自的场景下是否违反这些规则。《Effective C++》的第一版发布于1992年,那时尽管C++还很年轻,但它已经是一个复杂微妙的语言,包含了许多坑;拥有一本关于它不同特征交互的指南是必不可少的。Rust也是一个年轻的语言,但与C++不同,它显著地几乎没有坑。它强大一致的类型系统意味着,如果一个Rust程序能编译通过,它就已经有很大可能能工作 - 这在以前只在一些更学院派、不那么易用的语言如Haskell中观察到的现象。然而,这种安全性 - 既类型安全又内存安全 - 也是有代价的。Rust有一个学习曲线陡峭的名声,新用户必须经历与借用检查器作斗争、重新设计数据结构以及被生命周期弄得困惑的入门仪式。一个能编译通过的Rust程序可能有很大机会正常工作,但是让它编译通过的挣扎是真实的 - 即使Rust编译器给出了惊人地有帮助的错误诊断。因此,这本书的目标层次与其他Effective <语言>系列书籍略有不同;它包含了更多关于Rust中新增概念的条目,尽管官方文档已经包含了这些主题的良好介绍。这些条目标题如“理解...”和“熟悉...”。Rust的安全性也导致完全没有“永不...”这样的条目。如果你真的不应该做某事,编译器通常会阻止你这样做。也就是说,本书仍然假设对语言基础有所了解。它也假设使用的是2018版Rust和稳定工具链。代码片段和错误信息使用的具体rustc版本是1.60。Rust现在已经足够稳定(并有充分的后向兼容性保证),所以代码片段不太可能需要为后续版本做改动,但错误信息可能因你使用的编译器版本不同而有变化。本书还包含了许多关于C++的参考和对比,因为这可能是最接近的等效语言(特别是考虑到C++11的移动语义),也最有可能是Rust新用户以前接触过的语言。构成本书的各条目被分为六个部分:类型:围绕Rust核心类型系统的建议。概念:构成Rust设计的核心思想。依赖:关于使用Rust包生态系统的建议。工具:关于如何通过不仅仅依赖Rust编译器来改进代码库的建议。异步Rust:关于使用Rust异步机制的建议。超越标准Rust:当必须在Rust标准的安全环境之外工作时的建议。尽管“概念”部分可以说比“类型”部分更基础,但为了让从头读到尾的读者先建立些信心,它被故意放在了第二位。（Claude-2-100k翻译）回复:已保存到你的NotionI love Go， 更易上手

Picture: [82c654dfly1hga9i28ie4j21360tlwgi.jpg](https://weibo.cn//mblog/pic/NbFAhkBcC?rl=1)

#### [苹果M1统一内存架构真的很厉害吗？从AMD APU的名存实亡谈起（上）苹果M1统一内存架构真的很厉害 @网页链接](https://weibo.com/2144454703/NbMp2E5gK)

Note: 苹果M1统一内存架构真的很厉害吗？从AMD APU的名存实亡谈起（上）苹果M1统一内存架构真的很厉害吗？稀松平常的UMA（下）还是要读历史.... ....CPU, GPU，memory的各种排列组合，大家之前穷尽了.... ... 因此要看变量... ... 和增量 软件的滞后，导致硬件设计没有被push去解决关键问题

#### [【LLaMA 1/2的JAX实现】’JAX Implementation of LLaMA - JA @#开源#](https://weibo.com/1402400261/NbFGRlJFh)

Note: 【LLaMA 1/2的JAX实现】’JAX Implementation of LLaMA - JAX implementation of LLaMA, aiming to train LLaMA on Google Cloud TPU' Ayaka GitHub: github.com/ayaka14732/llama-jax   

Picture: [5396ee05ly8hgah9fbodcj21bk0p043d.jpg](https://weibo.cn//mblog/pic/NbFGRlJFh?rl=1)

Github: [github.com/ayaka14732/llama-jax](https://github.com/ayaka14732/llama-jax)

#### [【octox：一个类Unix操作系统，灵感来自xv6-riscv，但完全采用纯Rust实现。从内核、 @#开源#](https://weibo.com/1402400261/NbDUD0cWn)

Note: 【octox：一个类Unix操作系统，灵感来自xv6-riscv，但完全采用纯Rust实现。从内核、用户空间、mkfs到构建系统，尽可能使用安全的Rust编写，没有依赖外部crate】'octox - Unix-like OS in Rust inspired by xv6-riscv' Hayato Ohhashi GitHub: github.com/o8vm/octox  mark

Github: [github.com/o8vm/octox](https://github.com/o8vm/octox)

#### [【LightLLM：基于Python的LLM推理和服务框架，其轻量化设计、易于扩展和高性能值得注意】 @#开源#](https://weibo.com/1402400261/NbFJA8JLJ)

Note: 【LightLLM：基于Python的LLM推理和服务框架，其轻量化设计、易于扩展和高性能值得注意】'LightLLM - a Python-based LLM (Large Language Model) inference and serving framework, notable for its lightweight design, easy scalability, and high-speed performance.' ModelTC GitHub: github.com/ModelTC/lightllm  

Picture: [5396ee05ly8hgahfnjtf4j21c40sejye.jpg](https://weibo.cn//mblog/pic/NbFJA8JLJ?rl=1)

Github: [github.com/ModelTC/lightllm](https://github.com/ModelTC/lightllm)

#### [【Mentat：AI编程助手，能帮助完成任意编程任务，直接在命令行中使用。与Copilot不同，Me @#开源#](https://weibo.com/1402400261/NbFOcpdbi)

Note: 【Mentat：AI编程助手，能帮助完成任意编程任务，直接在命令行中使用。与Copilot不同，Mentat可以在多个位置和文件之间协调编辑，无需复制粘贴即可了解项目的上下文】’Mentat - The AI Coding Assistant' biobootloader GitHub: github.com/biobootloader/mentat   

Picture: [5396ee05ly8hgahrbw5wxj21c00ligp1.jpg](https://weibo.cn//mblog/pic/NbFOcpdbi?rl=1)

Github: [github.com/biobootloader/mentat](https://github.com/biobootloader/mentat)

#### [【UniDiffusion：基于diffusers和现有SOTA方法的扩散训练工具箱，包括Dream @爱可可-爱生活](https://weibo.com/6004911042/NbQlS4QhG)

Note: 【UniDiffusion：基于diffusers和现有SOTA方法的扩散训练工具箱，包括Dreambooth、Texual Inversion、LoRA、Custom Diffusion、XTI等】'UniDiffusion - A Diffusion training toolbox based on diffusers and existing SOTA methods, including Dreambooth, Texual Inversion, LoRA, Custom Diffusion, XTI, ....' PRIV-Creation GitHub: github.com/PRIV-Creation/UniDiffusion  

Picture: [5396ee05ly8hgbf65x2kpj219u0je76b.jpg](https://weibo.cn//mblog/pic/NbNnu2QEd?rl=1)

Github: [github.com/PRIV-Creation/UniDiffusion](https://github.com/PRIV-Creation/UniDiffusion)

#### [stability.ai发布了SDXL 1.0，这是文本到图像生成模型演进的下一个迭代版本。SDXL @转发[12]](https://weibo.com/2194035935/NbUNza5Xi)

Note: stability.ai发布了SDXL 1.0，这是文本到图像生成模型演进的下一个迭代版本。SDXL可以生成几乎任何艺术风格的高质量图像。SDXL 1.0特别适合鲜艳和准确的颜色，具有比其前身更好的对比度，照明和阴影。提示词可以更简单更智能详细介绍：stability.ai/blog/stable-diffusion-sdxl-1-announcement 无人深空背景

Picture: [82c654dfly1hgcbwp0h0vj21kj1kohdu.jpg](https://weibo.cn//mblog/pic/NbUNza5Xi?rl=1)

#### [【"Segment Anything"和"MobileSAM”的C++封装，运行时无Python依赖 @#开源#](https://weibo.com/1402400261/Nc7LKpBwr)

Note: 【"Segment Anything"和"MobileSAM”的C++封装，运行时无Python依赖】’Segment Anything CPP Wrapper - a pure C++ inference api for Segment Anything and MobileSAM, with no dependence on Python during runtime' by dinglufe GitHub: github.com/dinglufe/segment-anything-cpp-wrapper  你好，你感兴趣的“开源”已开通了超话社区～ 超话社区是微博旗下兴趣互动社区，快来与志同道合的小伙伴们一起交流互动吧！ 戳我进入>> 

Github: [github.com/dinglufe/segment-anything-cpp-wrapper](https://github.com/dinglufe/segment-anything-cpp-wrapper)

#### [【Transformer网络架构全面解析：详细解析了Transformer的各个关键组成部分，从注意 @网页链接](https://weibo.com/1402400261/NcebUEOsO)

Note: 【Transformer网络架构全面解析：详细解析了Transformer的各个关键组成部分，从注意力机制到编-解码器结构，探讨了利用Transformer的大型语言模型在自然语言处理之外的应用，探讨了该架构当前面临的挑战以及未来的发展方向，文章还提供了一份开源实现和其他补充资源的精选列表】《The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture》  

Picture: [5396ee05ly8hgepchpoqbj21180l3dil.jpg](https://weibo.cn//mblog/pic/NcebUEOsO?rl=1)

#### [【LLM延迟优化：CTranslate2最快且容易使用，vLLM很快仅支持特定版本CUDA，TGI比 @网页链接](https://weibo.com/1402400261/Nce94iGlh)

Note: 【LLM延迟优化：CTranslate2最快且容易使用，vLLM很快仅支持特定版本CUDA，TGI比vLLM有更多的功能】《Hamel’s Blog - Optimizing LLM latency》   

Picture: [5396ee05ly8hgenvd0l16j212l0u00y7.jpg](https://weibo.cn//mblog/pic/Nce94iGlh?rl=1)

#### [【Large language models, explained with a minimum o @网路冷眼](https://weibo.com/1240212845/NcgMkF3JI)

Note: 【Large language models, explained with a minimum of math and jargon】https:///www.understandingai.org/p/large-language-models-explained-with 大型语言模型，用最少的数学和术语进行解释。 

Picture: [663aa05aly1hgeyxqueydj20k808ajrm.jpg](https://weibo.cn//mblog/pic/NcgjgsMj4?rl=1)

#### [《CPU性能分析与优化》读书笔记（5）1. 分支预测错误频繁发生时，会显著降低性能。现代CPU对分支 @小川CD](https://weibo.com/2194035935/NciccmGc1)

Note: 《CPU性能分析与优化》读书笔记（5）1. 分支预测错误频繁发生时，会显著降低性能。现代CPU对分支预测错误的开销约为15~20个时钟周期。建议只关注分支预测错误超过10%的情况。2. 在某些应用负载中，延迟敏感的代码执行频率较低，导致功能代码和相关数据在缓存中因老化而被换出。为确保这些代码保留在缓存中，可以进行缓存预热，即周期性运行延迟敏感型代码。3. 浮点值非规范时，在有浮点运算的应用程序中可能会导致异常场景。对非规范数值进行运算会极大地降低性能。4. 如果应用程序频繁受到系统管理中断或BIOS中断的干扰，并且这些中断持续10~100毫秒，那么大部分优化可能会失去意义。5. 缓存锁定是将部分CPU缓存预留给特定数据集的技术，可优化应用程序的内存延迟。6. 在优化多线程程序时，有时不能仅查看单个线程以识别热点，因为每个线程可能都有自己的热点。7. 阿姆达尔定律表明，并行程序的加速效果受其串行组件的限制。8. 通用可伸缩性定律描述了计算单元（线程）之间的通信对性能的影响，随着系统规模扩大，通信开销将阻碍性能，在某个临界点之后，性能开始下降。9. 优化多线程程序还需要检测和优化资源争用和一致性影响。10. 一个线程可能显示高CPU利用率和高IPC（每指令周期执行的指令数），但实际上可能只是在某个锁上自旋。推荐使用有效CPU利用率，该指标仅基于线程的有效执行时间。11. 有效CPU利用率只统计了有效时间，不包含并行运行系统引入的开销和自旋时间。12. 多线程可以充分利用资源，但过多的线程可能会浪费大量CPU时间，因为某些线程可能在等待其他线程结束，或者时间浪费在上下文切换上。13. 等待时间是指由于同步阻塞或同步锁的API导致的线程等待。等待时间是线程粒度的，总等待时间可能超过应用程序的运行时间。14. 高度竞争的同步对象可能导致同步等待时间增加，而大量的抢占等待时间可能是线程过多的问题。15. 自旋时间也属于等待时间，内核的同步原语实现更倾向于在锁上自旋一段时间，然而过多的自旋时间表明有效工作机会被浪费。16. 最著名的缓存一致性协议是MESI，它表示4种状态：修改（M）、独有（E）、共享（S）、无效（I）。17. 使用C++原子变量有助于解决真共享导致的数据竞争问题，但高效序列化原子变量访问可能影响性能。另一种解决真共享问题的方法是使用线程本地存储。18. False Sharing是多线程应用程序性能问题的主要来源，因此现代分析工具都支持检测这种情况。真共享和False Sharing都表现为“内存绑定”问题。19. 通过内存对象对齐填充可以消除False Sharing。20. 缓存行在M或E状态下维持的时间越长（即跨缓存的数据共享越少），多线程应用程序的一致性损耗就越低。

#### [《CPU性能分析与优化》读书笔记（5）1. 分支预测错误频繁发生时，会显著降低性能。现代CPU对分支 @转发[25]](https://weibo.com/1202332555/NchYS5vm3)

Note: 《CPU性能分析与优化》读书笔记（5）1. 分支预测错误频繁发生时，会显著降低性能。现代CPU对分支预测错误的开销约为15~20个时钟周期。建议只关注分支预测错误超过10%的情况。2. 在某些应用负载中，延迟敏感的代码执行频率较低，导致功能代码和相关数据在缓存中因老化而被换出。为确保这些代码保留在缓存中，可以进行缓存预热，即周期性运行延迟敏感型代码。3. 浮点值非规范时，在有浮点运算的应用程序中可能会导致异常场景。对非规范数值进行运算会极大地降低性能。4. 如果应用程序频繁受到系统管理中断或BIOS中断的干扰，并且这些中断持续10~100毫秒，那么大部分优化可能会失去意义。5. 缓存锁定是将部分CPU缓存预留给特定数据集的技术，可优化应用程序的内存延迟。6. 在优化多线程程序时，有时不能仅查看单个线程以识别热点，因为每个线程可能都有自己的热点。7. 阿姆达尔定律表明，并行程序的加速效果受其串行组件的限制。8. 通用可伸缩性定律描述了计算单元（线程）之间的通信对性能的影响，随着系统规模扩大，通信开销将阻碍性能，在某个临界点之后，性能开始下降。9. 优化多线程程序还需要检测和优化资源争用和一致性影响。10. 一个线程可能显示高CPU利用率和高IPC（每指令周期执行的指令数），但实际上可能只是在某个锁上自旋。推荐使用有效CPU利用率，该指标仅基于线程的有效执行时间。11. 有效CPU利用率只统计了有效时间，不包含并行运行系统引入的开销和自旋时间。12. 多线程可以充分利用资源，但过多的线程可能会浪费大量CPU时间，因为某些线程可能在等待其他线程结束，或者时间浪费在上下文切换上。13. 等待时间是指由于同步阻塞或同步锁的API导致的线程等待。等待时间是线程粒度的，总等待时间可能超过应用程序的运行时间。14. 高度竞争的同步对象可能导致同步等待时间增加，而大量的抢占等待时间可能是线程过多的问题。15. 自旋时间也属于等待时间，内核的同步原语实现更倾向于在锁上自旋一段时间，然而过多的自旋时间表明有效工作机会被浪费。16. 最著名的缓存一致性协议是MESI，它表示4种状态：修改（M）、独有（E）、共享（S）、无效（I）。17. 使用C++原子变量有助于解决真共享导致的数据竞争问题，但高效序列化原子变量访问可能影响性能。另一种解决真共享问题的方法是使用线程本地存储。18. False Sharing是多线程应用程序性能问题的主要来源，因此现代分析工具都支持检测这种情况。真共享和False Sharing都表现为“内存绑定”问题。19. 通过内存对象对齐填充可以消除False Sharing。20. 缓存行在M或E状态下维持的时间越长（即跨缓存的数据共享越少），多线程应用程序的一致性损耗就越低。回复:我也没找到有哪个指标可以直接反映“有效CPU时间”，通过perf工具，结合具体的程序业务，应该可以分析处理。回复:未成功收藏到notion，Notion 创建页面失败第10和11条确实碰到过，做了很多努力把IPC提上来了，但最后结果没有明显变化，请问哪些指标可以区分出“有效CPU利用率”呢？谢谢~

#### [Making FFmpeg Easier 地址：amiaopensource.github.io/f @转发[107]](https://weibo.com/2194035935/NcmZfqcLw)

Note: Making FFmpeg Easier 地址：amiaopensource.github.io/ffmprovisr/FFmpeg 是一个用于操作视听文件的强大工具。不幸的是，它也有一个陡峭的学习曲线，特别是对于不熟悉命令行界面的用户。这个页面列出了很多场景下FFmpeg 的命令以及其详细解释，以便更多的人能够获得 FFmpeg 的好处。 我现在都直接问chatgpt

Picture: [82c654dfly1hgfsez468lj20ir1nvdmd.jpg](https://weibo.cn//mblog/pic/NcmZfqcLw?rl=1)

#### [深度学习所需的矩阵微积分本文试图解释你为了理解深层神经网络的训练所需的全部矩阵微积分知识。我们假设您 @蚁工厂](https://weibo.com/2194035935/Ncytc4JgL)

Note: 深度学习所需的矩阵微积分本文试图解释你为了理解深层神经网络的训练所需的全部矩阵微积分知识。我们假设您没有超出微积分1的数学知识,并在需要时提供了帮助您复习必要数学知识的链接。请注意,在开始实际训练和使用深度学习之前,您不需要理解这些材料;相反,这些材料是针对那些已经熟悉神经网络基础并希望加深对基础数学的理解的人。如果在过程中你感到卡壳,不要担心——只需回头重新阅读前一节,并尝试书写和演算一些例子。

Picture: [82c654dfly1hggxpjjywej20qw109479.jpg](https://weibo.cn//mblog/pic/NcxYuw2Kn?rl=1)

#### [电子书 <图解算法小抄>作者： linwu算法被称为程序的灵魂，因为优秀的算法能在处理海量数据时保持 @网页链接](https://weibo.com/2194035935/NcBbN5vw3)

Note: 电子书 <图解算法小抄>作者： linwu算法被称为程序的灵魂，因为优秀的算法能在处理海量数据时保持高速计算能力。本笔记深入讲解数据结构和算法，内容系统完整，覆盖了各种数据结构和算法，包括但不限于字符串匹配算法、分治算法、回溯算法、深度优先搜索 (DFS) 和贪心算法，非常适合想要深入理解数据结构和算法的学习者。采用了"应用场景 -> 数据结构或算法 -> 剖析原理 -> 分析实现步骤 -> 代码实现"的教学步骤，力求通俗易懂。回复:已收藏到你的Notion回复:成功保存至你的Notion回复:已保存至你的notionMark

Picture: [82c654dfly1hgh38n6sz8j20u317a7cu.jpg](https://weibo.cn//mblog/pic/NcBbN5vw3?rl=1)

#### [这个仓库收集了多模态推理领域的论文、数据集和代码等资源，对于想学习多模态知识的朋友来说是非常宝贵的资 @BigYe程普](https://weibo.com/2194035935/NcGW5dCL2)

Note: 这个仓库收集了多模态推理领域的论文、数据集和代码等资源，对于想学习多模态知识的朋友来说是非常宝贵的资源。github.com/atfortes/Awesome-Multimodal-Reasoning 

Picture: [006qCzTzly1hgi4q35h3vj31co0m84d3.jpg](https://weibo.cn//mblog/pic/NcGVLmkIH?rl=1)

Github: [github.com/atfortes/Awesome-Multimodal-Reasoning](https://github.com/atfortes/Awesome-Multimodal-Reasoning)

#### [面向程序员的卡尔曼滤波器非数学介绍“卡尔曼滤波器确实很精妙。如果你以前从未听说过它们,那么一个非常直 @网页链接](https://weibo.com/2194035935/NcPgL93Y6)

Note: 面向程序员的卡尔曼滤波器非数学介绍“卡尔曼滤波器确实很精妙。如果你以前从未听说过它们,那么一个非常直观(也可以说是简化了的)的方式来思考它们就是将其视为一个漏斗,你可以从多个嘈杂的源头注入信息,将这些信息浓缩成一个更准确的统计数据。不要担心,如果这些听起来还很模糊。我们马上会把这个陈述剖析成一个更易于理解的例子,以期进一步增强我们的直观理解。可以很明确地说,没有比数学更好的工具来学习和推理卡尔曼滤波器了。但同样真实的是,卡尔曼滤波器的基础数学很具挑战性,包含了线性代数、概率论和微积分的组成部分。因此,它可能并不容易为所有人所理解。本文的目标是希望为您提供一个可以理解的直观感受,这可能会激励您在这个主题上深入研究。”good good看看

Picture: [82c654dfly1hgj99xceisj21kw0o4gp5.jpg](https://weibo.cn//mblog/pic/NcPgL93Y6?rl=1)

#### [Nvidia Grace-hopper  FP8  4Pflops/500W（8Tflops/w)， @转发](https://weibo.com/2144454703/NcPwyrSGb)

Note: Nvidia Grace-hopper  FP8  4Pflops/500W（8Tflops/w)， 这个性能很顶AMD MI300  FP8 2.5Pflops/850w（3Tflops/w），也是不错的。 不过FP16就差距最大，连FP64都没有打过...... AMD正经做超算的不过850w，认真的么？ 系统做成什么样子呢？看memory带宽，互联带宽，又是很能大的样子，感觉实测，应该性能不错。 但是CPU配比明显要少，不知道是24 Zen4性能与72 arm V2相当，还是GPU性能弱，就配弱一点的CPU？ 还是大家理解的CPU：GPU的ratio不一样无论如何， 192 GB of HBM3还是炸裂的高规格AMD's Instinct MI300 Series has a peak FP16 performance of 306 TFLOPS, which is significantly lower than NVIDIA's Grace Hopper Superchip which offers 1,979 TFLOPSwww.reddit.com/r/Amd/comments/149dbpr/how_amds_mi300_series_may_revolutionize_ai/回复:难道AMD是实的？回复:不过也差距挺大了nv这个是带sparse的，要除以二吧

Picture: [7fd1c82fgy1hgj8yy8qa8j20wp0h077b.jpg](https://weibo.cn//mblog/pic/NcPwyrSGb?rl=1)

#### [【ML/AI Research Papers Solved：精通机器学习/人工智能论文摘要集锦】’M @#开源#](https://weibo.com/1402400261/NcAR5q44b)

Note: 【ML/AI Research Papers Solved：精通机器学习/人工智能论文摘要集锦】’ML/AI Research Papers Solved - This repository contains everything you need to become proficient in ML/AI Research and Research Papers' Ignito GitHub: github.com/Coder-World04/ML-AI-Research-Papers---Solved  

Picture: [5396ee05ly8hghhg15uhvj20do0iw75f.jpg](https://weibo.cn//mblog/pic/NcAR5q44b?rl=1)

Github: [github.com/Coder-World04/ML-AI-Research-Papers---Solved](https://github.com/Coder-World04/ML-AI-Research-Papers---Solved)

#### [【Google Search Results in Python：Python搜索引擎结果集成包，支 @#开源#](https://weibo.com/1402400261/NcKRS9KKM)

Note: 【Google Search Results in Python：Python搜索引擎结果集成包，支持Google, Bing, Baidu, Yandex, Yahoo, Home Depot, eBay等】’Google Search Results in Python - Google Search Results via SERP API pip Python Package' serpapi GitHub: github.com/serpapi/google-search-results-python  

Picture: [5396ee05ly8hgippey8ctj21ci0l677f.jpg](https://weibo.cn//mblog/pic/NcKRS9KKM?rl=1)

Github: [github.com/serpapi/google-search-results-python](https://github.com/serpapi/google-search-results-python)

#### [【CoreML实现的NafNet去模糊模型推理】’NafNet deblur CoreML mode @#开源#](https://weibo.com/1402400261/NcKWKaqzw)

Note: 【CoreML实现的NafNet去模糊模型推理】’NafNet deblur CoreML model - NAFNet model inference using CoreML' Vadim Titko GitHub: github.com/Vadbeg/nafnet-coreml   

Picture: [5396ee05ly8hgiq6m0f3zj21070dxwfx.jpg](https://weibo.cn//mblog/pic/NcKWKaqzw?rl=1)

Github: [github.com/Vadbeg/nafnet-coreml](https://github.com/Vadbeg/nafnet-coreml)

#### [电子书《Probabilistic Machine Learning: Advanced Topic @蚁工厂](https://weibo.com/2194035935/NcTcvckLx)

Note: 电子书《Probabilistic Machine Learning: Advanced Topics》概率机器学习：高级主题 英文版地址：github.com/probml/pml2-book在项目release里可以下载本书pdf作者Kevin Patrick Murphy是谷歌的大牛。之前还出过同系列的书《Machine learning: a probability perspective》《概率机器学习导论》也是开放下载

Picture: [82c654dfly1h4talw92rkj20u00xwgpi.jpg](https://weibo.cn//mblog/pic/LFgxv4ETD?rl=1)

Github: [github.com/probml/pml2-book](https://github.com/probml/pml2-book)

#### [阿里的大模型通义千问的开源版来了地址：github.com/QwenLM/Qwen-7B通义千问-7 @转发[90]](https://weibo.com/2194035935/NcYuA1Mgy)

Note: 阿里的大模型通义千问的开源版来了地址：github.com/QwenLM/Qwen-7B通义千问-7B（Qwen-7B） 是阿里云研发的通义千问大模型系列的70亿参数规模的模型。Qwen-7B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。同时，在Qwen-7B的基础上，使用对齐机制打造了基于大语言模型的AI助手Qwen-7B-Chat。下载巨慢回复:已收藏到notion[666]转发微博

Picture: [82c654dfly1hgkdzqjzkej220e0wdgss.jpg](https://weibo.cn//mblog/pic/NcYuA1Mgy?rl=1)

Github: [github.com/QwenLM/Qwen-7B](https://github.com/QwenLM/Qwen-7B)

#### [tl-rtc-file-tool:基于webrtc的媒体流传输工具 .地址：github.com/t @转发[130]](https://weibo.com/2194035935/NcYJcyBRT)

Note: tl-rtc-file-tool:基于webrtc的媒体流传输工具 .地址：github.com/tl-open-source/tl-rtc-file特性: p2p网页在线文件传输，跨终端，不限平台，内网不限速，支持私有部署，支持多文件拖拽发送，支持本地屏幕录制，远程屏幕共享，远程音视频通话，密码房间，直播，oss云存储，中继服务设置，webrtc检测，统计，文字传输，公共聊天，远程画板，丰富的后台管理，实时执行日志展示，机器人告警通知等功能转发微博m转发微博回复:已保存至你的Notionm看起来不错tl-rtc-file-tool:基于webrtc的媒体流传输工具 . github.com/tl-open-source/tl-rtc-file 特性: p2p网页在线文件传输，跨终端，不限平台，内网不限速，支持私有部署，支持多文件拖拽发送，支持本地屏幕录制，远程屏幕共享，远程音视频通话，密码房间，直播

Picture: [82c654dfly1hgkf16wov1j21wa11laim.jpg](https://weibo.cn//mblog/pic/NcYJcyBRT?rl=1)

Github: [github.com/tl-open-source/tl-rtc-file](https://github.com/tl-open-source/tl-rtc-file)

#### [TinyRPC 是一款基于 C++11 标准开发的小型异步 RPC 框架。TinyRPC 的核心代码 @转发[73]](https://weibo.com/2194035935/NcYJYdETS)

Note: TinyRPC 是一款基于 C++11 标准开发的小型异步 RPC 框架。TinyRPC 的核心代码应该也就几千行样子，尽量保持了简洁且较高的易读性。地址：github.com/Gooddbird/tinyrpc麻雀虽小五脏俱全，从命名上就能看出来，TinyRPC 框架主要用义是为了让读者能快速地、轻量化地搭建出具有较高性能的异步RPC 服务。至少用 TinyRPC 搭建的 RPC 服务能应付目前大多数场景了。TinyRPC 的核心思想有两个：    让搭建高性能 RPC 服务变得简单    让异步调用 RPC 变得简单m请问一下，这个TinyRPC支持python语言吗？

Github: [github.com/Gooddbird/tinyrpc](https://github.com/Gooddbird/tinyrpc)

#### [6个值得拥有的C++学习网站你看过几个？还有哪些你觉得不错的学习网站？快来评论区补充啦~   Cou @#鹅厂技术干货#](https://weibo.com/7483028645/NcJj6uu94)

Note: 6个值得拥有的C++学习网站你看过几个？还有哪些你觉得不错的学习网站？快来评论区补充啦~   Coursera：有全球各大著名高校的c++课程。有没有 C 语言的又来偷师了嗯，计算机都挺高超的回复:回复:哈哈哈回复:学到了[开学季]回复:[开学季]码住回复:哈哈哈哈哈回复:下次一定！

Picture: [008aq1Apgy1hgigwuh1qaj30u0140qqv.jpg](https://weibo.cn//mblog/pic/NcJj6uu94?rl=1)

#### [“峰云就她了”作者的一系列技术分析PPTGithub地址：github.com/rfyiamcool @蚁工厂](https://weibo.com/2194035935/NdsaF4Fxp)

Note: “峰云就她了”作者的一系列技术分析PPTGithub地址：github.com/rfyiamcool/share_ppt内容包括：Etcd的设计与实现、git的那些事儿、分布式消息推送、Kafka的设计与实现、TCP的那些事儿、Golang高级编程技巧、GRPC的那些事儿、分布式任务系统、优雅的编程者、分布式行情推送系统(golang)、Redis经验之谈、http2和quic的那些事儿、kubernetes的那些事儿、istio的那些事儿、Service Mesh的那些事儿、RedisCluster那些事儿、golang高级讲义、golang高性能实战、mysql快速讲义、微服务那些事儿、异步io调度框架的实现、cdn设计原理、分析mysql acid设计实现、Python Gil全局锁那些事儿、Redis设计实现、分布式一致性raft实现原理、Python高级内存管理、美妙的多进程管理、集群管理

Picture: [82c654dfly1gt7zjt4fycj20px0elq4w.jpg](https://weibo.cn//mblog/pic/KsjX5tAef?rl=1)

Github: [github.com/rfyiamcool/share_ppt](https://github.com/rfyiamcool/share_ppt)

#### [最近翻译的系列课程：《Building Systems with the ChatGPT API》由 @宝玉xp](https://weibo.com/1727858283/NcQR8txCX)

Note: 最近翻译的系列课程：《Building Systems with the ChatGPT API》由OpenAI官方和DeepLearningAI共同推出的关于如何用ChatGPT的API构建常见应用，以及如何用好Prompt完成复杂的任务，并且保证其安全性和生成质量。课程地址： 微博播放列表：B站播放列表：YouTube播放列表：www.youtube.com/watch?v=1SZOGp1D17E&list=PLiuLMb-dLdWKjX8ib9PhlCIx1jKMNxMpy🔗Google 的《Generative AI learning path》Google出品的生成式AI的原理，浅显易懂。课程地址：微博播放列表：B站播放列表：YouTube播放列表：www.youtube.com/watch?v=tbLOQ533Up8&list=PLiuLMb-dLdWJPpybrCYNhi6D9Vd4vz16i🔗《基于LangChain的LLM开发》有LangChain创始人主讲，详细介绍了LangChain的原理和设计系统，并且讲了如何用LangChain操作大语言模型完成常见任务。课程地址：http://t.cn/A6pqFTDo微博播放列表：http://t.cn/A6pfw839B站播放列表：http://t.cn/A6pmbefNYouTube播放列表：www.youtube.com/watch?v=gUcYC0Iuw2g&list=PLiuLMb-dLdWIYYBF3k5JI_6Od593EIuEG🔗《扩散模型是如何工作的》现在很火的AI生成图片的技术，你所熟知的MIdJourney、Stable Diffusion都是基于扩散模型，这个系列教程详细介绍了扩散模型的工作原理。课程地址：http://t.cn/A6pqFTDo微博播放列表：http://t.cn/A6ptoMEeB站播放列表：http://t.cn/A6pmbefpYouTube播放列表：www.youtube.com/watch?v=oSmlciqXOaU&list=PLiuLMb-dLdWKh6Oq46LZ3pLwlmYuMYl_g🔗《LangChain：构建与数据对话的聊天机器人》由LangChain创始人主讲的如何利用LangChain实现一个基于自己数据的问答机器人，同时详细介绍了嵌入、数据检索等基本原理。课程地址：http://t.cn/A60OBUEG微博播放列表：http://t.cn/A6pkgIHEYouTube播放列表：youtube.com/watch?v=JMScDV251ho&list=PLiuLMb-dLdWJX_EWk4RtQAjjrPLWaswTVB站播放列表：http://t.cn/A60OBUEb《大语言模型微调之道》这是由Sharon Zhou主讲的，教你如何在自己的数据上进一步微调自己的LLM，以完成特定的任务。课程地址：http://t.cn/A6OwZ6qc微博播放列表：http://t.cn/A6OwtXsEYouTube：www.youtube.com/watch?v=3apAPNXogAQ&list=PLiuLMb-dLdWKtPM1YahmDHOjKN_a2UievB站：http://t.cn/A6OwtaTp《大型语言模型与生成式AI》这是一门亚马逊的人工智能科学家开的课程，对于大语言模型和生成式AI介绍的非常清楚，很适合入门。课程地址：www.coursera.org/learn/generative-ai-with-llms/lecture/sAKto/rlhf-fine-tuning-with-reinforcement-learning微博播放列表：http://t.cn/A6puSD2x油管：www.youtube.com/watch?v=X7r4rL2T2lg&list=PLiuLMb-dLdWL4KBaU3FTM5f_oMcSvXcZwB站：http://t.cn/A602slTT//:新翻译完结的课程：《LangChain：构建与数据对话的聊天机器人》 

#### [：Awesome CTO为CTO精心策划并提供意见的资源清单，重点关注初创企业！github.com @#开源项目推荐#](https://weibo.com/1727858283/NcZpZvp3f)

Note: ：Awesome CTO为CTO精心策划并提供意见的资源清单，重点关注初创企业！github.com/kuchin/awesome-cto 

Github: [github.com/kuchin/awesome-cto](https://github.com/kuchin/awesome-cto)

#### [系列技术博文：开源LLM的历史，完整的回顾了开源LLM的发展历程和技术变化开源LLM的历史:早期时光 @网页链接](https://weibo.com/2194035935/NdAtldAPC)

Note: 系列技术博文：开源LLM的历史，完整的回顾了开源LLM的发展历程和技术变化开源LLM的历史:早期时光(第一部分)开源LLM的历史:更好的基础模型(第二部分)开源LLM的历史:模仿和对齐(第三部分) 回复:成功收藏至你的Notion回复:已收藏至你的Notion

Picture: [82c654dfly1hgp1nkj1zqj20lt0oetd4.jpg](https://weibo.cn//mblog/pic/NdAtldAPC?rl=1)

#### [基于声学的侧信道攻击论文下载：arxiv.org/pdf/2308.01074.pdf随着深度学习的 @转发[44]](https://weibo.com/2194035935/NdAzLjEgn)

Note: 基于声学的侧信道攻击论文下载：arxiv.org/pdf/2308.01074.pdf随着深度学习的最新进展、麦克风的普及以及通过个人设备进行在线服务的兴起，声学侧信道攻击对键盘构成的威胁比以往任何时候都更加严重。本文介绍了一个实用的最先进深度学习模型的实现，以便利用智能手机集成麦克风对笔记本键击进行分类。当训练一台附近手机记录的键击时，分类器获得了95%的精度。当训练Zoom视频会议软件记录的键击时，达到了93%的精度。我们的结果证明了使用现成的设备和算法进行这些侧信道攻击的实用性。我们还讨论了一系列减轻方法来保护用户免受这些攻击的影响。得多高的地位才值得被这么攻击, 就怕这到了各种自媒体就开始添油加醋

Picture: [82c654dfly1hgp24gxfd3j20yj0qy79f.jpg](https://weibo.cn//mblog/pic/NdAzLjEgn?rl=1)

#### [Arm官方出的漫画： How does a mobile GPU work?pdf下载：armkei @蚁工厂](https://weibo.com/2194035935/NdBrcCpL3)

Note: Arm官方出的漫画： How does a mobile GPU work?pdf下载：armkeil.blob.core.windows.net/developer/Files/pdf/graphics-and-multimedia/how-does-a-mobile-gpu-work.pdf   

Picture: [82c654dfly1hgp0vn4rzgj211b1hakh1.jpg](https://weibo.cn//mblog/pic/NdAiti3qP?rl=1)

#### [【FasterTransformer：高度优化的基于Transformer的编码器和解码器组件的脚本 @#开源#](https://weibo.com/1402400261/NcTJm4bVP)

Note: 【FasterTransformer：高度优化的基于Transformer的编码器和解码器组件的脚本和配置】'FasterTransformer - Transformer related optimization, including BERT, GPT' Void Main GitHub: github.com/void-main/FasterTransformer   

Picture: [5396ee05ly8hgjst47lnij211m0u0wir.jpg](https://weibo.cn//mblog/pic/NcTJm4bVP?rl=1)

Github: [github.com/void-main/FasterTransformer](https://github.com/void-main/FasterTransformer)

#### [【TorchFort：基于NVIDIA GPU的HPC程序的在线深度学习接口】'TorchFort  @#开源#](https://weibo.com/1402400261/NcTMwej8F)

Note: 【TorchFort：基于NVIDIA GPU的HPC程序的在线深度学习接口】'TorchFort - An Online Deep Learning Interface for HPC programs on NVIDIA GPUs' NVIDIA GitHub: github.com/NVIDIA/TorchFort   

Picture: [5396ee05ly8hgjt5xv98ej21co0taqal.jpg](https://weibo.cn//mblog/pic/NcTMwej8F?rl=1)

Github: [github.com/NVIDIA/TorchFort](https://github.com/NVIDIA/TorchFort)

#### [【高效AIGC相关资源列表】’Awesome Efficient AIGC - A list of  @#开源#](https://weibo.com/1402400261/NcU1a3UzD)

Note: 【高效AIGC相关资源列表】’Awesome Efficient AIGC - A list of papers, docs, codes about efficient AIGC. This repo is aimed to provide the info for efficient AIGC research, including language and vision' Haotong Qin GitHub: github.com/htqin/awesome-efficient-aigc   

Picture: [5396ee05ly8hgju7vzb05j20xc0u0grt.jpg](https://weibo.cn//mblog/pic/NcU1a3UzD?rl=1)

Github: [github.com/htqin/awesome-efficient-aigc](https://github.com/htqin/awesome-efficient-aigc)

#### [【关于人工智能基础和概念的精选文章集，涵盖了从构建神经网络、训练网络到评估结果的整个过程】《Aman @网页链接](https://weibo.com/1402400261/NcYB49o5i)

Note: 【关于人工智能基础和概念的精选文章集，涵盖了从构建神经网络、训练网络到评估结果的整个过程】《Aman's AI Journal • Primers • AI》   有点搞笑

Picture: [5396ee05ly8hgkegebe46j20ur0u0ad4.jpg](https://weibo.cn//mblog/pic/NcYB49o5i?rl=1)

#### [【大型语言模型相关资源列表，关于大型模型训练或服务的系统论文、框架、代码和工具的总结】’Awesom @#开源#](https://weibo.com/1402400261/Nd1RRmI48)

Note: 【大型语言模型相关资源列表，关于大型模型训练或服务的系统论文、框架、代码和工具的总结】’Awesome Large Model (LM) System - Summary of system papers/frameworks/codes/tools on training or serving large model' ModelTC GitHub: github.com/ModelTC/awesome-lm-system   

Picture: [5396ee05ly8hgksvfsh07j20vl0u0aes.jpg](https://weibo.cn//mblog/pic/Nd1RRmI48?rl=1)

Github: [github.com/ModelTC/awesome-lm-system](https://github.com/ModelTC/awesome-lm-system)

#### [【大型语言模型(LLM)全面介绍讲座：LLM是什么，可以用LLM做什么，可以在LLM的基础上构建什么 @网页链接](https://weibo.com/1402400261/Nd887mNyp)

Note: 【大型语言模型(LLM)全面介绍讲座：LLM是什么，可以用LLM做什么，可以在LLM的基础上构建什么，如何训练LLM，以及安全、有效和合乎道德地使用LLM所涉及的一些挑战】《Catching up on the weird world of LLMs》   

Picture: [5396ee05ly8hglkgoxqe2j20u012ktfl.jpg](https://weibo.cn//mblog/pic/Nd887mNyp?rl=1)

#### [【Llama 2 Powered By ONNX：ONNX优化版Llama 2实现】’Llama 2 @#开源#](https://weibo.com/1402400261/NdaD5AXTG)

Note: 【Llama 2 Powered By ONNX：ONNX优化版Llama 2实现】’Llama 2 Powered By ONNX' by Microsoft GitHub: github.com/microsoft/Llama-2-Onnx     

Picture: [5396ee05ly8hglvden5y6j21460u0dkk.jpg](https://weibo.cn//mblog/pic/NdaD5AXTG?rl=1)

Github: [github.com/microsoft/Llama-2-Onnx](https://github.com/microsoft/Llama-2-Onnx)

#### [英伟达在世界顶级计算机图形学会议SIGGRAPH上宣布一系列重磅更新，包括下一代GH200超级芯片平 @互联网的那点事](https://weibo.com/1240212845/NdKEtwfi4)

Note: 英伟达在世界顶级计算机图形学会议SIGGRAPH上宣布一系列重磅更新，包括下一代GH200超级芯片平台、AI Workbench、OpenUSD等。这些创新将人工智能、虚拟世界、加速、模拟、协作等融合到一起。 1. GH200超级芯片平台：这个全新平台拥有多种配置，能够处理世界上最复杂的生成式工作负载，包括大语言模型、推荐系统和向量数据库等等。双核心方案包括一台配备了144个Arm Neoverse核心并搭载了282GB HBM3e内存的服务器，可以提供8 petaflops的AI算力。 2. AI Workbench：英伟达发布了全新的NVIDIA AI Workbench，来帮助开发和部署生成式AI模型。AI Workbench为开发者提供了一个统一且易于使用的工具包，能够快速在PC或工作站上创建、测试和微调模型，并无缝扩展到几乎任何数据中心、公有云或NVIDIA DGX Cloud上。 3. OpenUSD：OpenUSD（Universal Scene Description）提供了一个开源，通用的场景描述格式，使不同品牌、不同类型的3D设计软件可以无障碍的协作。基于OpenUSD这个开源的3D图像编辑格式，5家公司（苹果，皮克斯，Adobe，Autodesk，英伟达）成立了AOUSD联盟，进一步推动了3D图像业界采用OpenUSD格式。此外，英伟达还与Hugging Face成功达成了合作，开发者可以通过Hugging Face平台直接获得英伟达DGX Cloud AI超算的加持，从而更加高效地完成AI模型的训练和微调。

#### [IBM developerworks 中文网站文章备份Github地址：github.com/lab @蚁工厂](https://weibo.com/2194035935/NdPcgbBfA)

Note: IBM developerworks 中文网站文章备份Github地址：github.com/labulaka521/ibm_bak部分目录如图。 之前IBM Developer社区已关闭了 

Picture: [82c654dfly1gta8o7pcsmj20s71jdqg5.jpg](https://weibo.cn//mblog/pic/KsCgoAZYi?rl=1)

Github: [github.com/labulaka521/ibm_bak](https://github.com/labulaka521/ibm_bak)

#### [内存分段与分页 这篇文章《内存分段与分页》 详细阐述了这些年不断进阶的内存发展史，分蛮荒时代、青铜时 @网页链接](https://weibo.com/2194035935/NdNKArbFm)

Note: 内存分段与分页 这篇文章《内存分段与分页》 详细阐述了这些年不断进阶的内存发展史，分蛮荒时代、青铜时代、文明时代进行说明，并解答了很多关于内存的周边知识。本文可以顺带解决如下几个问题：    地址总线、数据总线、控制总线是什么？CPU 如何通过地址总线找到内存地址？    CPU 和内存之间的高速缓存引发的缓存一致性问题是怎么回事？    16 位 CPU 是如何操作 1M 内存空间的 (2^16=64kb)？32 位 CPU 是如何操作 64G 内存空间的 (2^32=4G)？他们的原理一样吗？    分段内存管理，里面的段指的是什么？    除了虚拟地址、物理地址，还有线性地址和逻辑地址，它们是什么？    两个进程的虚拟地址相同，是如何指向不同的物理地址的？熟悉的 0x05回复:成功保存到你的notion[开学季]

Picture: [82c654dfly1hgqkjf5x0uj20a81gdwqz.jpg](https://weibo.cn//mblog/pic/NdNKArbFm?rl=1)

#### [在推上看到了一个叫Sam Foreman的小哥分享了他的教学Slides，非常惊艳！(链接不让放，见 @光头怪博士](https://weibo.com/1727858283/NdGTGdOSm)

Note: 在推上看到了一个叫Sam Foreman的小哥分享了他的教学Slides，非常惊艳！(链接不让放，见图二截图)Sam是美国Argonne国家实验室的计算科学家，主要领域是机器学习在物理领域内的应用。这个报告内容是关于格点规范理论中的机器学习蒙特卡罗方法 (不要问我这是什么。。。)。Slides的内容不仅详尽而耐心，制作水平也是相当的高。从字体、配色、插图都很让人喜欢。我特别喜欢、也在尝试的一种技巧就是在讲公式的时候用字体把公式中的表达和语言解释联系起来。这个方法在Sam的报告里也体现得特别好。但更好玩的是，这一整套报告文档就是一个网页，可以像PPT一样播放的同时还有很多功能，比如在线标注、比如黑板，等等。特意去问了一下，Sam介绍说：- 这个Slides是用一个叫Quarto的平台提供的基于Revealjs的功能制作的 (图六)： 按网页介绍，Quarto是一个”开源科学与技术发表系统“，大概就是基于JupyterNotebook实现开源交互式的科学结果展示。看了一下，有一定的学习成本，但似乎不是很可怕，值得尝试。- 而报告里的很多示意图是使用 Excalidraw 在线白板工具制作的： 这个工具我之前注意到过，但没有特别使用过。- 如果你对Sam报告的具体样式也感兴趣，他把所用的Matplotlib绘图样式、CSS文件、甚至是用JS画报告开头的小动画的文件也都分享出来了。微博不喜欢Github链接，大家可以到他的推上找一下，也可以直接看他的 l2hmc-qcd，lattice23，以及 grid-worms-animation 这三个Repositories的内容。在他的个人主页上还可以找到很多很棒的报告内容。比较喜欢在推上逛的一个原因也是因为能真的学到一些东西，而且能遇到很多这样乐于分享的学者。回复:👍🏻我之前用SlideV，但是推荐用Gamma（直接上手、无需编码）

Picture: [6932890fgy1hgir96jxxyj20q40q4n6f.jpg](https://weibo.cn//mblog/pic/NcLrviVq3?rl=1)

#### [为开发人员提供的最佳 LLMOps 工具的精选列表。地址：github.com/tensorchor @转发[54]](https://weibo.com/2194035935/Ne4jVfJ8X)

Note: 为开发人员提供的最佳 LLMOps 工具的精选列表。地址：github.com/tensorchord/Awesome-LLMOpsLLMOps是专注于微调现有基础模型和将这些优化模型部署为产品一部分所需的运营能力和基础设施。 回复:已保存到你的notion

Picture: [82c654dfly1hgslp84l74j20nk1lih11.jpg](https://weibo.cn//mblog/pic/Ne4jVfJ8X?rl=1)

Github: [github.com/tensorchord/Awesome-LLMOpsLLMOps](https://github.com/tensorchord/Awesome-LLMOpsLLMOps)

#### [这个开源的墨水屏手表太好看了，疯狂心动。 github.com/qewer33/qpaperOS  @酱紫表](https://weibo.com/1691468715/Ne5o4r1GZ)

Note: 这个开源的墨水屏手表太好看了，疯狂心动。 github.com/qewer33/qpaperOS 

Picture: [bb0e59bfly1hgrm9cxcvzj218g0xc123.jpg](https://weibo.cn//mblog/pic/NdVs1DMPd?rl=1)

Github: [github.com/qewer33/qpaperOS](https://github.com/qewer33/qpaperOS)

#### [【FlagEmbedding：将任意文本映射为低维稠密向量，以用于检索、分类、聚类或语义匹配等任务， @#开源#](https://weibo.com/1402400261/NdsVSgzmT)

Note: 【FlagEmbedding：将任意文本映射为低维稠密向量，以用于检索、分类、聚类或语义匹配等任务，并可支持为大模型调用外部知识】'FlagEmbedding - Open-source Embeddings, can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search’ FlagOpen GitHub: github.com/FlagOpen/FlagEmbedding  

Picture: [5396ee05ly8hgo4diyo6mj212p0u00xu.jpg](https://weibo.cn//mblog/pic/NdsVSgzmT?rl=1)

Github: [github.com/FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

#### [【AI辅助开发者工具大列表，用来帮助开发者完成诸如代码补全、重构、调试、文档编写等任务。列表按功能分 @#开源#](https://weibo.com/1402400261/NdsXz8oZW)

Note: 【AI辅助开发者工具大列表，用来帮助开发者完成诸如代码补全、重构、调试、文档编写等任务。列表按功能分类，包括IDE(集成开发环境)、助手、代理、文档生成、持续集成机器人、基础模型、代理平台、OpenAI插件、搜索和测试等】’Awesome AI-Powered Developer Tools - Curated list of AI-powered developer tools.' James Murdza GitHub: github.com/jamesmurdza/awesome-ai-devtools  

Picture: [5396ee05ly8hgo4en0n8wj21c20nsgow.jpg](https://weibo.cn//mblog/pic/NdsXz8oZW?rl=1)

Github: [github.com/jamesmurdza/awesome-ai-devtools](https://github.com/jamesmurdza/awesome-ai-devtools)

#### [电子书《The Little Book of Deep Learning》pdf格式，大小适合手机阅 @网页链接](https://weibo.com/2194035935/NecF32h1E)

Note: 电子书《The Little Book of Deep Learning》pdf格式，大小适合手机阅读。作者的话（机翻）：人工智能的当前进步阶段是由Krizhevsky等人（2012年）发现的：一个简单结构的人工神经网络（已经出现了20多年LeCun等人，1989）通过扩大100倍并在类似扩大的数据集上进行培训，简单击败了复杂的最先进图像识别方法。这一突破得益于图形处理器（GPU），这种大规模、高度并行的计算设备用于实时图像合成，随后被用于人工神经网络领域。自那以后，“深度学习”在这些网络的结构、训练策略和专门的硬件上的创新，导致它们的规模和利用的训练数据数量呈指数级增长，应用领域涉及技术的各个方面，从计算机视觉和机器人到语音和自然语言处理等。虽然深度学习的大部分内容不难理解，但它结合了线性代数、微积分、概率、优化、信号处理、编程、算法和高性能计算等不同的组件，使其难以学习。这本小书不尝试穷尽深度学习的所有内容，而是仅限于了解一些重要模型所必需的背景知识。这种方法被证明非常受欢迎，自宣布推特上后下载PDF文件的数量达到25万次。求翻译版本回复:成功收藏到你的Notion转了就等于看了，看了就等于会了竟然还没有出翻译版本吗

Picture: [82c654dfly1hgtq2njrw1j20q91c5n1p.jpg](https://weibo.cn//mblog/pic/NecF32h1E?rl=1)

#### [RoCE环境中，构建无损以太网有两种机制ECN和PFC，为什么需要两种机制? 是否可以只使用其中一种 @转发[3]](https://weibo.com/1659957501/Ne8qcDUwu)

Note: RoCE环境中，构建无损以太网有两种机制ECN和PFC，为什么需要两种机制? 是否可以只使用其中一种机制?PFC能保证不丢包，但是问题比较多。例如，六台服务器，a b c之间发生拥塞，d e f之间没有拥塞，那么pfc可能导致d e f之间流量也收到影响。ecn就没有这个问题。但是，ecn不能保证绝对不丢包，所以要ecn和pfc一起用。我理解ecn 才是主要防线，pfc 只是最后的一道保底防线，pfc 杀敌1000，自损800…两种特性都用了还有可能发生链路错误丢包，这时就需要重传丢包

Picture: [62f0f0fdgy1hgt7faz64qj20i00ns16h.jpg](https://weibo.cn//mblog/pic/Ne8qcDUwu?rl=1)

#### [垃圾回收算法是如何设计的？本文从底层的垃圾回收算法开始，着重去阐释不同垃圾回收器在算法设计和实现时的 @蚁工厂](https://weibo.com/2194035935/NeeR9EBho)

Note: 垃圾回收算法是如何设计的？本文从底层的垃圾回收算法开始，着重去阐释不同垃圾回收器在算法设计和实现时的一些技术细节，去探索「why」这一部分，通过对比不同的垃圾回收算法和其实现，进一步感知目前垃圾回收的发展脉络。 

Picture: [82c654dfly1h53pt9cmfjj20eb0nmgmh.jpg](https://weibo.cn//mblog/pic/M0FVepoVT?rl=1)

#### [【Stable-Diffusion-Burn：Stable Diffusion v1.4的Rust移 @#开源#](https://weibo.com/1402400261/Ndt6iuUVo)

Note: 【Stable-Diffusion-Burn：Stable Diffusion v1.4的Rust移植版】’Stable-Diffusion-Burn - Stable Diffusion v1.4 ported to Rust's burn framework' Gadersd GitHub: github.com/Gadersd/stable-diffusion-burn   

Picture: [5396ee05ly8hgo54azo4hj21ck0oidlj.jpg](https://weibo.cn//mblog/pic/Ndt6iuUVo?rl=1)

Github: [github.com/Gadersd/stable-diffusion-burn](https://github.com/Gadersd/stable-diffusion-burn)

#### [探索 Linux v0.01 的内部结构Linux 的第一个版本 v0.01 非常小。它仅包含 10 @网页链接](https://weibo.com/2194035935/NelpYmTkI)

Note: 探索 Linux v0.01 的内部结构Linux 的第一个版本 v0.01 非常小。它仅包含 10,239 行代码。除去注释和空行，只有 8,670 行。它足够小，易于理解，是了解类 UNIX 操作系统内核内部结构的良好起点。一篇不太长的文章 马马good回复:成功保存到你的notion

#### [下面这行bash可以找出 go.mod 文件中的依赖项有多少是可能不再维护的存储库以及这些库在Git @转发[35]](https://weibo.com/2194035935/NewsTbGc1)

Note: 下面这行bash可以找出 go.mod 文件中的依赖项有多少是可能不再维护的存储库以及这些库在Github上有多少颗星。for repo in $(cat ./go.mod | grep "\tgithub.com/" | sed 's/\t//' | sed 's/\ v.*//' | sed 's|github\.com/\([^/]*\/[^/]*\).*|\1|p' | uniq); do; printf "Last commit: %10s | %5s ⭐ | %s\n" "$(gh api /repos/$repo/commits | jq -r '.[0].commit.author.date' | sed 's/\(.*\)T.*/\1/')" "$(gh api /repos/$repo -q '.stargazers_count')" "github.com/$repo"; done;作者是Frederico Bittencourt，详细介绍：blog.fredrb.com/2023/08/13/bash-one-liner-gomod/[苦涩]编程语言的包管理有一点不好，就是胡乱引用，好坏难辨。依赖一堆没听过也不了解的依赖，总让人怀疑系统的稳定性和安全性。这一点在PHP，js, go里尤其突出。用perl写单行更简洁回复:这倒是回复:已保存至你的Notion回复:成功保存至notion[赢牛奶][苦涩]编程语言的包管理有一点不好，就是胡乱引用，好坏难辨。依赖一堆没听过也不了解的依赖，总让人怀疑系统的稳定性和安全性。这一点在PHP，js, go里尤其突出。眼花用perl写单行更简洁

Picture: [82c654dfly1hgw05kf8tjj21dl0ot7lp.jpg](https://weibo.cn//mblog/pic/NewsTbGc1?rl=1)

#### [群里有个朋友文到，目前开源方面finetune, serving最火的项目是不是主要还是ggml,  @祝威廉二世](https://weibo.com/2194035935/NeDANwPUF)

Note: 群里有个朋友文到，目前开源方面finetune, serving最火的项目是不是主要还是ggml, vllm, peft这些？我这里说说我的一些看法。首先Peft算是一个比较底层的finetune 库，仅限于模型实现部分(比如对QLora算法的支持)，但微调显然需要数据处理以及各种trick在，一般上面都需要再包装下， 这个时候就有了诸如 fastchat , firely，LLaMA-Efficient-Tuning 等项目的生存土壤，他们完成端到端的fintune; serving方面的话vLLM,TGI 性能较好，vLLM支持的模型较少，并且对模型的衍生版本支持比较差，需要用户动手能力比较强。TGI 架构比较复杂，由Rust前端和Python后端构成。 Transformers/Optimumn 性能较差，但通用性最好,几乎支持所有主流模型。此外像 vLLM 也支持ray做backend(但做的比较糙)。这里还值得一提的是 AnyScale 官方发起的 Aviary 项目，该项目试图将主流的这些serving框架整合到 ray serving体系里去。Byzer-LLM这个项目的野心会更大一些，涵盖了数据处理，模型训练，serving 以及模型管理，SQL集成等方方面面。 其中通过基于Ray+Deepspeed 完成了预训练支持，微调则默认集成了QLora，支持Llama2, Falcon等。在Serving上，几乎可以和 Aviary对标,基于Ray集成诸如 vLLM,DeepSpeed Inference，Transformers等主流serving框架。另外谈价 ggml, 实际上对于CPU推理这块我目前是不看好的，对于超过13B的模型，目前来看CPU的Latency 和 吞吐瓶颈很大。在当前GPU性能都存在瓶颈的情况下，CPU 只能当做远景目标来看。

#### [卡内基梅隆大学的公开课：Algorithms in the "Real World" 真实世界中的算 @网页链接](https://weibo.com/2194035935/NeFSztFeu)

Note: 卡内基梅隆大学的公开课：Algorithms in the "Real World" 真实世界中的算法该课程介绍了算法和理论如何在“现实世界”中应用。该课程将涵盖算法背后的理论以及如何应用理论的案例研究。课程按主题组织，主题会随着年份的变化而变化。  CMU在我们这里 感觉很自豪CMU在我们这里 感觉很自豪回复:已保存到你的Notion[开学季]回复:已收藏至你的notion mark

Picture: [82c654dfly1hgx51xeb0uj20qc0g2474.jpg](https://weibo.cn//mblog/pic/NeFSztFeu?rl=1)

#### [电子书 《Advanced Bash-Scripting Guide》高级 Bash 脚本指南，深入 @网页链接](https://weibo.com/2194035935/NeIorg5GG)

Note: 电子书 《Advanced Bash-Scripting Guide》高级 Bash 脚本指南，深入探索 shell 脚本艺术本教程假设您之前没有脚本或编程知识，但可以快速向中级/高级教学水平迈进。 告诉我，你们也是收藏了但是不看[苦涩]回复:已保存到notion把目录喂给chatgpt 了，以后就用这个session 来问 bash script 问题，不知道能不能比一般session 答得好，回复:成功保存到notion

Picture: [82c654dfly1hgx6h01mv9j20rv1no7tx.jpg](https://weibo.cn//mblog/pic/NeIorg5GG?rl=1)

#### [中国哲学书电子化计划中国哲学书电子化计划是一个线上开放电子图书馆，为中外学者提供中国历代传世文献。收 @蚁工厂](https://weibo.com/2194035935/NeJ2bE94n)

Note: 中国哲学书电子化计划中国哲学书电子化计划是一个线上开放电子图书馆，为中外学者提供中国历代传世文献。收藏的文本已超过三万部著作，并有五十亿字之多。 里面很多信息都是中英文对照的。中文也有简繁两种。 

Picture: [82c654dfly1gtewbdhpf2j20br1j0q6w.jpg](https://weibo.cn//mblog/pic/Ktxnt2spO?rl=1)

#### [【You're the OS!：这是一个游戏，你将扮演一台计算机的操作系统，需要管理进程、内存和输入 @爱可可-爱生活](https://weibo.com/1888981347/NeJIogmFV)

Note: 【You're the OS!：这是一个游戏，你将扮演一台计算机的操作系统，需要管理进程、内存和输入/输出事件。在游戏中，你需要避免让进程空闲时间过长，否则用户会变得不耐烦并重启你】'You're the OS! - A game where you are a computer's OS and you have to manage processes, memory and I/O events.' Pier-Luc Brault  GitHub: github.com/plbrault/youre-the-os  

Picture: [5396ee05ly8hgxedqrd48j20zk0k0ad2.jpg](https://weibo.cn//mblog/pic/NeGBkb8Th?rl=1)

Github: [github.com/plbrault/youre-the-os](https://github.com/plbrault/youre-the-os)

#### [Stable Diffusion Web UI  中文文档 - - 想自己玩的 可以看看教程不喜欢动 @网页链接](https://weibo.com/6326715527/NeAISdjZX)

Note: Stable Diffusion Web UI  中文文档 - - 想自己玩的 可以看看教程不喜欢动手的，看  

#### [转发 RS 7️⃣（twitter.com/rsrs7777777）的介绍高质量YouTube频道的 @kurzgesagt](https://weibo.com/1727858283/NejEtpXuF)

Note: 转发 RS 7️⃣（twitter.com/rsrs7777777）的介绍高质量YouTube频道的推文twitter.com/rsrs7777777/status/1690206795632197632我觉得Youtube高质量英文Documentry频道早已经超越了电视频道，今天分享绍几个我觉得好的频道，这些频道都相同特点：分享有趣新鲜知识，视频制作不无聊，并且提及内容均有出处(reference)。1，Kurzgesagt youtube.com/可能是Youtube质量最高的动画科学频道，Kurzgesagt是德语In a nutshell的意思，内容包括太空，生物，历史，代表作包括介绍人体免疫系统，费米悖论等。2，ColdFusion TV youtube.com/Voice over讲故事频道，主要覆盖商业趣事，代表作包括介绍Theranos的整个骗局，WeWork如何崩盘等。3，CGP Grey youtube.com/另一个大名鼎鼎动画频道，代表作包括介绍分析各种选举制度的利弊，统治阶级如何维系统治等，其中我最喜欢的是Rule for Rulers这一集：youtube.com/watch?v=rStL7niR7gs4，3Blue1Brownyoutube.com/硬核数学频道，用简单风趣的动画把复杂的数学问题讲得引人入胜，其中有两集我很喜欢，分别是用数学模型解释COVID的传播（http://t.cn/A601HIf8）和两个物体的撞击为什么能算出来圆周率（http://t.cn/A601HIfR）。5，Wendover Productions youtube.com/内容包括经济，地理，运输，代表作包括介绍航空母舰如何日常运作，航空公司如何划分仓位等。回复:VOX也不错 yputube 纪录片国内营销号很多搬运这些的，剪辑翻译后配上AI语音谢谢推荐3Blue1Brown特别牛，可视化做得很好回复:谢谢分享还有很多好频道，Veritasium, Mark Rober, smarter every day

Picture: [66fd066bgy1hgukzf21q6j20tv0xp4hd.jpg](https://weibo.cn//mblog/pic/NejEtpXuF?rl=1)

#### [arm IPO的新闻总结1，先是一篇总结性的文章    重点数字：  net sales incre @网页链接](https://weibo.com/2144454703/New6plsqJ)

Note: arm IPO的新闻总结1，先是一篇总结性的文章    重点数字：  net sales increasing 40% year-over-year to $2 billion2， Intel可能也会投一点钱么？其实没有永远的朋友，也没有永远的敌人， Intel和arm，arm和高通，真是相杀相爱，长长久久    3，AWS也会投资  4，arm估值 600亿 ~800亿？ 老黄当年出400亿，大家都觉得价格没问题，但是合并成的巨无霸更打不过了，因此玩了命的阻击了 ... ... 现在这个估值，大约是表扬老黄的眼光吧，然后来个众筹arm   我的二分钱： arm的估值， 靠AI大旗么？ 现在是不是什么都得扯上AI两句？在AI路线上，走grace-hopper的CPU+GPU 路线，还是Sapphire Rapids的路线，在CPU上AI算力？ 还是成年人都要？ AWS来个Graviton+Traintium，不过看看arm的销售情况，arm的销售翻译了半导体市场的研发投入，这么高的销售额意味着2年后的市场是非常繁荣，竞争激烈的。arm还是受大家认可服务器领域大佬看好ARM生态嘛？

#### [智东西的公开课的PPT，真是大全，而且B站应该是有回放的。 我还没来得及看，不过自己很难收集到这么多 @网页链接](https://weibo.com/2144454703/New8alrKV)

Note: 智东西的公开课的PPT，真是大全，而且B站应该是有回放的。 我还没来得及看，不过自己很难收集到这么多的资料，自取吧。 自媒体，还是很敬业的，我是喜欢且赞成的。 AI芯片峰会（2020-2022）全套PPT及「AI芯片技术公开课」全套PPT 

Picture: [7fd1c82fgy1hgw44s1et9j20oc0j2qij.jpg](https://weibo.cn//mblog/pic/New8alrKV?rl=1)

#### [第一代芯片MTIA v1于2020年开始设计，其采用台积电7nm制程工艺，运行频率800MHz，TD @网页链接](https://weibo.com/2144454703/NexnJcQrr)

Note: 第一代芯片MTIA v1于2020年开始设计，其采用台积电7nm制程工艺，运行频率800MHz，TDP仅为25W，INT8整数运算能力为102.4 TOPS，FP16浮点运算能力为51.2 TFLOPS。在架构方面，这款芯片由PE运算单元、片上缓存、片外缓存、传输接口、控制单元等组成。~~~~~ 2TFlops（FP16）/W  还是不错的，不过，为什么是25W/35W？ 这是什么设备的功耗限制？被动散热不上强制？

#### [我最近撰写的机器学习研究课件和我的在线直播侧重于Learning Theory里面当然很多数学啦 。 @徐亦达教授](https://weibo.com/2194035935/NePwN8wuQ)

Note: 我最近撰写的机器学习研究课件和我的在线直播侧重于Learning Theory里面当然很多数学啦 。相信很多小伙伴们会觉得有点难度。所以我刚刚上传了几个机器学习数学基础讲义。虽然都是一些老掉牙的话题，但我还是喜欢以我自己的口味来撰写。希望会对你们有用 （用latex 画图确实很费时间) 课件在我的GitHub下载Class 1: Model Evaluation 分类模型评估的常见概念和技术，包括自举抽样、混淆矩阵、接收器操作特征 (ROC) 曲线Class 2: Decision Tree 除了决策树的所有基础知识之外，我还在此说明中添加了χ2 测试部分。Class 3: Simple Bayes 本课件旨在对概率的基本概念、贝叶斯定理、概率的图形模型提供直观的解释Class 4: Regression 这课件是为了解释最简单的回归模型：线性回归和多项式回归，以及一些评估回归性能的技术，尤其是确定系数 (CoD) 方法Class 5: Neural Network 首先，我展示了三个不同的最后输出层模型：逻辑回归、多项式和线性回归。然后我展示了梯度下降的概念。主要部分是展示一个基本的全连接神经网络，最后是一个卷积神经网络。Class 6: Unsupervised Learning 本课件描述了无监督学习中的一些常见主题。从最明显的方法（如聚类）到主题建模和传统的词嵌入（如 word2vec 算法）

Picture: [006ibAEngy1h58o1mix53j30v91vo1fm.jpg](https://weibo.cn//mblog/pic/M1jcB0gao?rl=1)

#### [一篇博客《编写一个最小的 64 位 Hello World》把gcc编译的大小为16K的hellow @蚁工厂](https://weibo.com/2194035935/NeSkseDr9)

Note: 一篇博客《编写一个最小的 64 位 Hello World》把gcc编译的大小为16K的helloworld程序一步步缩小到170个字节，并可在任意的 x64 架构的 64 位 Linux上运行。代码Github地址：github.com/cj1128/tiny-x64-helloworld博客： 

Github: [github.com/cj1128/tiny-x64-helloworld](https://github.com/cj1128/tiny-x64-helloworld)

#### [A philosophy of software design - John Ousterhout《 @网页链接](https://weibo.com/2194035935/NgceOtrYx)

Note: A philosophy of software design - John Ousterhout《软件设计哲学》的读书笔记（英文）“我大约18个月前读过《软件设计的哲学》。这是一本结构良好、简洁明了的读物，关于如何管理软件设计中的复杂性。我认为其中建议的方法并不适用于所有情况（John Ousterhout自己也这么说过），但我在书中发现了很多所描述的问题，并且发现它提供了一些有用的方式来阐述概念在代码审查中（比如：在一个代码库中添加一个浅层函数是否会增加复杂性，复杂性是否可以被降低到一个实现中，或者在哪里有用一致性的代码）。下面是我从这本书中得出的笔记和我对一些想法的看法（不包括我曾经参与过的一些真实代码的有趣参考）。我将这些笔记发布出来，因为这是我重新阅读并记住这些信息的好方法。”转发微博是架构师吗？回复:成功保存到你的notion 

Picture: [82c654dfly1hh8t5duyfoj219u0rdn76.jpg](https://weibo.cn//mblog/pic/NgceOtrYx?rl=1)

#### [GRUB 主引导记录的磁盘编辑器视图和代码注释“注意：虽然在这个主引导记录中使用的代码对于任何拥有搜 @网页链接](https://weibo.com/2194035935/Ngc3s2t5u)

Note: GRUB 主引导记录的磁盘编辑器视图和代码注释“注意：虽然在这个主引导记录中使用的代码对于任何拥有搜索引擎和一点汇编知识的人来说都不是秘密（因为它是所有开源代码），但我决定让更多人看到GRUB的MBR（或称为“第一阶段”）的汇编指令清单和注释，以帮助计算机用户了解当他们的主引导记录被GRUB的引导代码取代时会发生什么。”

Picture: [82c654dfly1hh8su2yr9sj217m18ehdt.jpg](https://weibo.cn//mblog/pic/Ngc3s2t5u?rl=1)

#### [【TINY DREAM： C++ 实现的Stable Diffusion无依赖嵌入式头文件库，主要关 @#开源#](https://weibo.com/1402400261/Nh9CsqXOl)

Note: 【TINY DREAM： C++ 实现的Stable Diffusion无依赖嵌入式头文件库，主要关注 CPU 效率和更小的内存占用，在普通消费级硬件上运行速度相当快，仅需要 1.7 ~ 5.5 GB 的 RAM 即可执行，不强制使用 GPU】’TINY DREAM - An embedded, Header Only, Stable Diffusion C++ implementation' PixLab | Symisc Systems GitHub: github.com/symisc/tiny-dream  

Picture: [5396ee05ly8hhg5vevalsj20u00vl7cc.jpg](https://weibo.cn//mblog/pic/Nh9CsqXOl?rl=1)

Github: [github.com/symisc/tiny-dream](https://github.com/symisc/tiny-dream)

#### [【depyf: Python反编译库，从字节码到源代码】'depyf: decompile pyth @#开源#](https://weibo.com/1402400261/Nh9DFcDA0)

Note: 【depyf: Python反编译库，从字节码到源代码】'depyf: decompile python functions, from bytecode to source code!' youkaichao GitHub: github.com/youkaichao/depyf   

Picture: [5396ee05ly8hhg60sp796j21540u0422.jpg](https://weibo.cn//mblog/pic/Nh9DFcDA0?rl=1)

Github: [github.com/youkaichao/depyf](https://github.com/youkaichao/depyf)

#### [【cNeRF：使用 LibTorch 实现的神经辐射场(NeRF)的C++简洁版】’cNeRF -  @#开源#](https://weibo.com/1402400261/NhfQztQkp)

Note: 【cNeRF：使用 LibTorch 实现的神经辐射场(NeRF)的C++简洁版】’cNeRF - A concise C++ implementation of Neural Radiance Fields (NeRF) using LibTorch.' Rafael Anderka GitHub: github.com/rafaelanderka/cNeRF   你好，你感兴趣的“开源”已开通了超话社区～ 超话社区是微博旗下兴趣互动社区，快来与志同道合的小伙伴们一起交流互动吧！ 戳我进入>> 

Picture: [5396ee05ly8hhgxg6vrvgj219z0u0ade.jpg](https://weibo.cn//mblog/pic/NhfQztQkp?rl=1)

Github: [github.com/rafaelanderka/cNeRF](https://github.com/rafaelanderka/cNeRF)

#### [【Web-RWKV：纯WebGPU/Rust实现的RWKV语言模型】’Web-RWKV - Impl @#开源#](https://weibo.com/1402400261/NhijsvRXu)

Note: 【Web-RWKV：纯WebGPU/Rust实现的RWKV语言模型】’Web-RWKV - Implementation of the RWKV language model in pure WebGPU/Rust.' cryscan GitHub: github.com/cryscan/web-rwkv   

Picture: [5396ee05ly8hhh8cuwcadj20w10u0djp.jpg](https://weibo.cn//mblog/pic/NhijsvRXu?rl=1)

Github: [github.com/cryscan/web-rwkv](https://github.com/cryscan/web-rwkv)

#### [大语言模型微调之道6——训练过程在本视频中，我们深入探讨了大语言模型的微调训练流程。从基础的PyTo @#大语言模型微调之道#](https://weibo.com/1727858283/Nhh6bCnuK)

Note: 大语言模型微调之道6——训练过程在本视频中，我们深入探讨了大语言模型的微调训练流程。从基础的PyTorch代码到使用HuggingFace和Lamini的Llama库，我们一步步演示了如何进行模型的微调。不仅如此，我们还分享了如何有效地制导模型的输出，使其更加专注于特定的任务或话题。以下是视频的主要亮点：1. 基础训练概念：了解训练数据、损失值计算、权重更新等基本概念。2. 训练的核心代码：探索PyTorch中的训练代码块，并了解其工作原理。3. 使用Llama库：只需三行代码，即可在外部GPU上训练模型。4. 模型微调示例：我们将展示如何微调模型以改进其在特定任务上的性能。5. 主题纠偏：学习如何训练模型，使其始终关注相关的主题或内容。6. 模型评估：查看模型微调前后的表现，并了解其重要性。7. 免费的在线训练：介绍如何在Lamini平台上免费训练您的模型。无论您是初学者还是经验丰富的研究者，这个视频都会为您提供大量有价值的信息。不要错过，马上观看！课程地址：YouTube：www.youtube.com/watch?v=3apAPNXogAQ&list=PLiuLMb-dLdWKtPM1YahmDHOjKN_a2UievB站： 

#### [我日常翻译字幕时，第一件事是要将视频的字幕识别出来，用的最多的还是 WhisperX ，但我是Mac @网页链接](https://weibo.com/1727858283/Nhmm7lR1W)

Note: 我日常翻译字幕时，第一件事是要将视频的字幕识别出来，用的最多的还是 WhisperX ，但我是Mac电脑没有GPU加速，所以没法直接用 WhisperX，好在有Google Colab，免费够用。我常用的场景主要有两种：1. YouTube视频提取字幕2. 其他来源的视频文件提取字幕我为此写了两个不同的Notebooks，在我的GitHub都有下载：github.com/JimLiu/whisper-subtitles基本用法1. 从GitHub下载你要用的Nodebook文件，扩展名是 .ipynb 2. 注册登录 Google Colab，免费版是够用的colab.research.google.com3. 从菜单的 file -> upload notebook 打开上传界面4. 上传你下载好的Notebook，也就是.ipynb 提取YouTube字幕的这个Notebook很简单：github.com/JimLiu/whisper-subtitles/blob/main/whisperx_youtube_subtitle.ipynb1. 复制粘贴你要提取字幕的YouTube视频地址在右边的输入框（参考图三）2. 其他参数可选，Prompt参数我经常用，因为它可以帮助更好的识别一些专有名词，比如ChatGPT。3. 从菜单中选择 Runtime - Run all4. 完成后会自动下载注意除了 srt 文件外，我还下载了json文件，这个对大多数人没有用，不过对我自己很重要，可以无视这个文件。另一个本地上传视频文件的Notebook稍微麻烦一点，要从左侧把视频文件上传上去，一定要等上传完了才能执行最后一步。github.com/JimLiu/whisper-subtitles/blob/main/whisperx_for_uploading_file.ipynb如果想要节约一点时间的话，就可以在上传文件的同时，先安装WhisperX（图7）嫌麻烦就可以等文件上传好了直接 Run All参数的话可能你需要修改一下语言或者Prompt帮你识别专有名词 如果是中文的话，在language参数里面输入“zh”，如果要区分简体繁体，在Prompt里面加一点文字和标点符号引导一下应该就可以。英语的话，medium.en 模型速度会快一点点，中文的话还是选择large-v2老师，请问whisperX比whisper有何独到之处？我看您二者都安装了好像只用了X，何故？谢谢试了一个日文的，输出srt错误，JSON文件倒是正常下载下来有11k。所以这套流程的输入是任意视频，输出是机翻好的srt字幕轴？中间还有必须人工矫正的必要环节吗？感觉麻烦的点在于专有名词要校对。我感觉我注册的这个github浪费了，啥都看不懂转发微博码住码住回复 :现在还不行哈哈回复:要先翻译成英文，然后用文字转语音TTS技术生成，比如微软的本地的中文教材视频，或中文.srt，要翻译成英文字幕，甚至 AI 英文配音，有什么好的方案吗？你现在也能噢太厉害啦👍感谢，收藏回复:原来如此。多谢！主要原因是在于断句，WhisperX：1. 可以按照完整句子分割；2. 它提供一个JSON文件有每一个词的时间戳，这样我在二次分割的时候就不需要去对时间轴

Picture: [66fd066bly8hhhq4fjjwrj20gn08vgmf.jpg](https://weibo.cn//mblog/pic/Nhmm7lR1W?rl=1)

Github: [github.com/JimLiu/whisper-subtitles](https://github.com/JimLiu/whisper-subtitles)

Github: [github.com/JimLiu/whisper-subtitles/blob/main/whisperx_youtube_subtitle.ipynb1.](https://github.com/JimLiu/whisper-subtitles/blob/main/whisperx_youtube_subtitle.ipynb1.)

Github: [github.com/JimLiu/whisper-subtitles/blob/main/whisperx_for_uploading_file.ipynb](https://github.com/JimLiu/whisper-subtitles/blob/main/whisperx_for_uploading_file.ipynb)

#### [【关于AIGC的各种精选教程和资源，既适合初学者也适合进阶AI爱好者】'Awesome AIGC T @#开源#](https://weibo.com/1402400261/Nhq6n4UQR)

Note: 【关于AIGC的各种精选教程和资源，既适合初学者也适合进阶AI爱好者】'Awesome AIGC Tutorials - Curated tutorials and resources for Large Language Models, AI Painting, and more.' luban-agi GitHub: github.com/luban-agi/Awesome-AIGC-Tutorials/blob/main/README_zh.md   

Picture: [5396ee05ly8hhi6qky245j20u00uv77g.jpg](https://weibo.cn//mblog/pic/Nhq6n4UQR?rl=1)

Github: [github.com/luban-agi/Awesome-AIGC-Tutorials/blob/main/README_zh.md](https://github.com/luban-agi/Awesome-AIGC-Tutorials/blob/main/README_zh.md)

#### [《可解释的机器学习--黑盒模型可解释性理解指南》该书为《Interpretable Machine  @蚁工厂](https://weibo.com/2194035935/NhAE542BQ)

Note: 《可解释的机器学习--黑盒模型可解释性理解指南》该书为《Interpretable Machine Learning》中文版。原作者是 Christoph Molnar，译者朱明超github.com/MingchaoZhu/InterpretableMLBook 

Picture: [82c654dfly1gid6msdn73j21ch1jxtip.jpg](https://weibo.cn//mblog/pic/JiSxvvKgA?rl=1)

Github: [github.com/MingchaoZhu/InterpretableMLBook](https://github.com/MingchaoZhu/InterpretableMLBook)

#### [解决 CMake 编译错误小技巧，开启 makefile 详细日志 set(CMAKE_VERBOS @Loken2022](https://weibo.com/2194035935/NhU1FzKa8)

Note: 解决 CMake 编译错误小技巧，开启 makefile 详细日志 set(CMAKE_VERBOSE_MAKEFILE ON) 就可以看到编译一个文件用到的完整的 gcc/g++ 命令，就能根据命令的参数反推回去 CMake 的代码哪里写错了。 

Picture: [008rupcegy1hhlut1o9cxj31ax0nj43e.jpg](https://weibo.cn//mblog/pic/NhU0YriRE?rl=1)

#### [适合非技术人员看的数据库技术的介绍：A More Human Approach To Databas @玩家老C](https://weibo.com/2194035935/NhW3yqOpF)

Note: 适合非技术人员看的数据库技术的介绍：A More Human Approach To Databases： https:////ccorcos.github.io/filing-cabinets/（中文翻译版）数据库简明入门： 

#### [中文LLaMA&Alpaca大模型的第二期项目地址：github.com/ymcui/Chinese @转发[46]](https://weibo.com/2194035935/NhZHry7fJ)

Note: 中文LLaMA&Alpaca大模型的第二期项目地址：github.com/ymcui/Chinese-LLaMA-Alpaca-2本项目基于Meta发布的可商用大模型Llama-2开发，是中文LLaMA&Alpaca大模型的第二期项目，开源了中文LLaMA-2基座模型和Alpaca-2指令精调大模型。这些模型在原版Llama-2的基础上扩充并优化了中文词表，使用了大规模中文数据进行增量预训练，进一步提升了中文基础语义和指令理解能力，相比一代相关模型获得了显著性能提升。相关模型支持FlashAttention-2训练。标准版模型支持4K上下文长度，长上下文版模型支持16K上下文长度，并可通过NTK方法最高扩展至24K+上下文长度。回复:已收藏至Notion

Picture: [82c654dfly1hhmjvye56ij218g06y79s.jpg](https://weibo.cn//mblog/pic/NhZHry7fJ?rl=1)

Github: [github.com/ymcui/Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)

#### [百川推出了自己大模型的升级版Baichuan2介绍地址：github.com/baichuan-in @转发[28]](https://weibo.com/2194035935/Ni2t0jr4X)

Note: 百川推出了自己大模型的升级版Baichuan2介绍地址：github.com/baichuan-inc/Baichuan2Baichuan 2 是百川智能推出的新一代开源大语言模型，采用 2.6 万亿 Tokens 的高质量语料训练。Baichuan 2 在多个权威的中文、英文和多语言的通用、领域 benchmark 上取得同尺寸最佳的效果。本次发布包含有 7B、13B 的 Base 和 Chat 版本，并提供了 Chat 版本的 4bits 量化。回复:已收藏到你的notion[666]7B版本大幅弱于ChatGLM2-6B？

Picture: [82c654dfly1hhmw40ieg1j217p0tt144.jpg](https://weibo.cn//mblog/pic/Ni2t0jr4X?rl=1)

Github: [github.com/baichuan-inc/Baichuan2Baichuan](https://github.com/baichuan-inc/Baichuan2Baichuan)

#### [TIIuae 发布Falcon 180B 猎鹰大语言模型 性能超越LLaMA 2，逼近GPT4😲经本 @互联网的那点事](https://weibo.com/1727858283/Ni6q2xWAr)

Note: TIIuae 发布Falcon 180B 猎鹰大语言模型 性能超越LLaMA 2，逼近GPT4😲经本人测试，确实还是很牛p的，感觉是目前性能仅次于OpenAI的玩意了Falcon 180B是一个拥有1800亿参数的超强大语言模型，它在3.5万亿个标记上进行了训练。该模型在Hugging Face的预训练大型语言模型排行榜上位列首位。官方称Falcon 180B在各种任务，如推理、编码、熟练度和知识测试方面，表现得非常出色，甚至超过了Meta的LLaMA 2。在闭源模型中，它仅次于OpenAI的GPT 4，并与Google的PaLM 2 Large（即Bard的动力来源）表现相当，尽管模型大小只有后者的一半。Falcon 180B可用于研究和商业用途。我进行了初步的测试对中文支持都还挺不错的，逻辑也是没的说确实有点非常比较GPT4了，其他的没来的测试，有点小激动。但是确实是比LLaMA 2和Google的 Bard 强很多。但是输出速度有点慢，而且容易截断，视频我是2倍速。官网：体验： 实测这个模型是比不过 gpt3.5 的，更不用说 gpt4 了，我用的测试用例是 我建议您通过给每个汉字添加序号来数数，这样可以更准确。 “你不知道我什么时候离开的”这句子有几个汉字？ 步骤一：添加序号 步骤二：回答问题 步骤三：答案用json表示，格式为{"length": number}问了一下一个和powershell相关的权限不足的错误如何解决，和GPT-4的差距巨大……1加到101不是5050都180B了…可是计算过程和计算结果都错了呢，果然数学不好也继承了数学题答错了[毛球] 但解释的方法没错

#### [【Batched LoRAs：通过同一批次的多个 LoRA 路由推理，最大化 GPU 利用率】’Ba @#开源#](https://weibo.com/1402400261/NhSlThJ7O)

Note: 【Batched LoRAs：通过同一批次的多个 LoRA 路由推理，最大化 GPU 利用率】’Batched LoRAs - batched loras' Ali Sabet GitHub: github.com/sabetAI/BLoRA   

Picture: [5396ee05ly8hhlnfrcdl0j20j70diq43.jpg](https://weibo.cn//mblog/pic/NhSlThJ7O?rl=1)

Github: [github.com/sabetAI/BLoRA](https://github.com/sabetAI/BLoRA)

#### [【Rapidgzip: 用于几乎任何gzip文件的高性能并行解压缩命令行工具】’Rapidgzip: @#开源#](https://weibo.com/1402400261/NhSxodz17)

Note: 【Rapidgzip: 用于几乎任何gzip文件的高性能并行解压缩命令行工具】’Rapidgzip: Parallelized Decompression of Gzip Files with Support for Fast Random Access - Gzip Decompression and Random Access for Modern Multi-Core Machines' Maximilian Knespel GitHub: github.com/mxmlnkn/rapidgzip 

Github: [github.com/mxmlnkn/rapidgzip](https://github.com/mxmlnkn/rapidgzip)

#### [【AiDB：使用C++完成的深度学习模型部署工具箱。将主流深度学习推理框架抽象成统一接口，包括ONN @#开源#](https://weibo.com/1402400261/NhSyuctSL)

Note: 【AiDB：使用C++完成的深度学习模型部署工具箱。将主流深度学习推理框架抽象成统一接口，包括ONNXRUNTIME、MNN、NCNN、TNN、PaddleLite和OpenVINO】'AiDB - A toolbox for deep learning model deployment using C++ YoloX | YoloV7 | YoloV8 | Gan | OCR | MobileVit | Scrfd ........' Hulk GitHub: github.com/TalkUHulk/ai.deploy.box   回复:

Picture: [5396ee05ly8hhloch8fdcj20qh0hugob.jpg](https://weibo.cn//mblog/pic/NhSyuctSL?rl=1)

Github: [github.com/TalkUHulk/ai.deploy.box](https://github.com/TalkUHulk/ai.deploy.box)

#### [  全国六强“会魔法的老人”团队决赛答辩分享团队名称：会魔法的老人团队成员：刘克林（）敖宇（）杨敏（ @#数据派thu的独家放送#](https://weibo.com/6004911042/Niapw0MRy)

Note:   全国六强“会魔法的老人”团队决赛答辩分享团队名称：会魔法的老人团队成员：刘克林（）敖宇（）杨敏（）团队名次：全国第二名  