# 深度学习硬件及应用

## 硬件结构与分析

[NPU加速器建模设计(完整版)](https://mp.weixin.qq.com/s/Jn8av9Ld4lg3gEjJBDX2OQ)

[Simon Knowles智源大会分享：芯片和算法之间存在巨大错差，没有通用的XPU](https://mp.weixin.qq.com/s/TT6pff8c5jh1UtMj2LKeXQ)
* 视频在[bilibili](https://www.bilibili.com/video/BV15M4y1Y7Da/?spm_id_from=333.999.0.0), 很深度,芯片设计和发展方向相关,值得单独来看

[ane](https://github.com/eiln/ane)
* 苹果神经网络引擎逆向


## 硬件结合的优化

[Accelerated Computing - Programming GPUs](https://tschmidt23.github.io/cse599i/)
* GPU硬件教程,
* [warp shuffle](https://tschmidt23.github.io/cse599i/CSE%20599%20I%20Accelerated%20Computing%20-%20Programming%20GPUs%20Lecture%2018.pdf)

[COOPERATIVE GROUPS](https://on-demand.gputechconf.com/gtc/2017/presentation/s7622-Kyrylo-perelygin-robust-and-scalable-cuda.pdf)
* Nvidia GPU cooperative groups介绍

[Module 4.4 - Memory and Data Locality](https://engineering.purdue.edu/~smidkiff/ece563/NVidiaGPUTeachingToolkit/Mod4/Lecture-4-4-tiled-matrix-multiplication-kernel.pdf)
* Tiling相关介绍

## 厂商硬件信息

[后摩智能车载系列: 鸿途H30](https://www.geekpark.net/news/320934)
* SRAM,存算一体大算力AI, 智能驾驶
* 12nm,256TOPS@INT8, TDP 35W, 能效比7.3TOPS/W(冯诺伊曼结构一般为2TOPS/W)
* resnet-50-int8: 8700FPS@bs1, 10300FPS@bs8, 是Nvidia Orin的2.3和5.7倍
* 力驭平台(域控制器):CPU算力为200Kdmips, AI256TOPS
* 后摩大道:支持torch/tf/onnx/cuda/SIMD/SIMT
* 23年6月送测,已再新石器无人车/环宇智行部署;24年推出H50, 25年量产

[GPU Prices Quick Reference](http://arthurchiao.art/blog/gpu-prices/)
* 各家云厂商的GPU实例在23年6月份价格

[Hetzner CAX 系列 ARM64 服务器性能简评以及 WebP Cloud Services 在其上的实践](https://blog.webp.se/hetzner-arm64-zh/)

[Nvidia Document Hub](https://docs.nvidia.com/#nvidia-nsight-developer-tools): nvidia文档入口
* [GPU Mangement and Deployment](https://docs.nvidia.com/deploy/index.html):NVML库
* [NVIDIA Nsight Systems installation guide](https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html): Nsight文档
* [Get Started With Deep Learning Performance](https://docs.nvidia.com/deeplearning/performance/dl-performance-getting-started/index.html)
* [Convolutional Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#checklist)
* [NVIDIA Deep Learning TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#valgrind)

[CUDA_VISIBLE_DEVICES](https://blog.csdn.net/jzwong/article/details/103813999)

[机器学习系统:设计和实现](https://openmlsys.github.io/index.html): ML系统构建方法,开源书

[一文解析，Linux内核——Intel CPU体系结构](https://zhuanlan.zhihu.com/p/506663731?utm_source=com.microsoft.todos&utm_medium=social&utm_oi=49336847171584): intel CPU架构分析,较为详细

[Intel 4 Deep Dive](https://semiwiki.com/semiconductor-manufacturers/intel/314047-intel-4-presented-at-vlsi/): Intel 4工艺介绍

[Intel® Xeon® Processor Scalable Family Technical Overview](https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html)

[nvdla](https://github.com/orgs/nvdla/repositories)

[nvdla/hw: RTL, Cmodel, and testbench for NVDLA](https://github.com/nvdla/hw)

[NVDLA Index of Documentation — NVDLA Documentation](http://nvdla.org/contents.html)

[NVDLA - Microarchitectures - Nvidia - WikiChip](https://en.wikichip.org/wiki/nvidia/microarchitectures/nvdla)

[NVDLA Primer — NVDLA Documentation](http://nvdla.org/primer.html)

[NVDLA 深度学习推理编译器现已开源 - 知乎](https://zhuanlan.zhihu.com/p/83143622)


[Preliminary report:Initial evaluation of StdPar implementations on AMD GPUs for HPC](https://browse.arxiv.org/html/2401.02680v1): 一篇论文,通过intel sycl编译器实现了MI210上的stdpar程序(这是一个并行库,用于科学计算), 论文中信息对于使用hip和XNACK有一定启示.

[重新拿回计算机硬件的红利](https://mp.weixin.qq.com/s/1OSRcBfd58s0tgZTUZHB9g): 微信文章, 包含硬件发展信息

[大算力与高能效AI芯片发起冲锋！2023全球AI芯片峰会第二日干货总结](https://mp.weixin.qq.com/s/tS1nkme3YPYdt7w2VST68Q): 国产芯片企业, 一些数字

[Intel Demos Lunar Lake Client Processor In Action, Silicon Pulled In To Intel 20A?](https://www.anandtech.com/show/20061/intel-demos-lunar-lake-in-action-silicon-pulled-in-to-intel-20a): intel A20制程芯片制造推测

[Estimating your memory bandwidth](https://lemire.me/blog/2024/01/13/estimating-your-memory-bandwidth/): 内存带宽测试的一个简单实现

[dynamic-page-retirement](https://docs.nvidia.com/deploy/dynamic-page-retirement/index.html): Nvidia 显存页面动态拉黑

##### [Machine Learning Engineering Open Book](https://github.com/stas00/ml-engineering/blob/master/README.md)

以下为本repo提到的一些外部的链接, 主要来自
* Nvidia/AMD/intel等硬件厂商的官方链接和使用指南
* 大模型框架pytorch/deepspeed等的repo/代码/issue/debug/doc
* 大模型训练的论文/训练日志
* 日常debug流程和工具

[gpu-debug-guidelines](https://docs.nvidia.com/deploy/gpu-debug-guidelines/index.html): Nvidia推荐的排错流程

[NVIDIA® Data Center GPU Manager](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/index.html): Nvidia DCGM使用文档, 用于监控/配置/检测/模拟错误等

[[math] what network throughput is required to handle ZeRO-3 traffic?](https://github.com/microsoft/DeepSpeed/issues/2928#issuecomment-1463041491):Deepspeed上的讨论, 100Gbps网络只能实现20 TFLOPs/200~400Gbps < 40TFLOPs / 800Gbps 40+TFLOPs

[Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473): 混合使用各种并行实现千卡量级训练,效率50%

[Data Movement Is All You Need: A Case Study on Optimizing Transformers](https://arxiv.org/abs/2007.00072): 减少数据搬移

[Transformer Math 101](https://blog.eleuther.ai/transformer-math/): transformer的计算

[Memory management](https://pytorch.org/docs/stable/notes/cuda.html#memory-management): torch内存相关的设置

[DISTRIBUTEDDATAPARALLEL](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html): pytorch的DDP介绍

[Zero Redundancy Optimizer](https://www.deepspeed.ai/tutorials/zero/): Deepspeed ZERO optimizer 介绍

[FULLYSHARDEDDATAPARALLEL](https://pytorch.org/docs/stable/fsdp.html): Torch的ZERO Optimizer实现

[[Benchmark] HF Trainer on RTX-3090](https://github.com/huggingface/transformers/issues/14608): huggingface transformer的训练精度影响因素探究

[Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/abs/2205.05198): 减少重计算的90%计算量

[[pure bf16 training] w/ AnyPrecisionAdamW and Kahan summation](https://github.com/huggingface/transformers/pull/21312): 正在往transformers里集成纯bf16训练

[The Depth-to-Width Interplay in Self-Attention](https://arxiv.org/abs/2006.12467): 加宽和加深模型都有效,但深度有效率问题, 模型在一定程度后更需要加宽, 并给出了指导原则

[Revisiting BFloat16 Training](https://arxiv.org/abs/2010.06192): 在bf16训练场景中加入随机舍入和kahan求和以减少精度损失

[Linear/Fully-Connected Layers User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features): Nvidia FC层性能指导

[Matrix Multiplication Background User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc): Nvidia GEMM优化指导

[WRITING DISTRIBUTED APPLICATIONS WITH PYTORCH](https://pytorch.org/tutorials/intermediate/dist_tuto.html#collective-communication): torch分布式程序示例

[Wiki: FLOPS](https://en.wikipedia.org/wiki/FLOPS): FLOPS

[https://en.wikipedia.org/wiki/NVLink](https://en.wikipedia.org/wiki/NVLink): NVLINK wiki, 目前只进行了数量升级,并未进行架构更新

[Upgrading Multi-GPU Interconnectivity with the Third-Generation NVIDIA NVSwitch](https://developer.nvidia.com/blog/upgrading-multi-gpu-interconnectivity-with-the-third-generation-nvidia-nvswitch/): Nvlink介绍文档

[Intel® HLS-Gaudi®2 AI Accelerator Server](https://habana.ai/wp-content/uploads/2023/10/HLS-Gaudi2_Datasheet_10_23.pdf): gaudi2 datasheet

[Non-uniform memory access](https://en.wikipedia.org/wiki/Non-uniform_memory_access): NUMA架构wiki

[hwloc](https://github.com/open-mpi/hwloc): mpi子项目,用于检测硬件的远近关系

[Utilize Non-Uniform Memory Access (NUMA) Controls](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-non-uniform-memory-access-numa-controls): pytorch的NUMA优化设置

[bigscience-workshop/Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed): bigscience的fork版本Megatron-deepspeed

[p2pBandwidthLatencyTest](https://github.com/NVIDIA/cuda-samples/tree/master/Samples/5_Domain_Specific/p2pBandwidthLatencyTest): cuda samples中的p2p带宽测试

[Debugging Compiled Programs](https://github.com/stas00/the-art-of-debugging/tree/master/compiled-programs#shared-libraries-ldsoconf-nm-unresolved-symbols-ldd-ld_library_path-ld_preload): 编译程序debug: 配置core文件,gdb,nm, LD_LIBRARY_PATH等

[tr8-104B Chronicles(pre-BLOOM 108B)](https://github.com/bigscience-workshop/bigscience/blob/master/train/tr8-104B-wide/chronicles.md): bigscience的训练记录,包括如何调整使训练稳定的尝试

[NUMERICAL ACCURACY](https://pytorch.org/docs/stable/notes/numerical_accuracy.html): pytorch的数据精度讨论, TF32和BF16等对训练精度的影响

[Corby's numerically more stable self attn version](https://github.com/bigscience-workshop/Megatron-DeepSpeed/pull/118): attn在打dim时候的优化方法, 减少inf产生

[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf): 当特定step会精度问题时候, 跳过该step

[Exclusive: Nvidia's new China-focused AI chip set to be sold at similar price to Huawei product](https://www.reuters.com/technology/nvidias-new-china-focused-ai-chip-set-be-sold-similar-price-huawei-product-2024-02-01/): 路透报道H20价格在1.2~1.5W美元24Q1小幅出货, 与华为910B价格相近,但FP32性能不及后者一半, 互联性能略有提高

[Parallel stream processing with zero-copy fan-out and sharding](https://stevana.github.io/parallel_stream_processing_with_zero-copy_fan-out_and_sharding.html): 硬件优化, 扇出和pipeline